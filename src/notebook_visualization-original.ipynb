{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f100201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from utils.register_dataset import register_vrd_dataset\n",
    "from config import parse_args, get_vrd_cfg\n",
    "from utils.trainer import CustomTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19468c06",
   "metadata": {},
   "source": [
    "### [Optional] Training Visual Backbone Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18682efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/14 13:18:27 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=101, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=400, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/14 13:18:28 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"
     ]
    }
   ],
   "source": [
    "#Train Dataset\n",
    "cfg = get_vrd_cfg()\n",
    "# register_vrd_dataset('vrd')\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = CustomTrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96f8a938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mresume_or_load(resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.resume_or_load(resume=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93451a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "detectron = build_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = detectron.backbone\n",
    "b = None\n",
    "detectron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a2c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "detectron.roi_heads.box_pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.modeling.poolers import ROIPooler\n",
    "pooler = ROIPooler((7, 7), pooler_type='ROIAlignV2', scales=[1/4, 1/8, 1/16, 1/32], sampling_ratio=4)\n",
    "pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b88019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detections Output\n",
    "import random\n",
    "from utils.register_dataset import register_vrd_dataset\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "register_vrd_dataset('vrd')\n",
    "detectron_dataset = DatasetCatalog.get(\"vrd_train\")\n",
    "vrd_metadata = MetadataCatalog.get(\"vrd_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49adacf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "for d in random.sample(detectron_dataset, 3):\n",
    "    print(d[\"file_name\"])\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=vrd_metadata, scale=1)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    img = vis.get_image()\n",
    "    plt.figure(dpi=1200)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from detectron2.structures import BoxMode\n",
    "from config import ROOT_DIR, VRD_DATASET_PATH, VG200_DATASET_PATH\n",
    "\n",
    "def load_dataset_annotations(dataset_name):\n",
    "    \"\"\"\n",
    "        Load raw annotation dataset and convert it into standard detectron2 format\n",
    "        Input:\n",
    "            dataset_name: 'vrd/train' or 'vrd/val'\n",
    "    \"\"\"\n",
    "    dataset_dicts = []\n",
    "    set_name, set_type = dataset_name.split('/')\n",
    "    if (set_name == 'vrd'):\n",
    "        \n",
    "        #VRD data path\n",
    "        dataset_path = VRD_DATASET_PATH\n",
    "\n",
    "        #load annotations file\n",
    "        annotations = {}\n",
    "        file_name = \"new_annotations_{}.json\".format(set_type)\n",
    "        with open(os.path.join(dataset_path, file_name)) as annotations_file:\n",
    "            annotations = json.load(annotations_file)\n",
    "        \n",
    "        #Map 'annotations' to the correct format\n",
    "        img_file_dir = os.path.join(VRD_DATASET_PATH, '{}_images'.format(set_type))\n",
    "        for key, val in annotations.items():\n",
    "            record = {}\n",
    "\n",
    "            #populating image fields\n",
    "            img_file_path = os.path.join(img_file_dir, key)\n",
    "            height, width = cv2.imread(img_file_path).shape[:2]\n",
    "\n",
    "            record['file_name'] = img_file_path\n",
    "            record['height'] = height\n",
    "            record['width'] = width\n",
    "            record['image_id'] = int(key.split('.')[0])\n",
    "\n",
    "            #populating annotations from record\n",
    "            objs = []\n",
    "            visited = set()\n",
    "            for anno in val:\n",
    "                \n",
    "                subj = anno['subject'] #ymin, ymax, xmin, xmax\n",
    "                obj = anno['object']\n",
    "                bbox_subj = subj['bbox']\n",
    "                bbox_obj = obj['bbox']\n",
    "                new_bbox_subj = [bbox_subj[2], bbox_subj[0], bbox_subj[3], bbox_subj[1]]\n",
    "                new_bbox_obj = [bbox_obj[2], bbox_obj[0], bbox_obj[3], bbox_obj[1]]\n",
    "                \n",
    "                subj_dict = {\n",
    "                    'bbox': new_bbox_subj,\n",
    "                    'bbox_mode': BoxMode.XYXY_ABS,\n",
    "                    'category_id': subj['category']\n",
    "                }\n",
    "                if (tuple(bbox_subj) not in visited):\n",
    "                    objs.append(subj_dict)\n",
    "                    visited.add(tuple(bbox_subj))\n",
    "\n",
    "                obj_dict = {\n",
    "                    'bbox': new_bbox_obj,\n",
    "                    'bbox_mode': BoxMode.XYXY_ABS,\n",
    "                    'category_id': obj['category'],\n",
    "                }\n",
    "                if (tuple(bbox_obj) not in visited):\n",
    "                    objs.append(obj_dict)\n",
    "                    visited.add(tuple(bbox_obj))\n",
    "\n",
    "            record['annotations'] = objs\n",
    "                         \n",
    "            #add img to dataset dict\n",
    "            dataset_dicts.append(record)\n",
    "    \n",
    "    return dataset_dicts\n",
    "data = load_dataset_annotations('vrd/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeaa11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1418a24d",
   "metadata": {},
   "source": [
    "### [Required] DataSet and DataLoader (Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da5eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "from torch._C import import_ir_module\n",
    "from utils.boxes import boxes_union\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from config import ROOT_DIR, VRD_DATASET_PATH, VG200_DATASET_PATH\n",
    "VRD_DATASET_PATH = \"../data/vrd/\"\n",
    "\n",
    "# custom transformation\n",
    "detectron_transform = T.Resize((800, 800))\n",
    "\n",
    "class VRDDataset(Dataset):\n",
    "    def __init__(self, set_type=\"train\", transform=detectron_transform):\n",
    "        \"\"\"\n",
    "            Note:\n",
    "                transform only applys on bounding boxes. The transformation to the image should be done by detectron2 dataloader.\n",
    "        \"\"\"\n",
    "        annotations_path = os.path.join(\n",
    "            VRD_DATASET_PATH, \"new_annotations_{}.json\".format(set_type)\n",
    "        )\n",
    "        self.images_dir = os.path.join(VRD_DATASET_PATH, \"{}_images\".format(set_type))\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(annotations_path) as fp:\n",
    "            raw_annotation = json.load(fp)\n",
    "        self.annotations = list(raw_annotation.items())\n",
    "\n",
    "        # check if the data is pre-generated\n",
    "        roidb_chkpt = os.path.join(VRD_DATASET_PATH, \"vrd_roidb_{}.json\")\n",
    "        if os.path.exists(roidb_chkpt):\n",
    "            with open(roidb_chkpt) as fp:\n",
    "                self.roidb = json.load(fp)\n",
    "        else:\n",
    "            roidb = []\n",
    "            for img_name, annotations in raw_annotation.items():\n",
    "                subj_bboxes = []\n",
    "                obj_bboxes = []\n",
    "                union_bboxes = []\n",
    "                unique_objects = {}\n",
    "                subj_classes = []\n",
    "                obj_classes = []\n",
    "                pred_classes = []\n",
    "                for anno in annotations:\n",
    "                    subj = anno[\"subject\"]\n",
    "                    obj = anno[\"object\"]\n",
    "\n",
    "                    subj[\"bbox\"] = [\n",
    "                        subj[\"bbox\"][2],\n",
    "                        subj[\"bbox\"][0],\n",
    "                        subj[\"bbox\"][3],\n",
    "                        subj[\"bbox\"][1],\n",
    "                    ]  # XYXY\n",
    "                    obj[\"bbox\"] = [\n",
    "                        obj[\"bbox\"][2],\n",
    "                        obj[\"bbox\"][0],\n",
    "                        obj[\"bbox\"][3],\n",
    "                        obj[\"bbox\"][1],\n",
    "                    ]\n",
    "\n",
    "                    union_bboxes.append(\n",
    "                        boxes_union(np.array([subj[\"bbox\"]]), np.array([obj[\"bbox\"]]))[0]\n",
    "                    )\n",
    "                    subj_bboxes.append(subj[\"bbox\"])\n",
    "                    obj_bboxes.append(obj[\"bbox\"])\n",
    "                    unique_objects[(tuple(subj[\"bbox\"]))] = {\n",
    "                        \"bbox\": subj[\"bbox\"],\n",
    "                        \"bbox_mode\": 0,  # BoxMode.XYXY_ABS\n",
    "                        \"category_id\": subj[\"category\"],\n",
    "                    }\n",
    "                    unique_objects[(tuple(obj[\"bbox\"]))] = {\n",
    "                        \"bbox\": obj[\"bbox\"],\n",
    "                        \"bbox_mode\": 0,\n",
    "                        \"category_id\": obj[\"category\"],\n",
    "                    }\n",
    "                    subj_classes.append(subj[\"category\"])\n",
    "                    obj_classes.append(obj[\"category\"])\n",
    "                    pred_classes.append(anno[\"predicate\"])\n",
    "                                \n",
    "                roidb.append(\n",
    "                    {\n",
    "                        # Detectron\n",
    "                        \"file_name\": os.path.join(self.images_dir, img_name),\n",
    "                        \"image_id\": int(img_name.split(\".\")[0]),\n",
    "                        \"annotations\": list(unique_objects.values()),\n",
    "                        # Relationships\n",
    "                        \"relationships\": {\n",
    "                            \"subj_bboxes\": subj_bboxes,\n",
    "                            \"obj_bboxes\": obj_bboxes,\n",
    "                            \"union_bboxes\": union_bboxes,\n",
    "                            \"subj_classes\": subj_classes,\n",
    "                            \"pred_classes\": pred_classes,\n",
    "                            \"obj_classes\": obj_classes,\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "            self.roidb = roidb\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.roidb)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.roidb[idx]\n",
    "        cv2.setNumThreads(0)\n",
    "        image = cv2.imread(item[\"file_name\"])\n",
    "        \n",
    "        #get transformation\n",
    "        auginput = T.AugInput(image)\n",
    "        transform = self.transform(auginput)\n",
    "        auginput2 = T.AugInput(image)\n",
    "        transform2 = self.transform(auginput2)\n",
    "        relationships = item[\"relationships\"]\n",
    "        \n",
    "        #update bboxes\n",
    "        subj_bboxes = []\n",
    "        obj_bboxes = []\n",
    "        union_bboxes = []\n",
    "        \n",
    "        for subj_bbox, obj_bbox in zip(relationships['subj_bboxes'], relationships['obj_bboxes']):\n",
    "            new_subj_box = transform.apply_box(subj_bbox)\n",
    "            new_obj_box = transform2.apply_box(obj_bbox)\n",
    "            new_union_box = boxes_union(copy.deepcopy(new_subj_box), copy.deepcopy(new_obj_box))[0]\n",
    "            \n",
    "            subj_bboxes.append(new_subj_box)\n",
    "            obj_bboxes.append(new_obj_box)\n",
    "            union_bboxes.append(new_union_box)\n",
    "        relationships['subj_bboxes'] = subj_bboxes\n",
    "        relationships['obj_bboxes'] = obj_bboxes\n",
    "        relationships['union_bboxes'] = union_bboxes\n",
    "        # add height and width\n",
    "        height, width = image.shape[:2]\n",
    "        item[\"height\"] = height\n",
    "        item[\"width\"] = width\n",
    "        \n",
    "        return item\n",
    "\n",
    "\n",
    "def get_object_classes(set_name):\n",
    "    if set_name == \"vrd\":\n",
    "        classes = []\n",
    "        with open(os.path.join(VRD_DATASET_PATH, \"objects.json\")) as fp:\n",
    "            classes = json.load(fp)\n",
    "        return classes\n",
    "    elif set_name == \"vg\":\n",
    "        # to be implemented\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_predicate_classes(set_name):\n",
    "    if set_name == \"vrd\":\n",
    "        classes = []\n",
    "        with open(os.path.join(VRD_DATASET_PATH, \"predicates.json\")) as fp:\n",
    "            classes = json.load(fp)\n",
    "        # add unknown predicate class for missing predicates\n",
    "        # classes.insert(0, 'unknown')\n",
    "        classes.append('unrelated')\n",
    "        return classes\n",
    "    elif set_name == \"vg\":\n",
    "        # to be implemented\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def visualize_image_bboxes(image, instances, object_classes=None):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            image: image tensor\n",
    "            instances: Instances object from detectron2\n",
    "    \"\"\"\n",
    "    img = copy.deepcopy(image)\n",
    "    instances_dict = instances[0].get_fields()\n",
    "    bounding_boxes = [box.tolist() for box in instances_dict['pred_boxes']]\n",
    "    labels = instances_dict['pred_classes']\n",
    "    \n",
    "    for bbox in bounding_boxes:\n",
    "        img = cv2.rectangle(img, [int(coord) for coord in bbox[0:2]], [int(coord) for coord in bbox[2:4]], (255,255,255), 2)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_bboxes(dataset):\n",
    "    pred_classes = get_predicate_classes(\"vrd\")\n",
    "    obj_classes = get_object_classes(\"vrd\")\n",
    "\n",
    "    for i in random.sample(range(len(dataset)), 60):\n",
    "        image, cropped_img, anno = dataset[i]\n",
    "        img = copy.deepcopy(image)\n",
    "        subj = anno[\"subject\"]\n",
    "        obj = anno[\"object\"]\n",
    "\n",
    "        subject_class = obj_classes[subj[\"category\"]]\n",
    "        predicate_class = pred_classes[anno[\"predicate\"]]\n",
    "        object_class = obj_classes[obj[\"category\"]]\n",
    "\n",
    "        img = cv2.rectangle(img, subj[\"bbox\"][0:2], subj[\"bbox\"][2:4], (0, 0, 255), 2)\n",
    "        img = cv2.rectangle(img, obj[\"bbox\"][0:2], obj[\"bbox\"][2:4], (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow(\n",
    "            \" \".join((subject_class, predicate_class, object_class)), cropped_img\n",
    "        )\n",
    "        cv2.waitKey(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86653a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract ROI Features\n",
    "\"\"\"\n",
    "Relevant Links:\n",
    "    https://towardsdatascience.com/faster-rcnn-object-detection-f865e5ed7fc4#:~:text=Faster%20RCNN%20is%20an%20object,SSD%20(%20Single%20Shot%20Detector).\n",
    "\"\"\"\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures import ImageList\n",
    "\n",
    "from detectron2.modeling import build_backbone\n",
    "from detectron2.modeling.poolers import ROIPooler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from config import get_vrd_cfg\n",
    "\n",
    "def get_roi_features(images, box_lists, output_size=(14,14)):\n",
    "    \"\"\"\n",
    "        Get image features from the backbone network\n",
    "        Input:\n",
    "            images: (ImageList.from_tensors) with dimension (B,C,W,H)\n",
    "            box_lists: A list of N boxes\n",
    "    \"\"\"\n",
    "    cfg = get_vrd_cfg()\n",
    "    backbone = build_backbone(cfg)\n",
    "    pooler = ROIPooler(output_size, pooler_type='ROIAlignV2', scales=[1/4, 1/8, 1/16, 1/32, 1/64], sampling_ratio=4)\n",
    "    feature_maps = backbone(images)\n",
    "    feature_maps = [feature_maps['p{}'.format(i)] for i in range(2,7)]\n",
    "    regions_feature = pooler(feature_maps, box_lists)\n",
    "    print(regions_feature.shape)\n",
    "\n",
    "    return regions_feature\n",
    "\n",
    "def generate_roi_features(dataset):\n",
    "    pass\n",
    "    # images = ImageList.from_tensors(tensors)  # preprocessed input tensor\n",
    "    # model = build_model(cfg)\n",
    "    # DetectionCheckpointer(model).load(\"output/model_final.pth\")\n",
    "    # model.eval()\n",
    "    # features = model.backbone(images.tensor)\n",
    "    # proposals, _ = model.proposal_generator(images, features)\n",
    "    # instances, _ = model.roi_heads(images, features, proposals)\n",
    "    # mask_features = [features[f] for f in model.roi_heads.in_features]\n",
    "    # mask_features = model.roi_heads.mask_pooler(mask_features, [x.pred_boxes for x in instances])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36429a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#[Only Run once] Register dataset with detectron2 instead of using my own dataloader\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from utils.annotations import get_object_classes\n",
    "\n",
    "def get_vrd_dicts(dataset_name):\n",
    "    set_name, set_type = dataset_name.split('/')\n",
    "    dataset = None\n",
    "    if (set_name == 'vrd'):\n",
    "        dataset = VRDDataset(set_type=set_type) \n",
    "    return dataset\n",
    "\n",
    "def register_vrd_dataset(set_name):\n",
    "    \"\"\"\n",
    "        Register dataset and its metadata to the detectron2 engine\n",
    "        Input:\n",
    "            set_name: 'vrd' or vg200\n",
    "    \"\"\"\n",
    "    thing_classes = get_object_classes(set_name)\n",
    "\n",
    "    #register the annotations\n",
    "    for d_type in ['train', 'val']:\n",
    "        DatasetCatalog.register('_'.join((set_name, d_type)), lambda d_type=d_type: get_vrd_dicts('/'.join((set_name, d_type))))\n",
    "        MetadataCatalog.get('_'.join((set_name, d_type))).set(thing_classes=thing_classes)\n",
    "register_vrd_dataset('vrd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf6467d",
   "metadata": {},
   "source": [
    "### [Optional] Testing/Check whether ROI is updated with Image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dataloader\n",
    "from detectron2.data import (\n",
    "    DatasetCatalog, DatasetMapper,\n",
    "    build_detection_train_loader\n",
    ")\n",
    "from config import get_vrd_cfg\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "cfg = get_vrd_cfg()\n",
    "dataloader = build_detection_train_loader(cfg,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c75ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = next(data_iter)\n",
    "train_features = train_features[0] #first element, dataloader with batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e82032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_features\n",
    "relationships = data['relationships']\n",
    "subj_boxes = relationships['subj_bboxes']\n",
    "union_boxes = relationships['union_bboxes']\n",
    "obj_boxes = relationships['obj_bboxes']\n",
    "subj_classes = relationships['subj_classes']\n",
    "pred_classes = relationships['pred_classes']\n",
    "obj_classes = relationships['obj_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.boxes import boxes_intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_line_overlap(box1, box2):\n",
    "    \"\"\"\n",
    "        box1: (xmin1, xmax1)\n",
    "        box2: (xmin2, xmax2)\n",
    "    \"\"\"\n",
    "    xmin1, xmax1 = box1\n",
    "    xmin2, xmax2 = box2\n",
    "    return xmax1 >= xmin2 and xmax2 >= xmin1\n",
    "\n",
    "def is_box_overlap(box1, box2):\n",
    "    xmin1, ymin1, xmax1, ymax1 = box1\n",
    "    xmin2, ymin2, xmax2, ymax2 = box2\n",
    "    \n",
    "    return is_line_overlap((xmin1, xmax1), (xmin2, xmax2)) and is_line_overlap((ymin1, ymax1), (ymin2, ymax2))\n",
    "\n",
    "boxes_intersect(np.array([box[0] for box in subj_boxes]), np.array([box[0] for box in obj_boxes]))\n",
    "# np.array([subj[\"bbox\"]]), np.array([obj[\"bbox\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from utils.annotations import get_object_classes\n",
    "\n",
    "#thing classes\n",
    "thing_classes = get_object_classes('vrd')\n",
    "\n",
    "#Visualizing the trained_features\n",
    "# train_features = data\n",
    "img = train_features['image']\n",
    "img = img.numpy()\n",
    "img = copy.deepcopy(np.transpose(img, (1,2, 0)))\n",
    "\n",
    "boxes = train_features['instances'].get_fields()['gt_boxes']\n",
    "classes = train_features['instances'].get_fields()['gt_classes']\n",
    "for cls, box in zip(classes, boxes):\n",
    "    int_box = [int(i) for i in box]\n",
    "    \n",
    "plt.figure(dpi=800)\n",
    "plt.imshow(img[:,:,[2,1,0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940dd72a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get Region of Interests of the ground truth bboxes of the image\n",
    "images = torch.unsqueeze(train_features['image'], axis=0)\n",
    "boxes = train_features['instances'].get_fields()['gt_boxes']\n",
    "visual_features = get_roi_features(images.float(), box_lists=[boxes], output_size=(7, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520197c",
   "metadata": {},
   "source": [
    "### [Required] BERT Modeling (extracting features from text) (Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab13229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_word_features(triples, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        triples: (Subj, Pred, Obj)\n",
    "    Return:\n",
    "        dict of [CLS, Subj, Pred, Obj, SEP] embeddings\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    marked_text = \"[CLS] \" + \" \".join(triples) + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    \n",
    "    # Save the token split to average them later on\n",
    "    token_placements = defaultdict(list)\n",
    "    triples_temp = list(triples)\n",
    "    for i, tok in enumerate(tokenized_text):\n",
    "        stip_tok = tok.replace('#', '')\n",
    "        if (stip_tok in triples_temp[0]):\n",
    "            token_placements['subj'].append(i)\n",
    "            triples_temp[0] = triples_temp[0].replace(stip_tok, '')\n",
    "        elif (stip_tok in triples_temp[1]):\n",
    "            token_placements['pred'].append(i)\n",
    "            triples_temp[1] = triples_temp[1].replace(stip_tok, '')\n",
    "        elif (stip_tok in triples_temp[2]):\n",
    "            token_placements['obj'].append(i)\n",
    "            triples_temp[2] = triples_temp[2].replace(stip_tok, '')\n",
    "        elif (not tok == '[CLS]' and not tok == '[SEP]'):\n",
    "            print(tok, triples)\n",
    "\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)  # one sentence\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on\n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
    "        # becase we set `output_hidden_states = True`, the third item will be the\n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings.size()\n",
    "\n",
    "    # remove dimension 1\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "\n",
    "    # get token embeddings (list of token embeddings)\n",
    "    token_vecs_cat = []\n",
    "    for token in token_embeddings:\n",
    "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "        token_vecs_cat.append(cat_vec)\n",
    "    results['CLS'] = token_vecs_cat[0]\n",
    "    results['SEP'] = token_vecs_cat[-1]\n",
    "\n",
    "    # average the token embeddings for word that are splitted to get word embeddings\n",
    "    for key, val in token_placements.items():\n",
    "        results[key] = token_vecs_cat[val[0]]\n",
    "        for i in range(1, len(val)):\n",
    "            results[key] += token_vecs_cat[val[i]]\n",
    "        results[key] = results[key] / len(val)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be38a58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = BertModel.from_pretrained(\n",
    "#     \"bert-base-uncased\",\n",
    "#     output_hidden_states=True,  # Whether the model returns all hidden-states.\n",
    "# )\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d045fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#object and predicate labels\n",
    "from utils.annotations import get_object_classes, get_predicate_classes\n",
    "\n",
    "object_classes = get_object_classes('vrd')\n",
    "predicate_classes = get_predicate_classes('vrd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd42873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "triples_embeddings_path = '../generated/triples_embeddings.pt'\n",
    "def get_triples_features(set_name='vrd'):\n",
    "    triples_memo = {}\n",
    "    if (os.path.exists(triples_embeddings_path)):\n",
    "        triples_memo = torch.load(triples_embeddings_path)\n",
    "        return triples_memo\n",
    "        \n",
    "    # initialize the model and tokenizer\n",
    "    model = BertModel.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        output_hidden_states=True,  # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # iterate through the all the triples, and extract the features\n",
    "    dataset = get_vrd_dicts('vrd/train')\n",
    "    for train_feat in dataset:\n",
    "        rel = train_feat['relationships']\n",
    "        for subj_ind, pred_ind, obj_ind in zip(rel['subj_classes'], rel['pred_classes'], rel['obj_classes']):\n",
    "            triples_text = (object_classes[subj_ind], predicate_classes[pred_ind], object_classes[obj_ind])\n",
    "            if ('-'.join(triples_text) in triples_memo):\n",
    "                continue\n",
    "            word_feat = get_word_features(triples_text, model, tokenizer)\n",
    "            triples_memo['-'.join(triples_text)] = word_feat\n",
    "    \n",
    "    try:\n",
    "        torch.save(triples_memo, triples_embeddings_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return triples_memo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575df0db",
   "metadata": {},
   "source": [
    "### [Optional] Visualize language triples similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e73e5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "def cosine_similarity(a, b):\n",
    "    return (a @ b.T) / (norm(a)*norm(b))\n",
    "\n",
    "def get_predicate_similarity_scores(test_triples_feats, token_name='pred'):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            test_triples_feat: triple feature to test\n",
    "            token: pred, obj, subj, or CLS\n",
    "        token\n",
    "    \"\"\"\n",
    "    similarity_scores = {}\n",
    "    for triples, features in triples_memo.items():\n",
    "        try:\n",
    "            similarity_scores[triples] = cosine_similarity(test_triples_feats[token_name], features[token_name])\n",
    "        except:\n",
    "            print(triples)\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cc841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_triples = 'motorcycle-on-road'\n",
    "test_triples_feats = triples_memo[test_triples]\n",
    "similarity_scores = get_predicate_similarity_scores(test_triples_feats, token_name='obj')\n",
    "dict(sorted(similarity_scores.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacks = {\n",
    "    'CLS': [],\n",
    "    'SEP': [],\n",
    "    'subj': [],\n",
    "    'pred': [],\n",
    "    'obj': [],\n",
    "    'subj-obj': [],\n",
    "    'subj+pred': [],\n",
    "}\n",
    "triples_label = []\n",
    "for key, trip in triples_memo.items():\n",
    "    for k, val in trip.items():\n",
    "        stacks[k].append(val)\n",
    "    stacks['subj-obj'].append(torch.cat((trip['subj'], trip['obj']), dim=0))\n",
    "    stacks['subj+pred'].append((trip['subj'] + trip['pred'])/2)\n",
    "    \n",
    "    triples_label.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mp\n",
    "\n",
    "#color maping for plot\n",
    "colors =  [ list(np.random.choice(range(256), size=3)/255) for _ in range(100)]\n",
    "cmap = mp.colors.ListedColormap(colors, name='from_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5491e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#t-sne 2-dim predicate\n",
    "tsne = TSNE(2, perplexity=30, verbose=1)\n",
    "predicate_stack = np.array([item.numpy() for item in stacks['pred']])\n",
    "tsne_proj = tsne.fit_transform(predicate_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab997810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subj-obj 2-dim t-sne\n",
    "tsne = TSNE(2, perplexity=30, verbose=1)\n",
    "subj_obj_stack = np.array([item.numpy() for item in stacks['subj-obj']])\n",
    "tsne_proj = tsne.fit_transform(subj_obj_stack)\n",
    "\n",
    "#plotting\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "num_categories = 100\n",
    "predicate_classes = get_predicate_classes('vrd')\n",
    "pred_labels = np.array([item.split('-')[1] for item in triples_label])\n",
    "for lab, pred in enumerate(predicate_classes):\n",
    "    indices = pred_labels == pred\n",
    "    ax.scatter(tsne_proj[indices,0],tsne_proj[indices,1], c=np.array(cmap(lab)).reshape(1,4), label = pred ,alpha=0.5)\n",
    "ax.legend(fontsize='large', markerscale=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58382dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(subj+pred)/2 2-dim t-sne\n",
    "tsne = TSNE(2, perplexity=30, verbose=1)\n",
    "subj_pred_stack = np.array([item.numpy() for item in stacks['subj+pred']])\n",
    "tsne_proj = tsne.fit_transform(subj_pred_stack)\n",
    "\n",
    "#plotting\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "num_categories = 100\n",
    "predicate_classes = get_predicate_classes('vrd')\n",
    "pred_labels = np.array([item.split('-')[1] for item in triples_label])\n",
    "for lab, pred in enumerate(predicate_classes):\n",
    "    indices = pred_labels == pred\n",
    "    ax.scatter(tsne_proj[indices,0],tsne_proj[indices,1], c=np.array(cmap(lab)).reshape(1,4), label = pred ,alpha=0.5)\n",
    "ax.legend(fontsize='large', markerscale=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obj 2-dim t-sne\n",
    "tsne = TSNE(2, perplexity=30, verbose=1)\n",
    "obj_stack = np.array([item.numpy() for item in stacks['obj']])\n",
    "tsne_proj = tsne.fit_transform(obj_stack)\n",
    "\n",
    "#plotting\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "num_categories = 100\n",
    "predicate_classes = get_predicate_classes('vrd')\n",
    "pred_labels = np.array([item.split('-')[1] for item in triples_label])\n",
    "for lab, pred in enumerate(predicate_classes):\n",
    "    indices = pred_labels == pred\n",
    "    ax.scatter(tsne_proj[indices,0],tsne_proj[indices,1], c=np.array(cmap(lab)).reshape(1,4), label = pred ,alpha=0.5)\n",
    "ax.legend(fontsize='large', markerscale=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeeba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word feature length\n",
    "obj_stack[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3288f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#roi feature length\n",
    "visual_features.reshape((6, 256*7*7)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efebc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.token_embeddings, triples_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810abf10",
   "metadata": {},
   "source": [
    "### [Required] VRDTransR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efcbd48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleDict\n",
    "\n",
    "from utils.annotations import get_object_classes, get_predicate_classes\n",
    "from utils.boxes import boxes_union\n",
    "\n",
    "from modeling.roi_features import get_roi_features\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.modeling import build_backbone\n",
    "from detectron2.modeling.poolers import ROIPooler\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures.boxes import Boxes\n",
    "import random\n",
    "\n",
    "class RelTransR(nn.Module):\n",
    "    def __init__(self, cfg, pooling_size=(7,7), training=True):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Object and Predicate Classes\n",
    "        self.object_classes = get_object_classes('vrd')\n",
    "        self.predicate_classes = get_predicate_classes('vrd')\n",
    "\n",
    "        # Embeddings dimensions\n",
    "        self.visual_feature_dim = 256*pooling_size[0]*pooling_size[1]\n",
    "        self.visual_hidden_dim = 128*pooling_size[0]*pooling_size[1]\n",
    "        self.word_feature_dim = 3072\n",
    "        self.trans_feature_dim = 1024\n",
    "        self.emb_feature_dim = 256\n",
    "        \n",
    "        # Spatial Module\n",
    "        self.spatial_feature_dim = 22\n",
    "        self.spatial_hidden_dim = 64\n",
    "        self.fc_spatial = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.spatial_feature_dim, self.spatial_hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.spatial_hidden_dim, self.spatial_hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # Visual Modal\n",
    "        self.detectron = build_model(cfg)\n",
    "        if (training):\n",
    "            self._load_detectron_chkpoints(cfg)\n",
    "\n",
    "        # Seperate for predicate\n",
    "        self.backbone = copy.deepcopy(self.detectron.backbone)\n",
    "        self.pooler = copy.deepcopy(self.detectron.roi_heads.box_pooler)\n",
    "\n",
    "        # Language Modal\n",
    "        self.bert_model = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\",\n",
    "            # Whether the model returns all hidden-states.\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        self.bert_model.to('cuda')\n",
    "        self.bert_model.eval()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        # Pre-trained token embeddings (static without changes for now)\n",
    "        self.triples_embeddings_path = '../generated/triples_embeddings.pt'\n",
    "        self.token_embeddings = self._load_words_chkpoints(cfg)\n",
    "\n",
    "        # Fully connect language\n",
    "        self.fc_word = ModuleDict({\n",
    "            'subj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.word_feature_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "            'pred': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.word_feature_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "            'obj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.word_feature_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "        })\n",
    "        \n",
    "        # Fully connect roi\n",
    "        self.fc_rois = ModuleDict({\n",
    "            'subj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.visual_feature_dim, self.visual_hidden_dim),\n",
    "                #torch.nn.BatchNorm1d(self.visual_hidden_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.visual_hidden_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.trans_feature_dim),\n",
    "            ),\n",
    "            'pred': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.visual_feature_dim + self.spatial_hidden_dim, self.visual_hidden_dim),\n",
    "                #torch.nn.BatchNorm1d(self.visual_hidden_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.visual_hidden_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.trans_feature_dim),\n",
    "            ),\n",
    "            'obj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.visual_feature_dim, self.visual_hidden_dim),\n",
    "                #torch.nn.BatchNorm1d(self.visual_hidden_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.visual_hidden_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.trans_feature_dim),\n",
    "            ),\n",
    "        })\n",
    "        \n",
    "        self.fc_rois2 = ModuleDict({\n",
    "            'subj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.emb_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.emb_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "            'pred': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.emb_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.emb_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "            'obj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.emb_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.emb_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "        })\n",
    "        \n",
    "        # Triplet Loss (Cosine Distance)\n",
    "        self.triplet_loss = ModuleDict({\n",
    "            'subj': nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y), margin=0.2),\n",
    "            'pred': nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y), margin=0.2),\n",
    "            'obj': nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y), margin=0.2),\n",
    "        })\n",
    "\n",
    "    def _freeze_parameters(self, cfg):\n",
    "        if cfg.FREEZE_BACKBONE:\n",
    "            freeze_detectron = True\n",
    "            if freeze_detectron:\n",
    "                for param in self.detectron.parameters():\n",
    "                    param.requires_grad = False\n",
    "                    \n",
    "    def _load_detectron_chkpoints(self, cfg):\n",
    "        \"\"\"\n",
    "            Extension of __init__ for modules\n",
    "        \"\"\"\n",
    "        # Load Detectron2 Pre-Trained Weights\n",
    "        if cfg.VRD_RESNETS101_PRETRAINED_WEIGHTS is not None:\n",
    "            DetectionCheckpointer(self.detectron).load(\n",
    "                os.path.join(cfg.OUTPUT_DIR, 'model_final.pth')\n",
    "            )\n",
    "    \n",
    "    def _load_words_chkpoints(self, cfg):\n",
    "        \"\"\"\n",
    "            Extension of __init__ for modules\n",
    "        \"\"\"\n",
    "        return get_triples_features(cfg.DATASETS.TRAIN[0].split('_')[0])\n",
    "\n",
    "    \n",
    "    def _save_words_chkpoints(self, cfg):\n",
    "        torch.save(self.token_embeddings, self.triples_embeddings_path)\n",
    "    \n",
    "    def _get_bert_features(self, triples):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            triples: (Subj, Pred, Obj)\n",
    "        Return:\n",
    "            dict of [CLS, Subj, Pred, Obj, SEP] embeddings\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Load pre-trained model tokenizer (vocabulary)\n",
    "        marked_text = \"[CLS] \" + \" \".join(triples) + \" [SEP]\"\n",
    "        tokenized_text = self.tokenizer.tokenize(marked_text)\n",
    "\n",
    "        # Save the token split to average them later on\n",
    "        token_placements = defaultdict(list)\n",
    "        triples_temp = list(triples)\n",
    "        for i, tok in enumerate(tokenized_text):\n",
    "            stip_tok = tok.replace('#', '')\n",
    "            if (stip_tok in triples_temp[0]):\n",
    "                token_placements['subj'].append(i)\n",
    "                triples_temp[0] = triples_temp[0].replace(stip_tok, '')\n",
    "            elif (stip_tok in triples_temp[1]):\n",
    "                token_placements['pred'].append(i)\n",
    "                triples_temp[1] = triples_temp[1].replace(stip_tok, '')\n",
    "            elif (stip_tok in triples_temp[2]):\n",
    "                token_placements['obj'].append(i)\n",
    "                triples_temp[2] = triples_temp[2].replace(stip_tok, '')\n",
    "            elif (not tok == '[CLS]' and not tok == '[SEP]'):\n",
    "                print(tok, triples)\n",
    "\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)  # one sentence\n",
    "\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')\n",
    "        segments_tensors = torch.tensor([segments_ids]).to('cuda')\n",
    "\n",
    "        # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(tokens_tensor, segments_tensors)\n",
    "\n",
    "            # Evaluating the model will return a different number of objects based on\n",
    "            # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
    "            # becase we set `output_hidden_states = True`, the third item will be the\n",
    "            # hidden states from all layers. See the documentation for more details:\n",
    "            # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "            hidden_states = outputs[2]\n",
    "\n",
    "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "        token_embeddings.size()\n",
    "\n",
    "        # remove dimension 1\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "\n",
    "        # get token embeddings (list of token embeddings)\n",
    "        token_vecs_cat = []\n",
    "        for token in token_embeddings:\n",
    "            cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "            token_vecs_cat.append(cat_vec)\n",
    "        results['CLS'] = token_vecs_cat[0]\n",
    "        results['SEP'] = token_vecs_cat[-1]\n",
    "\n",
    "        # average the token embeddings for word that are splitted to get word embeddings\n",
    "        for key, val in token_placements.items():\n",
    "            results[key] = token_vecs_cat[val[0]]\n",
    "            for i in range(1, len(val)):\n",
    "                results[key] += token_vecs_cat[val[i]]\n",
    "            results[key] = results[key] / len(val)\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def _get_word_features(self, triples):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                triples: ([(subj, pred, obj)]) list of triple\n",
    "            Return:\n",
    "                resulting embeddings for subjs, preds, and objs\n",
    "        \"\"\"\n",
    "        subj_embeddings = []\n",
    "        pred_embeddings = []\n",
    "        obj_embeddings = []\n",
    "        \n",
    "        for subj, pred, obj in triples:\n",
    "            subj_cls_label = self.object_classes[subj]\n",
    "            pred_cls_label = self.predicate_classes[pred]\n",
    "            obj_cls_label = self.object_classes[obj]\n",
    "            triples_text = '-'.join((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "            query_embeddings = None\n",
    "            if (triples_text not in self.token_embeddings):\n",
    "                query_embeddings = self._get_bert_features((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "                self.token_embeddings[triples_text] = query_embeddings\n",
    "            else:\n",
    "                query_embeddings = self.token_embeddings[triples_text]\n",
    "            subj_embeddings.append(query_embeddings['subj'].to('cuda'))\n",
    "            pred_embeddings.append(query_embeddings['pred'].to('cuda'))\n",
    "            obj_embeddings.append(query_embeddings['obj'].to('cuda'))\n",
    "        \n",
    "        subj_embeddings = torch.stack(subj_embeddings).to('cuda')\n",
    "        pred_embeddings = torch.stack(pred_embeddings).to('cuda')\n",
    "        obj_embeddings = torch.stack(obj_embeddings).to('cuda')\n",
    "        \n",
    "        return subj_embeddings, pred_embeddings, obj_embeddings\n",
    "    \n",
    "    def _get_word_predicate_features(self, subj, obj):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                subj: subject index\n",
    "                obj: object index\n",
    "            Output:\n",
    "                A stack pred_embeddings for the above two subj and obj\n",
    "        \"\"\"\n",
    "        subj_cls_label = self.object_classes[subj]\n",
    "        obj_cls_label = self.object_classes[obj]\n",
    "\n",
    "        #predicate label\n",
    "        pred_embeddings = []\n",
    "        subj_embeddings = []\n",
    "        obj_embeddings = []\n",
    "        for pred, pred_cls_label in enumerate(self.predicate_classes):\n",
    "            triples_text = '-'.join((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "            query_embeddings = None\n",
    "            if (triples_text not in self.token_embeddings):\n",
    "                query_embeddings = self._get_bert_features((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "                self.token_embeddings[triples_text] = query_embeddings\n",
    "            else:\n",
    "                query_embeddings = self.token_embeddings[triples_text]\n",
    "            subj_embeddings.append(query_embeddings['subj'].to('cuda'))\n",
    "            pred_embeddings.append(query_embeddings['pred'].to('cuda'))\n",
    "            obj_embeddings.append(query_embeddings['obj'].to('cuda'))\n",
    "            \n",
    "        subj_embeddings = torch.stack(subj_embeddings).to('cuda')\n",
    "        pred_embeddings = torch.stack(pred_embeddings).to('cuda')\n",
    "        obj_embeddings = torch.stack(obj_embeddings).to('cuda')\n",
    "\n",
    "        return subj_embeddings, pred_embeddings, obj_embeddings\n",
    "    \n",
    "    def get_instances_prediction(self, data):\n",
    "        \"\"\"\n",
    "            This function gets the predicted instances from the object detector, \n",
    "            and is only relevant to relationship detection evaluation.\n",
    "            \n",
    "            Input:\n",
    "                data - data format for detectron2\n",
    "            Output:\n",
    "                instances - Instances object by the detectron2 that contain predicted instances\n",
    "        \"\"\"\n",
    "        instances = []\n",
    "        \n",
    "        images =  torch.unsqueeze(data['image'], axis=0).cuda().float()\n",
    "        images_list = ImageList(images, [(800, 800)])\n",
    "        self.detectron.eval()\n",
    "        features = self.detectron.backbone(images)\n",
    "        proposals, _ = self.detectron.proposal_generator(images_list, features)\n",
    "        instances, _ = self.detectron.roi_heads(images, features, proposals)\n",
    "        \n",
    "        return instances\n",
    "    \n",
    "    def enumerate_relationships_from_instances(self, instances):\n",
    "        \"\"\"\n",
    "            This function convert the format of instancesinto all possible combinations\n",
    "            of relationships in the detectron2 format\n",
    "            \n",
    "            Input:\n",
    "                instances - Instances object by detectron2\n",
    "            Output:\n",
    "                relationships - a dictionary of relationships in the vrdtransr input format\n",
    "        \"\"\"\n",
    "        enumerated_relationships = {\n",
    "            'subj_bboxes': [],\n",
    "            'obj_bboxes': [],\n",
    "            'union_bboxes': [],\n",
    "            'subj_classes': [],\n",
    "            'obj_classes': [],\n",
    "            'subj_scores': [],\n",
    "            'obj_scores': []\n",
    "        }\n",
    "        \n",
    "        # TO DO: implement to support batch images instead of a single image\n",
    "        instance_dict = instances[0].get_fields()\n",
    "        \n",
    "        pred_bboxes = instance_dict['pred_boxes']\n",
    "        pred_cls = instance_dict['pred_classes']\n",
    "        pred_scores = instance_dict['scores']\n",
    "        \n",
    "        for i in range(len(pred_cls)):\n",
    "            for j in range(i + 1, len(pred_cls)):\n",
    "                obj1_bbox = pred_bboxes[i].tensor.cpu().detach().numpy().astype(int)\n",
    "                obj2_bbox = pred_bboxes[j].tensor.cpu().detach().numpy().astype(int)\n",
    "                union_bbox = boxes_union(copy.deepcopy(obj1_bbox), copy.deepcopy(obj2_bbox))[0]\n",
    "                \n",
    "                obj1_label = pred_cls[i]\n",
    "                obj2_label = pred_cls[j]\n",
    "                \n",
    "                obj1_score = pred_scores[i]\n",
    "                obj2_score = pred_scores[j]\n",
    "                \n",
    "                enumerated_relationships['subj_bboxes'].append(obj1_bbox)\n",
    "                enumerated_relationships['obj_bboxes'].append(obj2_bbox)\n",
    "                enumerated_relationships['union_bboxes'].append(union_bbox)\n",
    "                enumerated_relationships['subj_classes'].append(obj1_label)\n",
    "                enumerated_relationships['obj_classes'].append(obj2_label)\n",
    "                enumerated_relationships['subj_scores'].append(obj1_score)\n",
    "                enumerated_relationships['obj_scores'].append(obj2_score)\n",
    "                \n",
    "                \n",
    "                enumerated_relationships['subj_bboxes'].append(obj2_bbox)\n",
    "                enumerated_relationships['obj_bboxes'].append(obj1_bbox)\n",
    "                enumerated_relationships['union_bboxes'].append(union_bbox)\n",
    "                enumerated_relationships['subj_classes'].append(obj2_label)\n",
    "                enumerated_relationships['obj_classes'].append(obj1_label)\n",
    "                enumerated_relationships['subj_scores'].append(obj2_score)\n",
    "                enumerated_relationships['obj_scores'].append(obj1_score)\n",
    "                \n",
    "        return enumerated_relationships\n",
    "    \n",
    "    def get_predicted_relationships(self, data):\n",
    "        \"\"\"\n",
    "            This function gets the predicted relationships from the object detector, \n",
    "            and is only relevant to relationship detection evaluation.\n",
    "            \n",
    "            Input:\n",
    "                data - data format for detectron2\n",
    "            Output:\n",
    "                relationships - Instances object by the detectron2 that contain predicted instances\n",
    "        \"\"\"\n",
    "        #get predicted objects in the given image\n",
    "        instances = self.get_instances_prediction(data)\n",
    "        \n",
    "        #enumerate the relationships with the predicted instances (bounding boxes and labels)\n",
    "        relationships = self.enumerate_relationships_from_instances(instances)\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "    \n",
    "    def get_predicate_distances(self, data, is_rel_eval=False):\n",
    "        \"\"\"\n",
    "            Predict model's prediction based on the given data.\n",
    "            Return the prediction predicate, visual relationship, and phrase (to be implemented)\n",
    "            Input:\n",
    "                data: vrdtranse input format\n",
    "                is_rel_eval: boolean whether the evaluation is predicate detection or relationship detection\n",
    "        \"\"\"\n",
    "        relationships = data[\"relationships\"].copy()\n",
    "        all_predicate_distances = [] # for each (subj, obj) pair, we get a set of distances\n",
    "        all_subject_distances = [] #distance between subject visual and language\n",
    "        all_object_distances = []\n",
    "        all_predicate_subtract_distances = []\n",
    "        all_transe_visual_feature = []\n",
    "        all_transe_language_feature = []\n",
    "        all_transe_distance_feature = []\n",
    "        \n",
    "        #forward features for gt_visual and gt_text\n",
    "        fc_features = self.forward(data, None, get_fc_features=True)\n",
    "        \n",
    "        rel_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for subj, obj in zip(relationships['subj_classes'], relationships['obj_classes']):\n",
    "                #compute all potential predicate embeddings for the (subj, obj) pair\n",
    "                all_subj_embeddings, all_predicate_embeddings, all_object_embeddings = self._get_word_predicate_features(subj=subj, obj=obj)\n",
    "\n",
    "                #languge features\n",
    "                fc_all_subject_embeddings = self.fc_word[\"subj\"](all_subj_embeddings)\n",
    "                fc_all_predicate_embeddings = self.fc_word[\"pred\"](all_predicate_embeddings)\n",
    "                fc_all_object_embeddings = self.fc_word[\"obj\"](all_object_embeddings)\n",
    "                fc_pred_transe_language_feature = fc_all_subject_embeddings + fc_all_predicate_embeddings - fc_all_object_embeddings\n",
    "\n",
    "                #visual features\n",
    "                fc_pred_visual_feature = fc_features[\"visual\"][\"pred\"][rel_cnt,:]\n",
    "                fc_pred_subtract_visual_feature = fc_features[\"visual\"][\"obj\"][rel_cnt,:] - fc_features[\"visual\"][\"subj\"][rel_cnt,:]\n",
    "                fc_pred_transe_visual_feature = fc_features[\"visual\"][\"subj\"][rel_cnt,:] + fc_features[\"visual\"][\"pred\"][rel_cnt,:] - fc_features[\"visual\"][\"obj\"][rel_cnt,:]\n",
    "                fc_subj_visual_feature = fc_features[\"visual\"][\"subj\"][rel_cnt,:]\n",
    "                fc_obj_visual_feature = fc_features[\"visual\"][\"obj\"][rel_cnt,:]\n",
    "                \n",
    "                #compute distance between the fc_features[\"visual\"][\"pred\"] and fc_predicate_embeddings to get top n\n",
    "                pdist = lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    "                distance = []\n",
    "                distance_subject = []\n",
    "                distance_object = []\n",
    "                distance_subtract = []\n",
    "                distance_transe = []\n",
    "                \n",
    "                for subj_emb, pred_emb, obj_emb, pred_transe_emb in zip(fc_all_subject_embeddings, fc_all_predicate_embeddings, fc_all_object_embeddings, fc_pred_transe_language_feature):\n",
    "                    distance.append(pdist(torch.unsqueeze(fc_pred_visual_feature, dim=0), torch.unsqueeze(pred_emb, dim=0)))\n",
    "                    distance_subtract.append(pdist(torch.unsqueeze(fc_pred_subtract_visual_feature, dim=0), torch.unsqueeze(obj_emb - subj_emb, dim=0)))\n",
    "                    distance_transe.append(pdist(torch.unsqueeze(fc_pred_transe_visual_feature, dim=0), torch.unsqueeze(pred_transe_emb, dim=0)))\n",
    "                    distance_subject.append(pdist(torch.unsqueeze(fc_subj_visual_feature, dim=0), torch.unsqueeze(subj_emb, dim=0)))\n",
    "                    distance_object.append(pdist(torch.unsqueeze(fc_obj_visual_feature, dim=0), torch.unsqueeze(obj_emb, dim=0)))\n",
    "                     \n",
    "                    \n",
    "                # add set of distances to the given relationship\n",
    "                all_predicate_distances.append(distance)\n",
    "                all_subject_distances.append(distance_subject)\n",
    "                all_object_distances.append(distance_object)\n",
    "                all_predicate_subtract_distances.append(distance_subtract)\n",
    "                all_transe_visual_feature.append(fc_pred_transe_visual_feature)\n",
    "                all_transe_language_feature.append(fc_pred_transe_language_feature)\n",
    "                all_transe_distance_feature.append(distance_transe)\n",
    "                rel_cnt += 1\n",
    "        \n",
    "        if (is_rel_eval):\n",
    "            return all_predicate_distances, all_subject_distances, all_object_distances, all_predicate_subtract_distances, all_transe_distance_feature\n",
    "                \n",
    "        return all_predicate_distances, all_predicate_subtract_distances, all_transe_distance_feature\n",
    "    \n",
    "    def get_triples_distance(self, data):\n",
    "        \"\"\"\n",
    "            POINTER 3 [currently redundant]\n",
    "            Predicts the distance between subjects, predicates, objects, and subtracted distance between subject and object\n",
    "            \n",
    "            Only relevant for visual relationship detection task (evaluation)\n",
    "        \"\"\"\n",
    "        relationships = data[\"relationships\"].copy()\n",
    "        all_predicate_distances = []\n",
    "        all_subject_distances = []\n",
    "        all_object_distances = []\n",
    "        all_predicate_subtract_distances = []\n",
    "        \n",
    "        rel_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for subj, obj in zip(relationships['subj_classes'], relationships['obj_classes']):\n",
    "                #compute all potential predicate embeddings for the (subj, obj) pair\n",
    "                all_subj_embeddings, all_predicate_embeddings, all_object_embeddings = self._get_word_predicate_features(subj=subj, obj=obj)\n",
    "\n",
    "                #languge features\n",
    "                fc_all_subject_embeddings = self.fc_word[\"subj\"](all_subj_embeddings)\n",
    "                fc_all_predicate_embeddings = self.fc_word[\"pred\"](all_predicate_embeddings)\n",
    "                fc_all_object_embeddings = self.fc_word[\"obj\"](all_object_embeddings)\n",
    "                fc_pred_transe_language_feature = fc_all_subject_embeddings - fc_all_object_embeddings\n",
    "\n",
    "                #visual features\n",
    "                fc_pred_visual_feature = fc_features[\"visual\"][\"pred\"][rel_cnt,:]\n",
    "                fc_pred_subtract_visual_feature = fc_features[\"visual\"][\"obj\"][rel_cnt,:] - fc_features[\"visual\"][\"subj\"][rel_cnt,:]\n",
    "                fc_pred_transe_visual_feature = fc_features[\"visual\"][\"subj\"][rel_cnt,:] + fc_features[\"visual\"][\"pred\"][rel_cnt,:] - fc_features[\"visual\"][\"obj\"][rel_cnt,:]\n",
    "                \n",
    "                #compute distance between the fc_features[\"visual\"][\"pred\"] and fc_predicate_embeddings to get top n\n",
    "                pdist = lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    "                distance = []\n",
    "                distance_subtract = []\n",
    "                distance_transe = []\n",
    "                \n",
    "                for subj_emb, pred_emb, obj_emb, pred_transe_emb in zip(fc_all_subject_embeddings, fc_all_predicate_embeddings, fc_all_object_embeddings, fc_pred_transe_language_feature):\n",
    "                    distance.append(pdist(torch.unsqueeze(fc_pred_visual_feature, dim=0), torch.unsqueeze(pred_emb, dim=0)))\n",
    "                    distance_subtract.append(pdist(torch.unsqueeze(fc_pred_subtract_visual_feature, dim=0), torch.unsqueeze(obj_emb - subj_emb, dim=0)))\n",
    "                    distance_transe.append(pdist(torch.unsqueeze(fc_pred_transe_visual_feature, dim=0), torch.unsqueeze(pred_transe_emb, dim=0)))\n",
    "\n",
    "                # add set of distances to the given relationship\n",
    "                all_predicate_distances.append(distance)\n",
    "                all_predicate_subtract_distances.append(distance_subtract)\n",
    "                all_transe_visual_feature.append(fc_pred_transe_visual_feature)\n",
    "                all_transe_language_feature.append(fc_pred_transe_language_feature)\n",
    "                all_transe_distance_feature.append(distance_transe)\n",
    "                rel_cnt += 1\n",
    "        \n",
    "        fc_features = self.forward(data, None, get_fc_features=True)\n",
    "\n",
    "        \n",
    "        return all_predicate_distances, all_subject_distances, all_object_distances, all_predicate_subtract_distances\n",
    "    \n",
    "    def _get_word_object_features(self, subj, pred):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                subj: subject index\n",
    "                pred: predicate index\n",
    "            Output:\n",
    "                A stack pred_embeddings for the above two subj and obj\n",
    "        \"\"\"\n",
    "        subj_cls_label = self.object_classes[subj]\n",
    "        pred_cls_label = self.predicate_classes[pred]\n",
    "\n",
    "        #predicate label\n",
    "        pred_embeddings = []\n",
    "        subj_embeddings = []\n",
    "        obj_embeddings = []\n",
    "        for obj, obj_cls_label in enumerate(self.object_classes):\n",
    "            triples_text = '-'.join((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "            query_embeddings = None\n",
    "            if (triples_text not in self.token_embeddings):\n",
    "                query_embeddings = self._get_bert_features((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "                self.token_embeddings[triples_text] = query_embeddings\n",
    "            else:\n",
    "                query_embeddings = self.token_embeddings[triples_text]\n",
    "            subj_embeddings.append(query_embeddings['subj'].to('cuda'))\n",
    "            pred_embeddings.append(query_embeddings['pred'].to('cuda'))\n",
    "            obj_embeddings.append(query_embeddings['obj'].to('cuda'))\n",
    "            \n",
    "        subj_embeddings = torch.stack(subj_embeddings).to('cuda')\n",
    "        pred_embeddings = torch.stack(pred_embeddings).to('cuda')\n",
    "        obj_embeddings = torch.stack(obj_embeddings).to('cuda')\n",
    "\n",
    "        return subj_embeddings, pred_embeddings, obj_embeddings\n",
    "    \n",
    "    def get_object_distances(self, data):\n",
    "        \"\"\"\n",
    "            Predict model's prediction based on the given data.\n",
    "            Return the prediction predicate, phrase (to be implemented), and visual relationship (to be implemented)\n",
    "        \"\"\"\n",
    "        relationships = data[\"relationships\"]\n",
    "        all_object_distances = [] # for each (subj, pred) pair, we get a set of distances\n",
    "        all_object_add_distances = [] \n",
    "\n",
    "        #forward features for gt_visual and gt_text\n",
    "        fc_features = self.forward(data, None, get_fc_features=True, obfuscate_object=True)\n",
    "\n",
    "        rel_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for subj, pred in zip(relationships['subj_classes'], relationships['pred_classes']):\n",
    "                #compute all potential predicate embeddings for the (subj, obj) pair\n",
    "                all_subj_embeddings, all_predicate_embeddings, all_object_embeddings = self._get_word_object_features(subj=subj, pred=pred)\n",
    "\n",
    "                #languge features\n",
    "                fc_all_subject_embeddings = self.fc_word[\"subj\"](all_subj_embeddings)\n",
    "                fc_all_predicate_embeddings = self.fc_word[\"pred\"](all_predicate_embeddings)\n",
    "                fc_all_object_embeddings = self.fc_word[\"obj\"](all_object_embeddings)\n",
    "\n",
    "                #visual features\n",
    "                fc_obj_add_visual_feature = fc_features[\"visual\"][\"subj\"][rel_cnt,:] + fc_features[\"visual\"][\"pred\"][rel_cnt,:]\n",
    "\n",
    "                #compute distance between the fc_features[\"visual\"][\"pred\"] and fc_predicate_embeddings to get top n\n",
    "                pdist = lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    "                distance_add = []\n",
    "                distance = []\n",
    "                \n",
    "                for subj_emb, pred_emb, obj_emb in zip(fc_all_subject_embeddings, fc_all_predicate_embeddings, fc_all_object_embeddings):\n",
    "                    distance.append(pdist(torch.unsqueeze(fc_obj_add_visual_feature, dim=0), torch.unsqueeze(obj_emb, dim=0)))\n",
    "                    distance_add.append(pdist(torch.unsqueeze(fc_obj_add_visual_feature, dim=0), torch.unsqueeze(subj_emb + pred_emb, dim=0)))\n",
    "                    \n",
    "                # add set of distances to the given relationship\n",
    "                all_object_distances.append(distance)\n",
    "                all_object_add_distances.append(distance_add)\n",
    "                rel_cnt += 1\n",
    "\n",
    "        return all_object_distances, all_object_add_distances                \n",
    "    \n",
    "    def _get_prediced_bboxes(self, data):\n",
    "        \n",
    "        data['height'] = 800\n",
    "        data['width'] = 800\n",
    "\n",
    "        self.detectron.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.detectron([data])\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def _get_roi_features(self, images, box_lists):\n",
    "        \"\"\"\n",
    "            Get image features from the backbone network\n",
    "            Input:\n",
    "                images: (ImageList.from_tensors) with dimension (C,W,H)\n",
    "                box_lists: A list of N boxes\n",
    "            Return:\n",
    "                features:[N, 7*7*256]\n",
    "        \"\"\"\n",
    "        N = len(box_lists[0])\n",
    "        \n",
    "        cfg = self.cfg\n",
    "        feature_maps = self.backbone(images)\n",
    "        feature_maps = [feature_maps['p{}'.format(i)] for i in range(2,6)]\n",
    "        regions_feature = self.pooler(feature_maps, box_lists)\n",
    "        return regions_feature.reshape((N, self.visual_feature_dim))\n",
    "\n",
    "    def get_unrelated_instance(self, bbox, cls, gt_tuple_boxes, gt_classes, memo, other_memo=None):\n",
    "        negative_example = {}\n",
    "\n",
    "        tuple_bbox = tuple(bbox)\n",
    "        for i, neg_bbox in enumerate(gt_tuple_boxes):\n",
    "            if (other_memo is not None and neg_bbox not in other_memo and neg_bbox not in memo):\n",
    "                negative_example = {\n",
    "                    'bbox': torch.from_numpy(np.asarray(neg_bbox)).float().cuda(), #convert to tensor float\n",
    "                    'cls': gt_classes[i]\n",
    "                }\n",
    "                return negative_example\n",
    "            elif (neg_bbox != tuple_bbox and neg_bbox not in memo):\n",
    "                negative_example = {\n",
    "                    'bbox': torch.from_numpy(np.asarray(neg_bbox)).float().cuda(), #convert to tensor float\n",
    "                    'cls': gt_classes[i]\n",
    "                }\n",
    "                return negative_example\n",
    "\n",
    "        return negative_example\n",
    "    \n",
    "    def generate_negative_examples(self, data, K=3):\n",
    "        \"\"\"\n",
    "            for each triple relation in data, generate K negative examples\n",
    "\n",
    "            return: [{\n",
    "                'subj_bboxes': Boxes(tensor[[X,Y,X2,Y2],...])),\n",
    "                'union_bboxes': Boxes(tensor(([[X,Y,X2,Y2],...])),\n",
    "                'obj_bboxes': Boxes(tensor([[X,Y,X2,Y2],...])),\n",
    "                'subj_classes': [cls_subj,...],\n",
    "                'pred_classes': [cls_pred,...],\n",
    "                'obj_classes': [cls_obj,...]\n",
    "            }]\n",
    "        \"\"\"\n",
    "        boxes = data['instances'].get_fields()['gt_boxes']\n",
    "        gt_tuple_boxes = [tuple([ele.item() for ele in box]) for box in boxes] #convert ground truth boxes into tuples\n",
    "\n",
    "        classes = data['instances'].get_fields()['gt_classes']\n",
    "        gt_classes = [int(item) for item in classes]\n",
    "\n",
    "        # shuffle to random select first K\n",
    "        zip_gt_data = list(zip(gt_tuple_boxes, gt_classes))\n",
    "        random.shuffle(zip_gt_data)\n",
    "        gt_tuple_boxes, gt_classes = zip(*zip_gt_data)\n",
    "        \n",
    "        relationships = data['relationships']\n",
    "        subj_boxes = relationships['subj_bboxes']\n",
    "        union_boxes = relationships['union_bboxes']\n",
    "        obj_boxes = relationships['obj_bboxes']\n",
    "        subj_classes = relationships['subj_classes']\n",
    "        pred_classes = relationships['pred_classes']\n",
    "        obj_classes = relationships['obj_classes']\n",
    "        \n",
    "        #generate K negative examples\n",
    "        neg_examples = []\n",
    "        memo_subj = set()\n",
    "        memo_obj = set()\n",
    "        existed_predicates = dict(zip([tuple(item) for item in data['relationships']['union_bboxes']], data['relationships']['pred_classes']))\n",
    "        \n",
    "        for i in range(min(len(gt_tuple_boxes)-1, K)):\n",
    "            neg_ex = defaultdict(list)\n",
    "            if (len(memo_subj) == len(gt_tuple_boxes)-1 or len(memo_obj) == len(gt_tuple_boxes)-1):\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                for j in range(len(subj_boxes)): #iterate through the relationships in the image\n",
    "                    #subj\n",
    "                    subj_box = subj_boxes[j]\n",
    "                    subj_cls = subj_classes[j]\n",
    "\n",
    "                    #ISSUE: in the case where the number of object in the image is actually smaller than K, it is kind useless\n",
    "                    unrelated_subj_instance = self.get_unrelated_instance(subj_box[0], subj_cls, gt_tuple_boxes, gt_classes, memo=memo_subj)\n",
    "                    neg_ex['subj_bboxes'].append(unrelated_subj_instance['bbox'])\n",
    "                    neg_ex['subj_classes'].append(unrelated_subj_instance['cls'])\n",
    "\n",
    "                    #obj\n",
    "                    obj_box = obj_boxes[j]\n",
    "                    obj_cls = obj_classes[j]\n",
    "                    other_memo = set()\n",
    "                    other_memo.add(tuple(subj_box[0]))\n",
    "                    unrelated_obj_instance = self.get_unrelated_instance(obj_box[0], obj_cls, gt_tuple_boxes, gt_classes, memo=memo_obj, other_memo=other_memo)\n",
    "                    neg_ex['obj_bboxes'].append(unrelated_obj_instance['bbox'])\n",
    "                    neg_ex['obj_classes'].append(unrelated_obj_instance['cls'])\n",
    "                    \n",
    "                    #pred\n",
    "                    new_union_box = boxes_union(copy.deepcopy(unrelated_subj_instance['bbox'].reshape(1,4).to('cpu')),\n",
    "                                                copy.deepcopy(unrelated_obj_instance['bbox'].reshape(1,4).to('cpu')))[0]\n",
    "                    new_predicate_class = len(self.predicate_classes) - 1\n",
    "                    if (tuple(new_union_box) in existed_predicates):\n",
    "                        new_predicate_class = existed_predicates[tuple(new_union_box)]\n",
    "                    neg_ex['union_bboxes'].append(torch.from_numpy(np.asarray(new_union_box)).float().cuda())\n",
    "                    neg_ex['pred_classes'].append(new_predicate_class)\n",
    "\n",
    "                for j in range(len(subj_boxes)):\n",
    "                    memo_subj.add(tuple(neg_ex['subj_bboxes'][j].tolist()))\n",
    "                    memo_obj.add(tuple(neg_ex['obj_bboxes'][j].tolist()))\n",
    "            except:\n",
    "                break\n",
    "\n",
    "            #stack the bounding boxes\n",
    "            neg_ex['subj_bboxes'] = Boxes(torch.stack(neg_ex['subj_bboxes']))\n",
    "            neg_ex['obj_bboxes'] = Boxes(torch.stack(neg_ex['obj_bboxes']))\n",
    "            neg_ex['union_bboxes'] = Boxes(torch.stack(neg_ex['union_bboxes']))\n",
    "            \n",
    "            #append to memory\n",
    "            neg_examples.append(neg_ex)\n",
    "            \n",
    "        return neg_examples\n",
    "\n",
    "    def get_spatial_features(self, relationships, is_negative=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: see data definition in forward function\n",
    "        Return:\n",
    "            spatial_features: a tensor of spatial features containing coordinates of the bounding box\n",
    "        \"\"\"\n",
    "        def spatial_delta(entity1, entity2):\n",
    "            \"\"\"\n",
    "                entity1, entity2: [X,Y,X,Y]\n",
    "            \"\"\"\n",
    "            \n",
    "            width1, height1 = entity1[2] - entity1[0], entity1[3] - entity1[1]\n",
    "            width2, height2 = entity2[2] - entity2[0], entity2[3] - entity2[1]\n",
    "            \n",
    "            delta_feat = [\n",
    "                (entity1[0] - entity2[0])/width2,\n",
    "                (entity1[1] - entity2[1])/height2,\n",
    "                np.log(width1/width2),\n",
    "                np.log(height1/height2),\n",
    "            ]\n",
    "            return delta_feat\n",
    "        \n",
    "        def spatial_coordinates(entity):\n",
    "            \"\"\"\n",
    "                entity: [X,Y,X,Y]\n",
    "            \"\"\"\n",
    "            width, height = entity[2] - entity[0], entity[3] - entity[1]\n",
    "            coordinate_feat = [\n",
    "                entity[0]/800,\n",
    "                entity[1]/800,\n",
    "                entity[2]/800,\n",
    "                entity[3]/800,\n",
    "                width*height/800*800\n",
    "            ]\n",
    "            return coordinate_feat\n",
    "        \n",
    "        spatial_features = []\n",
    "        #iterate through every relationship pair and construct an array of spatial feature    \n",
    "        for subj_bbox, obj_bbox, union_bbox in zip(relationships['subj_bboxes'], relationships['obj_bboxes'], relationships['union_bboxes']):\n",
    "            feat = []\n",
    "            #XYXY\n",
    "            if (is_negative):\n",
    "                feat.extend(spatial_delta(subj_bbox.cpu(), obj_bbox.cpu()))\n",
    "                feat.extend(spatial_delta(subj_bbox.cpu(), union_bbox.cpu()))\n",
    "                feat.extend(spatial_delta(union_bbox.cpu(), obj_bbox.cpu()))\n",
    "                feat.extend(spatial_coordinates(subj_bbox.cpu()))\n",
    "                feat.extend(spatial_coordinates(obj_bbox.cpu()))            \n",
    "            else:\n",
    "                feat.extend(spatial_delta(subj_bbox[0], obj_bbox[0]))\n",
    "                feat.extend(spatial_delta(subj_bbox[0], union_bbox))\n",
    "                feat.extend(spatial_delta(union_bbox, obj_bbox[0]))\n",
    "                feat.extend(spatial_coordinates(subj_bbox[0]))\n",
    "                feat.extend(spatial_coordinates(obj_bbox[0]))\n",
    "            \n",
    "            spatial_features.append(torch.from_numpy(np.asarray(feat)).float().cuda())\n",
    "        \n",
    "        return torch.stack(spatial_features)\n",
    "    \n",
    "    def eval_phrase_detection(self, data):\n",
    "        #add batch dim to image\n",
    "        image = torch.unsqueeze(data['image'], axis=0).cuda()\n",
    "        \n",
    "        #get prediction bounding boxes\n",
    "        output = self._get_prediced_bboxes(data)\n",
    "        bboxes = output[0]['instances'].get_fields()['pred_boxes']\n",
    "        bboxes_features = self._get_roi_features(image.float(), box_lists=[bboxes])\n",
    "        \n",
    "        bboxes_classes = output[0]['instances'].get_fields()['pred_classes']\n",
    "        conf_score = output[0]['instances'].get_fields()['scores']\n",
    "        #iterate through every pairs and compute score\n",
    "        return output, bboxes_features\n",
    "    \n",
    "    def forward(self, data, negative_examples, get_fc_features=False, obfuscate_object=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: {\n",
    "                    #Detectron\n",
    "                    'file_name': os.path.join(self.images_dir, img_name),\n",
    "                    'image_id': int(img_name.split('.')[0]),\n",
    "                    'annotations': list(unique_objects.values()),\n",
    "\n",
    "                    #Relationships\n",
    "                    'relationships': {\n",
    "                        'subj_bboxes': subj_bboxes,\n",
    "                        'obj_bboxes': obj_bboxes,\n",
    "                        'union_bboxes': union_bboxes,\n",
    "\n",
    "                        'subj_classes': subj_classes,\n",
    "                        'pred_classes': pred_classes,\n",
    "                        'obj_classes': obj_classes,\n",
    "                    }\n",
    "                }\n",
    "            context: #any external data (not implemented as of current)\n",
    "        \"\"\"\n",
    "        image = torch.unsqueeze(data['image'], axis=0).cuda()\n",
    "\n",
    "        relationships = data['relationships']\n",
    "        \n",
    "        subj_bboxes = Boxes([bbox[0] for bbox in relationships['subj_bboxes']]).to('cuda')\n",
    "        if (obfuscate_object):\n",
    "            union_bboxes = subj_bboxes\n",
    "        else:\n",
    "            union_bboxes = Boxes([bbox for bbox in relationships['union_bboxes']]).to('cuda')\n",
    "        obj_bboxes = Boxes([bbox[0] for bbox in relationships['obj_bboxes']]).to('cuda')\n",
    "\n",
    "        \n",
    "        #ground_truth features\n",
    "        gt_features = {\"visual\": {}, \"textual\": {}}\n",
    "\n",
    "        #fully connected features\n",
    "        fc_features = {\"visual\": {}, \"textual\": {}}\n",
    "\n",
    "\n",
    "        #spatial features\n",
    "        gt_spatial_features = self.fc_spatial(self.get_spatial_features(relationships))\n",
    "        \n",
    "        #extract visual features from backbone and ROIPool for n relations in the image\n",
    "        gt_features[\"visual\"][\"subj\"] = self._get_roi_features(image.float(), box_lists=[subj_bboxes])\n",
    "        gt_features[\"visual\"][\"pred\"] = self._get_roi_features(image.float(), box_lists=[union_bboxes])\n",
    "        gt_features[\"visual\"][\"pred\"] = torch.cat((gt_features[\"visual\"][\"pred\"], gt_spatial_features), 1)\n",
    "        gt_features[\"visual\"][\"obj\"] = self._get_roi_features(image.float(), box_lists=[obj_bboxes])\n",
    "        \n",
    "        #fc visual (rois1 and rois2)\n",
    "        fc_features[\"visual\"][\"subj\"] = self.fc_rois[\"subj\"](gt_features[\"visual\"][\"subj\"])\n",
    "        fc_features[\"visual\"][\"pred\"] = self.fc_rois[\"pred\"](gt_features[\"visual\"][\"pred\"])\n",
    "        fc_features[\"visual\"][\"obj\"] = self.fc_rois[\"obj\"](gt_features[\"visual\"][\"obj\"])\n",
    "\n",
    "        fc_features[\"visual\"][\"subj\"] = self.fc_rois2[\"subj\"](fc_features[\"visual\"][\"subj\"])\n",
    "        fc_features[\"visual\"][\"pred\"] = self.fc_rois2[\"pred\"](fc_features[\"visual\"][\"pred\"])\n",
    "        fc_features[\"visual\"][\"obj\"] = self.fc_rois2[\"obj\"](fc_features[\"visual\"][\"obj\"])\n",
    "\n",
    "        \n",
    "        if (get_fc_features):\n",
    "            return fc_features\n",
    "    \n",
    "        #extract word embeddings for n examples in the image\n",
    "        word_embeddings = self._get_word_features(list(zip(relationships['subj_classes'],\n",
    "                                                           relationships['pred_classes'],\n",
    "                                                           relationships['obj_classes'])))\n",
    "        gt_features[\"textual\"][\"subj\"] = word_embeddings[0]\n",
    "        gt_features[\"textual\"][\"pred\"] = word_embeddings[1]\n",
    "        gt_features[\"textual\"][\"obj\"] = word_embeddings[2]\n",
    "\n",
    "\n",
    "        #fc word\n",
    "        fc_features[\"textual\"][\"subj\"] = self.fc_word[\"subj\"](gt_features[\"textual\"][\"subj\"])\n",
    "        fc_features[\"textual\"][\"pred\"] = self.fc_word[\"pred\"](gt_features[\"textual\"][\"pred\"])\n",
    "        fc_features[\"textual\"][\"obj\"] = self.fc_word[\"obj\"](gt_features[\"textual\"][\"obj\"])\n",
    "\n",
    "        #Visual and Language Consistency losses triplet_loss(anchor, positive, negative)\n",
    "        triplet_losses = {\"subj\": None, \"pred\": None, \"obj\": None} #, \"transr\": None}\n",
    "        \n",
    "        #NEGATIVE EXAMPLES\n",
    "        for neg_relationships in negative_examples:\n",
    "\n",
    "            neg_subj_boxes = neg_relationships['subj_bboxes']\n",
    "            neg_union_boxes = neg_relationships['union_bboxes']\n",
    "            neg_obj_boxes = neg_relationships['obj_bboxes']\n",
    "\n",
    "            #dictionary to store gt_features\n",
    "            neg_features = {\"visual\": {}, \"textual\": {}}\n",
    "\n",
    "            #spatial features\n",
    "            neg_spatial_features = self.fc_spatial(self.get_spatial_features(neg_relationships, is_negative=True))\n",
    "\n",
    "            #extract visual features from backbone and ROIPool for n relations in the image\n",
    "            neg_features[\"visual\"][\"subj\"] = self._get_roi_features(image.float(), box_lists=[neg_subj_boxes])\n",
    "            neg_features[\"visual\"][\"pred\"] = self._get_roi_features(image.float(), box_lists=[neg_union_boxes])\n",
    "            neg_features[\"visual\"][\"pred\"] = torch.cat((neg_features[\"visual\"][\"pred\"], neg_spatial_features), 1)\n",
    "            neg_features[\"visual\"][\"obj\"] = self._get_roi_features(image.float(), box_lists=[neg_obj_boxes])\n",
    "\n",
    "            #extract word embeddings for n examples in the image\n",
    "            neg_word_embeddings = self._get_word_features(list(zip(neg_relationships['subj_classes'],\n",
    "                                                               neg_relationships['pred_classes'],\n",
    "                                                               neg_relationships['obj_classes'])))\n",
    "            neg_features[\"textual\"][\"subj\"] = neg_word_embeddings[0]\n",
    "            neg_features[\"textual\"][\"pred\"] = neg_word_embeddings[1]\n",
    "            neg_features[\"textual\"][\"obj\"] = neg_word_embeddings[2]\n",
    "\n",
    "            #neg fc visual\n",
    "            neg_fc_features = {\"visual\": {}, \"textual\": {}}\n",
    "            neg_fc_features[\"visual\"][\"subj\"] = self.fc_rois[\"subj\"](neg_features[\"visual\"][\"subj\"])\n",
    "            neg_fc_features[\"visual\"][\"pred\"] = self.fc_rois[\"pred\"](neg_features[\"visual\"][\"pred\"])\n",
    "            neg_fc_features[\"visual\"][\"obj\"] = self.fc_rois[\"obj\"](neg_features[\"visual\"][\"obj\"])\n",
    "\n",
    "            neg_fc_features[\"visual\"][\"subj\"] = self.fc_rois2[\"subj\"](neg_fc_features[\"visual\"][\"subj\"])\n",
    "            neg_fc_features[\"visual\"][\"pred\"] = self.fc_rois2[\"pred\"](neg_fc_features[\"visual\"][\"pred\"])\n",
    "            neg_fc_features[\"visual\"][\"obj\"] = self.fc_rois2[\"obj\"](neg_fc_features[\"visual\"][\"obj\"])\n",
    "            \n",
    "            #neg fc word\n",
    "            neg_fc_features[\"textual\"][\"subj\"] = self.fc_word[\"subj\"](neg_features[\"textual\"][\"subj\"])\n",
    "            neg_fc_features[\"textual\"][\"pred\"] = self.fc_word[\"pred\"](neg_features[\"textual\"][\"pred\"])\n",
    "            neg_fc_features[\"textual\"][\"obj\"] = self.fc_word[\"obj\"](neg_features[\"textual\"][\"obj\"])\n",
    "\n",
    "            if (triplet_losses[\"subj\"] is None):\n",
    "                triplet_losses[\"subj\"] = self.triplet_loss[\"subj\"](fc_features[\"visual\"][\"subj\"], fc_features[\"textual\"][\"subj\"], neg_fc_features[\"visual\"][\"subj\"]) + self.triplet_loss[\"subj\"](fc_features[\"textual\"][\"subj\"], fc_features[\"visual\"][\"subj\"], neg_fc_features[\"textual\"][\"subj\"])\n",
    "                triplet_losses[\"pred\"] = self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"pred\"], fc_features[\"textual\"][\"pred\"], neg_fc_features[\"visual\"][\"pred\"]) + self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"pred\"], fc_features[\"visual\"][\"pred\"], neg_fc_features[\"textual\"][\"pred\"])\n",
    "                triplet_losses[\"obj\"] = self.triplet_loss[\"obj\"](fc_features[\"visual\"][\"obj\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"visual\"][\"obj\"]) + self.triplet_loss[\"obj\"](fc_features[\"textual\"][\"obj\"],fc_features[\"visual\"][\"obj\"],neg_fc_features[\"textual\"][\"obj\"])\n",
    "                \n",
    "                triplet_losses[\"transr\"] = self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"subj\"] + fc_features[\"visual\"][\"pred\"], fc_features[\"visual\"][\"obj\"], neg_fc_features[\"visual\"][\"subj\"] + neg_fc_features[\"visual\"][\"pred\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"subj\"] + fc_features[\"textual\"][\"pred\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"textual\"][\"subj\"] + neg_fc_features[\"textual\"][\"pred\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"subj\"] + fc_features[\"visual\"][\"pred\"], fc_features[\"visual\"][\"obj\"], neg_fc_features[\"visual\"][\"obj\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"subj\"] + fc_features[\"textual\"][\"pred\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"textual\"][\"obj\"])\n",
    "\n",
    "            else:\n",
    "                triplet_losses[\"subj\"] += self.triplet_loss[\"subj\"](fc_features[\"visual\"][\"subj\"], fc_features[\"textual\"][\"subj\"], neg_fc_features[\"visual\"][\"subj\"]) + self.triplet_loss[\"subj\"](fc_features[\"textual\"][\"subj\"],fc_features[\"visual\"][\"subj\"], neg_fc_features[\"textual\"][\"subj\"])\n",
    "                triplet_losses[\"pred\"] += self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"pred\"], fc_features[\"textual\"][\"pred\"], neg_fc_features[\"visual\"][\"pred\"]) + self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"pred\"],fc_features[\"visual\"][\"pred\"],neg_fc_features[\"textual\"][\"pred\"])\n",
    "                triplet_losses[\"obj\"] += self.triplet_loss[\"obj\"](fc_features[\"visual\"][\"obj\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"visual\"][\"obj\"]) + self.triplet_loss[\"obj\"](fc_features[\"textual\"][\"obj\"],fc_features[\"visual\"][\"obj\"],neg_fc_features[\"textual\"][\"obj\"])\n",
    "\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"subj\"] + fc_features[\"visual\"][\"pred\"], fc_features[\"visual\"][\"obj\"], neg_fc_features[\"visual\"][\"subj\"] + neg_fc_features[\"visual\"][\"pred\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"subj\"] + fc_features[\"textual\"][\"pred\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"textual\"][\"subj\"] + neg_fc_features[\"textual\"][\"pred\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"subj\"] + fc_features[\"visual\"][\"pred\"], fc_features[\"visual\"][\"obj\"], neg_fc_features[\"visual\"][\"obj\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"subj\"] + fc_features[\"textual\"][\"pred\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"textual\"][\"obj\"])\n",
    "\n",
    "        #divide by the number of training examples\n",
    "        triplet_losses[\"subj\"] = triplet_losses[\"subj\"] / len(negative_examples)\n",
    "        triplet_losses[\"pred\"] = triplet_losses[\"pred\"] / len(negative_examples)\n",
    "        triplet_losses[\"obj\"] = triplet_losses[\"obj\"] / len(negative_examples)        \n",
    "        triplet_losses[\"transr\"] =  triplet_losses[\"transr\"] / len(negative_examples)\n",
    "        return triplet_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3640f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_checkpoint(model, chkpoint_path, optimizer=None):\n",
    "    chkpoint = torch.load(chkpoint_path)\n",
    "    \n",
    "    # incompatibility fixes\n",
    "    del chkpoint['model']['detectron.proposal_generator.anchor_generator.cell_anchors.0']\n",
    "    del chkpoint['model']['detectron.proposal_generator.anchor_generator.cell_anchors.1']\n",
    "    del chkpoint['model']['detectron.proposal_generator.anchor_generator.cell_anchors.2']\n",
    "    del chkpoint['model']['detectron.proposal_generator.anchor_generator.cell_anchors.3']\n",
    "    del chkpoint['model']['detectron.proposal_generator.anchor_generator.cell_anchors.4']\n",
    "    \n",
    "    model.load_state_dict(chkpoint['model'])\n",
    "    if (optimizer is not None):\n",
    "        optimizer.load_state_dict(chkpoint['optimizer'])\n",
    "    return chkpoint['it'], chkpoint['epoch'], chkpoint['losses']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6629a",
   "metadata": {},
   "source": [
    "### [Optional] Test Instance Detector (Object detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71ab5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import (\n",
    "    DatasetCatalog, DatasetMapper,\n",
    "    build_detection_train_loader,\n",
    "    build_detection_test_loader,\n",
    ")\n",
    "from config import get_vrd_cfg\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "cfg = get_vrd_cfg()\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .6\n",
    "cfg.DATASETS.TEST = (\"vrd_val\", )\n",
    "\n",
    "#test dataset\n",
    "test_dataset = DatasetCatalog.get(\"vrd_val\")\n",
    "test_dataloader = build_detection_test_loader(dataset=test_dataset,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "#train dataset\n",
    "train_dataset = DatasetCatalog.get(\"vrd_train\")\n",
    "train_dataloader = build_detection_test_loader(dataset=train_dataset,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f0c5cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RelTransR(\n",
       "  (fc_spatial): Sequential(\n",
       "    (0): Linear(in_features=22, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (detectron): GeneralizedRCNN(\n",
       "    (backbone): FPN(\n",
       "      (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (top_block): LastLevelMaxPool()\n",
       "      (bottom_up): ResNet(\n",
       "        (stem): BasicStem(\n",
       "          (conv1): Conv2d(\n",
       "            3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (res2): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (res3): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (3): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (res4): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (3): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (4): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (5): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (6): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (7): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (8): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (9): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (10): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (11): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (12): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (13): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (14): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (15): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (16): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (17): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (18): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (19): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (20): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (21): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (22): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (res5): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proposal_generator): RPN(\n",
       "      (rpn_head): StandardRPNHead(\n",
       "        (conv): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (anchor_generator): DefaultAnchorGenerator(\n",
       "        (cell_anchors): BufferList()\n",
       "      )\n",
       "    )\n",
       "    (roi_heads): StandardROIHeads(\n",
       "      (box_pooler): ROIPooler(\n",
       "        (level_poolers): ModuleList(\n",
       "          (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "          (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "          (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "          (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "        )\n",
       "      )\n",
       "      (box_head): FastRCNNConvFCHead(\n",
       "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "        (fc_relu1): ReLU()\n",
       "        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (fc_relu2): ReLU()\n",
       "      )\n",
       "      (box_predictor): FastRCNNOutputLayers(\n",
       "        (cls_score): Linear(in_features=1024, out_features=101, bias=True)\n",
       "        (bbox_pred): Linear(in_features=1024, out_features=400, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (backbone): FPN(\n",
       "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (top_block): LastLevelMaxPool()\n",
       "    (bottom_up): ResNet(\n",
       "      (stem): BasicStem(\n",
       "        (conv1): Conv2d(\n",
       "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (res2): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res3): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res4): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (6): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (7): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (8): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (9): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (10): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (11): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (12): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (13): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (14): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (15): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (16): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (17): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (18): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (19): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (20): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (21): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (22): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res5): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): ROIPooler(\n",
       "    (level_poolers): ModuleList(\n",
       "      (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "      (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "      (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "      (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "    )\n",
       "  )\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc_word): ModuleDict(\n",
       "    (subj): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    )\n",
       "    (pred): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    )\n",
       "    (obj): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_rois): ModuleDict(\n",
       "    (subj): Sequential(\n",
       "      (0): Linear(in_features=12544, out_features=6272, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=6272, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (pred): Sequential(\n",
       "      (0): Linear(in_features=12608, out_features=6272, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=6272, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (obj): Sequential(\n",
       "      (0): Linear(in_features=12544, out_features=6272, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=6272, out_features=1024, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_rois2): ModuleDict(\n",
       "    (subj): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (pred): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (obj): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (triplet_loss): ModuleDict(\n",
       "    (subj): TripletMarginWithDistanceLoss()\n",
       "    (pred): TripletMarginWithDistanceLoss()\n",
       "    (obj): TripletMarginWithDistanceLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RelTransR(cfg)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "#Run only once\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf347f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpoint_path = '../checkpoint/'\n",
    "model_name = 'vrd2_model_transr_23000.pt'\n",
    "chkpoint_full_path = os.path.join(chkpoint_path, model_name)\n",
    "it, start_epoch, losses = load_checkpoint(model, chkpoint_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d1a2c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from detectron2.evaluation import COCOEvaluator, DatasetEvaluator, DatasetEvaluators, inference_on_dataset\n",
    "\n",
    "object_classes = model.object_classes\n",
    "\n",
    "class Counter(DatasetEvaluator):\n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        self.gt_count = 0\n",
    "        self.object_areas = defaultdict(list)\n",
    "        self.object_counts = defaultdict(int)\n",
    "        \n",
    "    def process(self, inputs, outputs):\n",
    "        for output in outputs:\n",
    "            self.count += len(output[\"instances\"])\n",
    "        \n",
    "        for inp in inputs:\n",
    "            self.gt_count += len(inp[\"instances\"])\n",
    "            \n",
    "            annotations = inp['instances'].get_fields()    \n",
    "            for box, cls in zip(annotations['gt_boxes'], annotations['gt_classes']):\n",
    "                object_width = box[2] - box[0] + 1\n",
    "                object_height = box[3] - box[1] + 1\n",
    "                object_area = object_width * object_height\n",
    "                self.object_areas['area-{}'.format(object_classes[cls])].append(object_area)\n",
    "                self.object_counts[object_classes[cls]] +=1\n",
    "                \n",
    "    def evaluate(self):\n",
    "        # save self.count somewhere, or print it, or return it.\n",
    "        resulting_dict = {\"count\": self.count, \"gt_count\": self.gt_count, \"object_counts\": self.object_counts, \"object_average_area\": {}}\n",
    "        \n",
    "        for key, areas in self.object_areas.items():\n",
    "            resulting_dict[\"object_average_area\"][key] = sum(areas) / len(areas)\n",
    "        \n",
    "        return resulting_dict\n",
    "    \n",
    "def beatify_detectron2_results(eval_results):\n",
    "    \"\"\"\n",
    "        Beautify the results output by detectron2\n",
    "    \"\"\"\n",
    "    object_areas = eval_results['object_average_area']\n",
    "    object_area_ap = {}\n",
    "    \n",
    "    for eval_method, eval_result in eval_results.items():\n",
    "        if (eval_method == 'count'):\n",
    "            print(\"Total Objects Detected:\", eval_result)\n",
    "        elif (eval_method == 'gt_count'):\n",
    "            print(\"Total Labeled Objects:\", eval_result)\n",
    "        elif (eval_method == 'object_average_area'):\n",
    "            continue\n",
    "        elif (eval_method == \"bbox\"):\n",
    "            print(\"Evaluation results for {}\".format(eval_method))\n",
    "\n",
    "            resulting_string = \"\"\n",
    "            for i, (key, res) in enumerate(eval_result.items()):\n",
    "                resulting_string = \"\".join((resulting_string, \"|   {:>16}\\t->\\t{:5.2f}\".format(key, res)))\n",
    "                if ((i + 1) <= 6):\n",
    "                    resulting_string = \"\".join((resulting_string, \"   |\"))\n",
    "                if ((i + 1) == 6):\n",
    "                    resulting_string = \"\".join((resulting_string, \"\\nEvaluation results by object category\\n\"))\n",
    "                elif ((i + 1) > 6):\n",
    "                    object_cls = key.split('-')[1]\n",
    "                    area_key = 'area-{}'.format(object_cls)\n",
    "                    object_area = object_areas[area_key]\n",
    "                    \n",
    "                    resulting_string = \"\".join((resulting_string, \"( {:5.2f} area )\\t|\".format(object_area)))\n",
    "                    object_area_ap[object_cls] = (res, object_area)\n",
    "                    \n",
    "                if ((i + 1) % 2 == 0):\n",
    "                    resulting_string = \"\".join((resulting_string, \"\\n\"))\n",
    "            print(resulting_string)\n",
    "            \n",
    "    return object_area_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08971bfd",
   "metadata": {},
   "source": [
    "##### Object Detector Evaluation for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ce338ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.214\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.071\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.162\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.181\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.204\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.015\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.099\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232\n"
     ]
    }
   ],
   "source": [
    "eval_results = inference_on_dataset(\n",
    "    model.detectron,\n",
    "    test_dataloader,\n",
    "    DatasetEvaluators([COCOEvaluator('vrd_val', output_dir=\"../generated/coco_evaluations_val\"), Counter()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e49d3444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for bbox\n",
      "|                 AP\t->\t13.69   ||               AP50\t->\t21.38   |\n",
      "|               AP75\t->\t15.02   ||                APs\t->\t 1.43   |\n",
      "|                APm\t->\t 7.13   ||                APl\t->\t16.15   |\n",
      "Evaluation results by object category\n",
      "\n",
      "|          AP-person\t->\t34.58( 76559.36 area )\t||             AP-sky\t->\t34.00( 240566.56 area )\t|\n",
      "|        AP-building\t->\t12.51( 143178.06 area )\t||           AP-truck\t->\t26.60( 115784.02 area )\t|\n",
      "|             AP-bus\t->\t58.39( 157357.81 area )\t||           AP-table\t->\t26.36( 171163.44 area )\t|\n",
      "|           AP-shirt\t->\t14.84( 35092.40 area )\t||           AP-chair\t->\t 6.35( 58440.02 area )\t|\n",
      "|             AP-car\t->\t23.45( 40891.77 area )\t||           AP-train\t->\t43.85( 165328.08 area )\t|\n",
      "|         AP-glasses\t->\t10.19( 16188.93 area )\t||            AP-tree\t->\t 5.20( 78067.21 area )\t|\n",
      "|            AP-boat\t->\t13.36( 131939.91 area )\t||             AP-hat\t->\t13.68( 6856.33 area )\t|\n",
      "|           AP-trees\t->\t 5.41( 168642.62 area )\t||           AP-grass\t->\t18.18( 188001.73 area )\t|\n",
      "|           AP-pants\t->\t11.50( 27497.62 area )\t||            AP-road\t->\t 1.34( 219012.50 area )\t|\n",
      "|      AP-motorcycle\t->\t26.59( 137659.02 area )\t||          AP-jacket\t->\t 3.14( 44384.67 area )\t|\n",
      "|         AP-monitor\t->\t39.43( 58283.25 area )\t||           AP-wheel\t->\t 9.22( 13802.85 area )\t|\n",
      "|        AP-umbrella\t->\t17.14( 58044.14 area )\t||           AP-plate\t->\t10.30( 43204.28 area )\t|\n",
      "|            AP-bike\t->\t27.16( 77017.67 area )\t||           AP-clock\t->\t18.36( 20313.14 area )\t|\n",
      "|             AP-bag\t->\t 4.14( 18558.25 area )\t||            AP-shoe\t->\t 0.00( 5325.92 area )\t|\n",
      "|          AP-laptop\t->\t44.53( 69151.68 area )\t||            AP-desk\t->\t16.58( 187479.03 area )\t|\n",
      "|         AP-cabinet\t->\t 1.11( 106261.52 area )\t||         AP-counter\t->\t11.78( 113178.15 area )\t|\n",
      "|           AP-bench\t->\t16.40( 101857.77 area )\t||           AP-shoes\t->\t 4.75( 10061.13 area )\t|\n",
      "|           AP-tower\t->\t20.10( 91290.62 area )\t||          AP-bottle\t->\t13.40( 15821.26 area )\t|\n",
      "|          AP-helmet\t->\t 6.43( 5840.40 area )\t||           AP-stove\t->\t11.73( 99298.34 area )\t|\n",
      "|            AP-lamp\t->\t 6.91( 24420.36 area )\t||            AP-coat\t->\t 2.38( 26689.43 area )\t|\n",
      "|             AP-bed\t->\t37.67( 283294.00 area )\t||             AP-dog\t->\t26.26( 91087.78 area )\t|\n",
      "|        AP-mountain\t->\t10.72( 156701.84 area )\t||           AP-horse\t->\t48.87( 124443.15 area )\t|\n",
      "|           AP-plane\t->\t14.65( 218616.00 area )\t||            AP-roof\t->\t 5.26( 71968.27 area )\t|\n",
      "|      AP-skateboard\t->\t33.26( 14634.12 area )\t||   AP-traffic light\t->\t 1.70( 21492.54 area )\t|\n",
      "|            AP-bush\t->\t 4.75( 61116.31 area )\t||           AP-phone\t->\t 9.55( 7087.81 area )\t|\n",
      "|        AP-airplane\t->\t23.27( 207302.50 area )\t||            AP-sofa\t->\t22.73( 179195.12 area )\t|\n",
      "|             AP-cup\t->\t 2.68( 8058.37 area )\t||            AP-sink\t->\t 7.93( 23896.81 area )\t|\n",
      "|           AP-shelf\t->\t 0.45( 40503.39 area )\t||             AP-box\t->\t 0.45( 22467.32 area )\t|\n",
      "|             AP-van\t->\t 4.25( 42637.69 area )\t||            AP-hand\t->\t 0.75( 11124.61 area )\t|\n",
      "|          AP-shorts\t->\t 9.45( 17229.33 area )\t||            AP-post\t->\t 0.00( 19911.62 area )\t|\n",
      "|           AP-jeans\t->\t 3.37( 21702.57 area )\t||             AP-cat\t->\t39.72( 35152.50 area )\t|\n",
      "|      AP-sunglasses\t->\t 5.76( 2637.89 area )\t||            AP-bowl\t->\t 4.27( 33583.89 area )\t|\n",
      "|        AP-computer\t->\t 0.00( 39283.40 area )\t||          AP-pillow\t->\t 4.37( 37444.18 area )\t|\n",
      "|           AP-pizza\t->\t16.27( 52745.75 area )\t||          AP-basket\t->\t 1.39( 38354.06 area )\t|\n",
      "|        AP-elephant\t->\t41.61( 164398.64 area )\t||            AP-kite\t->\t17.29( 37164.50 area )\t|\n",
      "|            AP-sand\t->\t10.25( 264610.78 area )\t||        AP-keyboard\t->\t36.28( 25222.50 area )\t|\n",
      "|           AP-plant\t->\t 3.61( 24744.39 area )\t||             AP-can\t->\t 7.33( 18185.31 area )\t|\n",
      "|            AP-vase\t->\t 1.58( 15479.43 area )\t||    AP-refrigerator\t->\t16.17( 105438.10 area )\t|\n",
      "|            AP-cart\t->\t 0.00( 103229.00 area )\t||            AP-skis\t->\t10.28( 38120.55 area )\t|\n",
      "|             AP-pot\t->\t 1.90( 10412.88 area )\t||       AP-surfboard\t->\t 2.59( 22901.00 area )\t|\n",
      "|           AP-paper\t->\t 1.11( 28240.95 area )\t||           AP-mouse\t->\t28.40( 4136.33 area )\t|\n",
      "|       AP-trash can\t->\t 3.04( 11847.54 area )\t||            AP-cone\t->\t 2.38( 33400.53 area )\t|\n",
      "|          AP-camera\t->\t 0.12( 14791.06 area )\t||            AP-ball\t->\t 0.00( 2925.22 area )\t|\n",
      "|            AP-bear\t->\t20.03( 86962.55 area )\t||         AP-giraffe\t->\t44.46( 100031.80 area )\t|\n",
      "|             AP-tie\t->\t25.69( 8472.00 area )\t||         AP-luggage\t->\t 4.69( 133869.73 area )\t|\n",
      "|          AP-faucet\t->\t 0.00( 11172.43 area )\t||         AP-hydrant\t->\t44.27( 41876.83 area )\t|\n",
      "|       AP-snowboard\t->\t12.44( 55021.80 area )\t||            AP-oven\t->\t 0.00( 163212.27 area )\t|\n",
      "|          AP-engine\t->\t 4.51( 31553.91 area )\t||           AP-watch\t->\t 1.59( 2093.95 area )\t|\n",
      "|            AP-face\t->\t 0.85( 6665.91 area )\t||          AP-street\t->\t10.28( 236292.02 area )\t|\n",
      "|            AP-ramp\t->\t 1.43( 111472.66 area )\t||        AP-suitcase\t->\t 3.43( 43487.75 area )\t|\n",
      "\n",
      "Total Objects Detected: 5521\n",
      "Total Labeled Objects: 6693\n"
     ]
    }
   ],
   "source": [
    "object_area_ap = beatify_detectron2_results(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea0aa3d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 1.0, \"a) Average Percision by Object's area (Test Set)\")]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "test_object_data = [ (key, item[0], item[1]) for key, item in object_area_ap.items()]\n",
    "\n",
    "area_ap_df = pd.DataFrame(pd.DataFrame(list(test_object_data),\n",
    "               columns =['Object', 'AP', 'Area']))\n",
    "\n",
    "ax = sns.scatterplot(data=area_ap_df, x=\"Area\", y=\"AP\")\n",
    "ax.set(title='a) Average Percision by Object\\'s area (Test Set)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81051a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_ap_df = pd.DataFrame(pd.DataFrame(list(test_object_data),\n",
    "               columns =['Object', 'Average Percision', 'Object Size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f48a6c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Object</th>\n",
       "      <th>Average Percision</th>\n",
       "      <th>Object Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person</td>\n",
       "      <td>34.582935</td>\n",
       "      <td>tensor(76559.3594)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sky</td>\n",
       "      <td>33.995682</td>\n",
       "      <td>tensor(240566.5625)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>building</td>\n",
       "      <td>12.514562</td>\n",
       "      <td>tensor(143178.0625)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>truck</td>\n",
       "      <td>26.603097</td>\n",
       "      <td>tensor(115784.0156)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bus</td>\n",
       "      <td>58.393152</td>\n",
       "      <td>tensor(157357.8125)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Object  Average Percision          Object Size\n",
       "0    person          34.582935   tensor(76559.3594)\n",
       "1       sky          33.995682  tensor(240566.5625)\n",
       "2  building          12.514562  tensor(143178.0625)\n",
       "3     truck          26.603097  tensor(115784.0156)\n",
       "4       bus          58.393152  tensor(157357.8125)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_ap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "403da0db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnt_object_data = [ (key, item) for key, item in eval_results['object_counts'].items()]\n",
    "\n",
    "cnt_ap_df = pd.DataFrame(pd.DataFrame(list(cnt_object_data),\n",
    "               columns =['Object', 'Count']))\n",
    "cnt_ap_df = cnt_ap_df.sort_values(by='Count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3154c166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.reset_defaults()\n",
    "ax= sns.catplot(x=\"Object\", y=\"Count\", kind=\"bar\", palette=\"ch:.25\",data=cnt_ap_df)\n",
    "ax.set(xticklabels=[])\n",
    "ax.set(xticks=[])\n",
    "\n",
    "ax.set(title='a) Number of objects by class (Test Set)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(area_ap_df['Object Size'],bins=30,kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "fa6ed32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst Performing Objects: ['shoe' 'post' 'oven' 'faucet' 'ball' 'cart' 'computer' 'camera' 'box'\n",
      " 'shelf' 'hand' 'face' 'cabinet' 'paper' 'road' 'basket' 'ramp' 'vase'\n",
      " 'watch' 'traffic light']\n"
     ]
    }
   ],
   "source": [
    "rank_ap = np.argsort([item[1] for item in test_object_data])\n",
    "ranked_object = np.array([item[0] for item in test_object_data])[rank_ap]\n",
    "print(\"Worst Performing Objects:\", ranked_object[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99ba7d",
   "metadata": {},
   "source": [
    "##### Object detector performance for Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c5f2b890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.653\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.843\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.771\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.425\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.614\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.688\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.621\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.708\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.708\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.456\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.651\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.738\n"
     ]
    }
   ],
   "source": [
    "eval_results = inference_on_dataset(\n",
    "    model.detectron,\n",
    "    train_dataloader,\n",
    "    DatasetEvaluators([COCOEvaluator('vrd_train', output_dir=\"../generated/coco_evaluations_train/\"), Counter()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "caa74ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for bbox\n",
      "|                 AP\t->\t65.29   ||               AP50\t->\t84.33   |\n",
      "|               AP75\t->\t77.14   ||                APs\t->\t42.54   |\n",
      "|                APm\t->\t61.40   ||                APl\t->\t68.77   |\n",
      "Evaluation results by object category\n",
      "\n",
      "|          AP-person\t->\t72.37( 71984.69 area )\t||             AP-sky\t->\t63.33( 251812.36 area )\t|\n",
      "|        AP-building\t->\t70.92( 139850.50 area )\t||           AP-truck\t->\t75.91( 115676.97 area )\t|\n",
      "|             AP-bus\t->\t83.10( 126690.67 area )\t||           AP-table\t->\t73.73( 155999.31 area )\t|\n",
      "|           AP-shirt\t->\t69.00( 34707.58 area )\t||           AP-chair\t->\t73.86( 49569.39 area )\t|\n",
      "|             AP-car\t->\t76.27( 33861.08 area )\t||           AP-train\t->\t80.86( 185003.98 area )\t|\n",
      "|         AP-glasses\t->\t61.50( 7179.47 area )\t||            AP-tree\t->\t67.34( 84326.91 area )\t|\n",
      "|            AP-boat\t->\t76.68( 80755.25 area )\t||             AP-hat\t->\t67.04( 7132.03 area )\t|\n",
      "|           AP-trees\t->\t54.56( 162260.86 area )\t||           AP-grass\t->\t59.64( 201710.95 area )\t|\n",
      "|           AP-pants\t->\t63.05( 20873.22 area )\t||            AP-road\t->\t12.41( 217147.84 area )\t|\n",
      "|      AP-motorcycle\t->\t71.75( 151622.64 area )\t||          AP-jacket\t->\t60.02( 38070.55 area )\t|\n",
      "|         AP-monitor\t->\t81.62( 52150.22 area )\t||           AP-wheel\t->\t61.90( 13386.50 area )\t|\n",
      "|        AP-umbrella\t->\t72.77( 65808.46 area )\t||           AP-plate\t->\t68.23( 42791.27 area )\t|\n",
      "|            AP-bike\t->\t64.91( 80304.20 area )\t||           AP-clock\t->\t70.37( 19354.90 area )\t|\n",
      "|             AP-bag\t->\t67.33( 17693.43 area )\t||            AP-shoe\t->\t50.80( 2934.38 area )\t|\n",
      "|          AP-laptop\t->\t84.60( 69827.24 area )\t||            AP-desk\t->\t61.99( 199350.91 area )\t|\n",
      "|         AP-cabinet\t->\t64.14( 78734.15 area )\t||         AP-counter\t->\t63.35( 131176.11 area )\t|\n",
      "|           AP-bench\t->\t72.31( 93124.05 area )\t||           AP-shoes\t->\t63.56( 8907.16 area )\t|\n",
      "|           AP-tower\t->\t68.81( 74683.84 area )\t||          AP-bottle\t->\t71.94( 9291.23 area )\t|\n",
      "|          AP-helmet\t->\t71.50( 5906.64 area )\t||           AP-stove\t->\t68.07( 82335.18 area )\t|\n",
      "|            AP-lamp\t->\t65.85( 19275.47 area )\t||            AP-coat\t->\t25.34( 32593.72 area )\t|\n",
      "|             AP-bed\t->\t73.49( 248047.50 area )\t||             AP-dog\t->\t78.69( 50730.95 area )\t|\n",
      "|        AP-mountain\t->\t47.62( 139752.38 area )\t||           AP-horse\t->\t79.82( 121256.35 area )\t|\n",
      "|           AP-plane\t->\t54.34( 161702.88 area )\t||            AP-roof\t->\t58.91( 50818.23 area )\t|\n",
      "|      AP-skateboard\t->\t71.69( 20501.91 area )\t||   AP-traffic light\t->\t61.17( 12297.36 area )\t|\n",
      "|            AP-bush\t->\t61.43( 62183.56 area )\t||           AP-phone\t->\t69.27( 7611.49 area )\t|\n",
      "|        AP-airplane\t->\t43.99( 170052.66 area )\t||            AP-sofa\t->\t75.54( 141789.03 area )\t|\n",
      "|             AP-cup\t->\t70.03( 11047.33 area )\t||            AP-sink\t->\t77.68( 55081.31 area )\t|\n",
      "|           AP-shelf\t->\t58.09( 68417.25 area )\t||             AP-box\t->\t64.48( 27199.48 area )\t|\n",
      "|             AP-van\t->\t68.71( 57604.06 area )\t||            AP-hand\t->\t57.07( 17640.84 area )\t|\n",
      "|          AP-shorts\t->\t69.48( 12563.07 area )\t||            AP-post\t->\t51.96( 19868.83 area )\t|\n",
      "|           AP-jeans\t->\t54.58( 26824.99 area )\t||             AP-cat\t->\t78.07( 71325.70 area )\t|\n",
      "|      AP-sunglasses\t->\t55.51( 2855.38 area )\t||            AP-bowl\t->\t76.58( 29442.71 area )\t|\n",
      "|        AP-computer\t->\t22.81( 69973.41 area )\t||          AP-pillow\t->\t77.29( 19280.19 area )\t|\n",
      "|           AP-pizza\t->\t66.44( 36342.25 area )\t||          AP-basket\t->\t65.32( 20453.39 area )\t|\n",
      "|        AP-elephant\t->\t81.68( 137788.80 area )\t||            AP-kite\t->\t66.67( 42472.73 area )\t|\n",
      "|            AP-sand\t->\t54.25( 231842.91 area )\t||        AP-keyboard\t->\t79.78( 27491.26 area )\t|\n",
      "|           AP-plant\t->\t70.49( 39675.46 area )\t||             AP-can\t->\t59.49( 13288.84 area )\t|\n",
      "|            AP-vase\t->\t76.66( 19237.35 area )\t||    AP-refrigerator\t->\t79.32( 177738.89 area )\t|\n",
      "|            AP-cart\t->\t69.28( 78391.50 area )\t||            AP-skis\t->\t63.67( 24757.51 area )\t|\n",
      "|             AP-pot\t->\t69.44( 8834.36 area )\t||       AP-surfboard\t->\t75.45( 47542.85 area )\t|\n",
      "|           AP-paper\t->\t66.59( 18679.09 area )\t||           AP-mouse\t->\t75.35( 8173.72 area )\t|\n",
      "|       AP-trash can\t->\t67.99( 10818.71 area )\t||            AP-cone\t->\t55.18( 11402.54 area )\t|\n",
      "|          AP-camera\t->\t52.86( 7117.39 area )\t||            AP-ball\t->\t63.87( 4654.00 area )\t|\n",
      "|            AP-bear\t->\t77.21( 96456.30 area )\t||         AP-giraffe\t->\t76.30( 154383.97 area )\t|\n",
      "|             AP-tie\t->\t61.67( 14692.90 area )\t||         AP-luggage\t->\t57.17( 54422.34 area )\t|\n",
      "|          AP-faucet\t->\t72.44( 8377.83 area )\t||         AP-hydrant\t->\t76.12( 41752.23 area )\t|\n",
      "|       AP-snowboard\t->\t69.89( 29388.98 area )\t||            AP-oven\t->\t57.50( 75488.84 area )\t|\n",
      "|          AP-engine\t->\t57.83( 28436.07 area )\t||           AP-watch\t->\t54.71( 1405.24 area )\t|\n",
      "|            AP-face\t->\t29.80( 17562.64 area )\t||          AP-street\t->\t44.88( 215614.05 area )\t|\n",
      "|            AP-ramp\t->\t57.49( 183246.34 area )\t||        AP-suitcase\t->\t65.14( 41546.28 area )\t|\n",
      "\n",
      "Total Objects Detected: 27919\n",
      "Total Labeled Objects: 26286\n"
     ]
    }
   ],
   "source": [
    "object_area_ap = beatify_detectron2_results(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e666328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 1.0, \"b) Average Percision by Object's area (Train Set)\")]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAHHCAYAAABKudlQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAABcOUlEQVR4nO3deVwU9f8H8BcgLCDsIqAcnogknmmeeIAlRmamiXmWpn618krNSis1u1ArK/1m9S1DyyvNK63s8NbQ1Mgji9DwSAQFZVdEWGQ/vz/87ebKLrvA7s4MvJ6PB49yZnb2s7MzO+95fy43IYQAERERkUK5S10AIiIiospgMENERESKxmCGiIiIFI3BDBERESkagxkiIiJSNAYzREREpGgMZoiIiEjRGMwQERGRojGYISIiIkVjMKMQr7zyCtzc3JCTk1PmdsXFxahfvz6WLFniopKRq+3atQtubm7YtWtXuV7XqFEjPPHEE04pU1nsPXel0KhRIzz00EM2t6voMaeKy8/PR506dbBy5UqXvm+PHj3Qo0cPl76no5w8eRI1atTAiRMnpC6KyzGYqWI8PT0xbdo0vPHGGygsLCzXa//44w+4ubnB29sbeXl5zimgQj3xxBNwc3Mz/anVatx999145513UFRUJHXx6P8JIfDFF18gNjYWAQEB8PX1RatWrfDqq6/i+vXrUhfPqpMnT+KVV17BmTNnSq3r0aOHJEGo1N5//334+/tjyJAhOHPmjNn1V9afpWMotePHj2PgwIFo2LAhvL29UbduXfTq1QuLFy+u0P5WrVqF9957r9Ty5s2bo0+fPpg9e3YlS6w8NaQuADneqFGjMGPGDKxatQqjR4+2+3UrVqxAaGgorl69iq+++gr/+c9/nFhK5VGpVPj0008BAHl5eVi/fj2mT5+OQ4cOYc2aNS4rR2xsLG7cuAEvL69yvS4tLQ3u7lX3+aWkpATDhg3D2rVr0b17d7zyyivw9fXF3r17MXfuXKxbtw4//fQTQkJCyr3vih5ze508eRJz585Fjx490KhRI6e8h5IUFxfj/fffx9SpU+Hh4YHatWvjiy++MNvmnXfewT///IN3333XbHnt2rUr9d4//PBDpV5/p59//hn33nsvGjRogLFjxyI0NBTnz5/HgQMH8P7772PSpEnl3ueqVatw4sQJTJkypdS6p556Cg8++CBOnz6NyMhIB3wChRCkCHPmzBEAxOXLl+3a/qGHHhLdu3e3e/8Gg0E0atRITJs2TTzyyCOiR48eFS1qpVy/fl2S97Vl5MiRombNmmbLSkpKRPv27QUAceHChUrtv6SkRNy4caNS+5Cr8p67FfXmm28KAGL69Oml1n399dfC3d1dPPDAA2bLGzZsKPr06ePUctlj3bp1AoDYuXNnqXVxcXFi5MiRLi/T7YqLi0VRUZHL3m/Dhg0CgDh16pTVbfr06SMaNmxY5n4MBoMoKChwcOnK58EHHxS1a9cWV69eLbUuOzu7Qvss67Pr9XpRq1YtMWvWrArtW6mq7mNaFZWTk4NBgwZBrVYjKCgIzzzzjMXqpF69emHfvn24cuWKXfvdv38/zpw5gyFDhmDIkCHYs2cP/vnnH9P6hx56CI0bN7b42piYGLRv395s2YoVK9CuXTv4+PggMDAQQ4YMwfnz58226dGjB1q2bIkjR44gNjYWvr6+ePHFFwEAmzdvRp8+fRAeHg6VSoXIyEi89tprKCkpKfX+H3zwARo3bgwfHx907NgRe/futVjvXVRUhDlz5qBJkyZQqVSoX78+nn/++QpXE7m7u5vew5jatvc93NzcMHHiRKxcuRItWrSASqXCtm3bAAAXLlzAmDFjTJ89IiICTz/9NPR6PQDL7TfS09ORmJiI0NBQeHt7o169ehgyZAi0Wq1pG0ttZv7++288+uijCAwMhK+vLzp37oxvvvnGbBvj+61duxZvvPEG6tWrB29vb/Ts2ROnTp2y+3jZOnfj4uJw9913W3xt06ZNkZCQYHXfN27cwFtvvYW77roLSUlJpdb37dsXI0eOxLZt23DgwIFS63/44Qe0adMG3t7eaN68OTZs2GC23lqbmYMHD+KBBx6ARqOBr68v4uLisH///lL7L+s7XbZsGR599FEAwL333muqLimrfc7ixYvRokUL+Pr6olatWmjfvj1WrVpldXsA0Ov1mD17Ntq1aweNRoOaNWuie/fu2Llzp9l2xiqdt99+G++99x4iIyOhUqlw8uRJAMCff/6JgQMHIjAwEN7e3mjfvj2+/vprs31cuXIF06dPR6tWreDn5we1Wo3evXvj6NGjZZbRaNOmTWjUqFG5MwvGNlDff/892rdvDx8fH3z88ccAgOTkZNx3332oU6cOVCoVmjdvjg8//LDUPu787ajs+X/69Gm0aNECAQEBpdbVqVOn1DJbv509evTAN998g7Nnz5rOlduzeZ6enujRowc2b95ss2xVCauZFGbQoEFo1KgRkpKScODAASxatAhXr17F559/brZdu3btIITAzz//bFcDx5UrVyIyMhIdOnRAy5Yt4evri9WrV+O5554DAAwePBgjRozAoUOH0KFDB9Przp49iwMHDuCtt94yLXvjjTcwa9YsDBo0CP/5z39w+fJlLF68GLGxsUhNTTW7qHNzc9G7d28MGTIEjz32mKkKYNmyZfDz88O0adPg5+eHHTt2YPbs2dDpdGbv9eGHH2LixIno3r07pk6dijNnzqB///6oVasW6tWrZ9rOYDDg4Ycfxr59+zBu3Dg0a9YMx48fx7vvvou//voLmzZtKtf3YHT69GkAQFBQULnfY8eOHVi7di0mTpyI4OBgNGrUCJmZmejYsSPy8vIwbtw4REdH48KFC/jqq69QUFBgsZpDr9cjISEBRUVFmDRpEkJDQ3HhwgVs3boVeXl50Gg0FsuenZ2NLl26oKCgAJMnT0ZQUBCWL1+Ohx9+GF999RUeeeQRs+3nzZsHd3d3TJ8+HVqtFgsWLMDw4cNx8OBBu46VrXP38ccfx9ixY3HixAm0bNnS9LpDhw7hr7/+wssvv2x13/v27cPVq1fxzDPPoEYNyz9rI0aMQHJyMrZu3YrOnTublqenp2Pw4MF46qmnMHLkSCQnJ+PRRx/Ftm3b0KtXL6vvuWPHDvTu3Rvt2rXDnDlz4O7ubrph7t27Fx07dgQAm99pbGwsJk+ejEWLFuHFF19Es2bNAMD03zt98sknmDx5MgYOHGgKCI8dO4aDBw9i2LBhVsur0+nw6aefYujQoRg7diyuXbuGpUuXIiEhAb/88gvatGljtn1ycjIKCwsxbtw4qFQqBAYG4vfff0fXrl1Rt25dzJgxAzVr1sTatWvRv39/rF+/3nTO/P3339i0aRMeffRRREREIDs7Gx9//DHi4uJw8uRJhIeHWy0ncKtq5p577ilzG2vS0tIwdOhQPPnkkxg7diyaNm0K4NZvRYsWLfDwww+jRo0a2LJlC8aPHw+DwYAJEybY3G9Fz/+GDRsiJSWl1HltiT2/nS+99BK0Wq1ZFZufn5/Zftq1a4fNmzdDp9NBrVbb/GxVgtSpIbKPMVX/8MMPmy0fP368ACCOHj1qtjwzM1MAEPPnz7e5b71eL4KCgsRLL71kWjZs2DBx9913m/6t1WqFSqUSzz77rNlrFyxYINzc3MTZs2eFEEKcOXNGeHh4iDfeeMNsu+PHj4saNWqYLY+LixMAxEcffVSqTJZSw08++aTw9fUVhYWFQgghioqKRFBQkOjQoYMoLi42bbds2TIBQMTFxZmWffHFF8Ld3V3s3bvXbJ8fffSRACD2799v7fAIIf6tZrp8+bK4fPmyOHXqlHjzzTeFm5ubaN26dbnfA4Bwd3cXv//+u9m2I0aMEO7u7uLQoUOlymAwGIQQQuzcudOsSiI1NVUAEOvWrSvzMzRs2NCsumLKlCkCgFl5r127JiIiIkSjRo1ESUmJ2fs1a9bMrKrh/fffFwDE8ePHy3xfe8/dvLw84e3tLV544QWz7SZPnixq1qwp8vPzrb7He++9JwCIjRs3Wt3mypUrAoAYMGCAaVnDhg0FALF+/XrTMq1WK8LCwkTbtm1Ny+485gaDQURFRYmEhATT9yLErfM2IiJC9OrVy7TMnu+0rGqmO/Xr10+0aNHC5nZ3unnzZqmqoqtXr4qQkBAxevRo07KMjAwBQKjVanHp0iWz7Xv27ClatWplugaNn6FLly4iKirKtKywsNB0/ty+X5VKJV599dUyy1lcXCzc3NxK/dbcyVJVi/H73LZtW6ntLf2mJCQkiMaNG5sti4uLM/vtqOz5/8MPPwgPDw/h4eEhYmJixPPPPy++//57odfrzbYrz2+nrSq2VatWCQDi4MGDZZatKmE1k8Lc+QRhbDz27bffmi2vVasWANjVHfa7775Dbm4uhg4dalo2dOhQHD16FL///jsAmNLEa9euhRDCtN2XX36Jzp07o0GDBgCADRs2wGAwYNCgQcjJyTH9hYaGIioqqlRKW6VSYdSoUaXK5OPjY/r/a9euIScnB927d0dBQQH+/PNPAMDhw4eRm5uLsWPHmj2NDx8+3PT5jdatW4dmzZohOjrarFz33XcfAJQqlyXXr19H7dq1Ubt2bTRp0gQvvvgiYmJisHHjxgq9R1xcHJo3b276t8FgwKZNm9C3b99S1XbAraopS4yZl++//x4FBQU2P4fRt99+i44dO6Jbt26mZX5+fhg3bhzOnDljqlYwGjVqlFlmqHv37gBuPYXbw9a5q9Fo0K9fP6xevdp0jpWUlODLL79E//79UbNmTav7vnbtGgDA39/f6jbGdTqdzmx5eHi4WRZKrVZjxIgRSE1NRVZWlsV9/fbbb0hPT8ewYcOQm5tr+q6vX7+Onj17Ys+ePTAYDBX+TssSEBCAf/75B4cOHSrX6zw8PEzfn8FgwJUrV3Dz5k20b98ev/76a6ntExMTzRrTXrlyBTt27MCgQYNM12ROTg5yc3ORkJCA9PR0XLhwAcCt69rY2LykpAS5ubnw8/ND06ZNLb7X7a5cuQIhRKlr2F4REREWqyRv/03RarXIyclBXFwc/v77b7PqWGsqev736tULKSkpePjhh3H06FEsWLAACQkJqFu3rln1XHl/O8tSnt//qoLVTAoTFRVl9u/IyEi4u7uX6o5ovBnY82O5YsUKREREQKVSmeqAIyMj4evri5UrV+LNN98EcKuqadOmTUhJSUGXLl1w+vRpHDlyxKyLYHp6OoQQpcpp5OnpafbvunXrWqw6+f333/Hyyy9jx44dpW4+xh+es2fPAgCaNGlitr5GjRqleoSkp6fjjz/+sNrT4dKlSxaX387b2xtbtmwBAFO7h9urssr7HhEREWb/vnz5MnQ6nc1U9J0iIiIwbdo0LFy4ECtXrkT37t3x8MMP47HHHrNaxQTcOn6dOnUqtdxYvXH27FmzshgDViPjD+bVq1ftKqc95+6IESPw5ZdfYu/evYiNjcVPP/2E7OxsPP7442Xu2xioGIMaS6wFPE2aNCl1ndx1110AbrUfCQ0NLbWv9PR0AMDIkSOtvp9Wq4Ver6/Qd1qWF154AT/99BM6duyIJk2a4P7778ewYcPQtWtXm69dvnw53nnnHfz5558oLi42Lb/zXLS07NSpUxBCYNasWZg1a5bF/V+6dAl169aFwWDA+++/jyVLliAjI8OsrVtQUJBdn/P2h6bysPRZgFvtAufMmYOUlJRSQb9Wqy3zWgEqd/536NABGzZsgF6vx9GjR7Fx40a8++67GDhwIH777Tc0b9683L+dZSnP739VwWBG4aydrMYLLDg4uMzX63Q6bNmyBYWFhRYvolWrVuGNN96Am5sb+vbtC19fX6xduxZdunTB2rVr4e7ubmq8CNx64nNzc8N3330HDw+PUvu7s2739qclo7y8PMTFxUGtVuPVV19FZGQkvL298euvv+KFF16AwWAo8zNZYjAY0KpVKyxcuNDi+vr169vch4eHB+Lj4x32HpY+e0W98847eOKJJ7B582b88MMPmDx5sqltyu0BV2VY+j6Bit90LJ27CQkJCAkJwYoVKxAbG2saLqCs4w78G4AdO3YM/fv3t7jNsWPHAMAsG1ZRxnPwrbfeKtXWxMjPz8/uBvjl0axZM6SlpWHr1q3Ytm0b1q9fjyVLlmD27NmYO3eu1detWLECTzzxBPr374/nnnsOderUgYeHB5KSkkxtv2535/lp/MzTp0+32hjb+GDx5ptvYtasWRg9ejRee+01BAYGwt3dHVOmTLF5/QYGBsLNzc3uINlWuYFbbdt69uyJ6OhoLFy4EPXr14eXlxe+/fZbvPvuu3b9pjji/Pfy8kKHDh3QoUMH3HXXXRg1ahTWrVuHOXPmlPu3syz2/v5XJQxmFCY9Pd3syePUqVMwGAylMhEZGRkArDciNNqwYQMKCwvx4Ycfljrx09LS8PLLL2P//v3o1q0batasiYceegjr1q3DwoUL8eWXX6J79+5mjfkiIyMhhEBERITp6ba8du3ahdzcXGzYsAGxsbGlPpNRw4YNAdw6Bvfee69p+c2bN3HmzBm0bt3arFxHjx5Fz549nfa0Utn3qF27NtRqdYVH72zVqhVatWqFl19+GT///DO6du2Kjz76CK+//rrF7Rs2bIi0tLRSy43VeMbj6yj2nLseHh4YNmwYli1bhvnz52PTpk0YO3as1RuJUbdu3RAQEIBVq1bhpZdesri9saHxnQ3ijRmH27+zv/76CwCsjvli7GWjVqvLDLTs/U7Le77UrFkTgwcPxuDBg6HX6zFgwAC88cYbmDlzJry9vS2+5quvvkLjxo2xYcMGs/ebM2eOXe9p7M3o6elpM7j86quvcO+992Lp0qVmy/Py8mzeYGvUqIHIyMhS13tlbNmyBUVFRfj666/NMizlqbpxNGO148WLFwGU77fT1vmSkZEBd3f3Cv8GKxHbzCjMBx98YPZv4wiSvXv3Nlt+5MgRuLm5ISYmpsz9rVixAo0bN8ZTTz2FgQMHmv1Nnz4dfn5+ZsOJDx48GJmZmfj0009x9OhRDB482Gx/AwYMgIeHB+bOnVvqiUUIgdzcXJuf0Xgjuv31er2+1BQN7du3R1BQED755BPcvHnTtHzlypWlnuoGDRqECxcu4JNPPin1fjdu3HDI6LCVfQ93d3f0798fW7ZsweHDh0utt/YEqNPpzD4/cCuwcXd3L7Pb+YMPPohffvkFKSkppmXXr1/H//73PzRq1MghGYzb2XvuPv7447h69SqefPJJ5Ofn47HHHrO5b19fX0yfPh1paWl46aWXSq3/5ptvsGzZMiQkJJj1ZAJu9TYytnsCbh3Pzz//HG3atLFYxQTc6i0SGRmJt99+G/n5+aXWX758GYD936mxPZA9I2/feQ15eXmhefPmEEKYVR3dydJ1dfDgQbPvvyx16tRBjx498PHHH5tuwLczfmbje915vq5bt87UpsaWmJgYi8eroix9dq1Wi+TkZIe9hzU7d+60eO0a24oZe1uV57ezZs2aZbbzOXLkCFq0aGGz6qwqYWZGYTIyMvDwww/jgQceQEpKClasWIFhw4aVGp/jxx9/RNeuXcusn87MzMTOnTsxefJki+tVKhUSEhKwbt06LFq0CJ6ennjwwQfh7++P6dOnw8PDA4mJiWaviYyMxOuvv46ZM2eaukn7+/sjIyMDGzduxLhx4zB9+vQyP2OXLl1Qq1YtjBw5EpMnT4abmxu++OKLUhe4l5cXXnnlFUyaNAn33XcfBg0ahDNnzmDZsmWIjIw0e3p5/PHHsXbtWjz11FPYuXMnunbtipKSEvz5559Yu3ataVyKynDEe7z55pv44YcfEBcXZ+reffHiRaxbtw779u2zOFbFjh07MHHiRDz66KO46667cPPmTXzxxRcWv5/bzZgxA6tXr0bv3r0xefJkBAYGYvny5cjIyMD69esdPlqwvedu27Zt0bJlS1ODanu76M6YMQOpqamYP38+UlJSkJiYCB8fH+zbtw8rVqxAs2bNsHz58lKvu+uuuzBmzBgcOnQIISEh+Oyzz5CdnV3mjc7d3R2ffvopevfujRYtWmDUqFGoW7cuLly4gJ07d0KtVpvaV9nznbZp0wYeHh6YP38+tFotVCqVaUyUO91///0IDQ1F165dERISgj/++AP//e9/0adPnzIbQD/00EPYsGEDHnnkEfTp0wcZGRn46KOP0Lx5c4sBmSUffPABunXrhlatWmHs2LFo3LgxsrOzkZKSgn/++cc0jsxDDz2EV199FaNGjUKXLl1w/PhxrFy50upYVXfq168fvvjiC/z1118OyS7cf//98PLyQt++fU1B8ieffII6depYDMwcadKkSSgoKMAjjzyC6Oho6PV6/Pzzz/jyyy/RqFEjUweI8vx2tmvXDl9++SWmTZuGDh06wM/PD3379gVwa/Tk3bt3Y/z48U79XLLjol5TVEnG7q0nT54UAwcOFP7+/qJWrVpi4sSJpUaOzcvLE15eXuLTTz8tc5/vvPOOACC2b99udRtjN+fNmzeblg0fPlwAEPHx8VZft379etGtWzdRs2ZNUbNmTREdHS0mTJgg0tLSTNvExcVZ7WK6f/9+0blzZ+Hj4yPCw8NN3RlhofvqokWLRMOGDYVKpRIdO3YU+/fvF+3atSs12qterxfz588XLVq0ECqVStSqVUu0a9dOzJ07V2i12rIOlcURgC2x9z0AiAkTJljcx9mzZ8WIESNE7dq1hUqlEo0bNxYTJkwwdQu9s5vw33//LUaPHi0iIyOFt7e3CAwMFPfee6/46aefzPZ7Z9dsIYQ4ffq0GDhwoAgICBDe3t6iY8eOYuvWrWbbGN/vzq7fxi68ycnJZR6T8py7RgsWLBAAxJtvvlnmvu9UUlIikpOTRdeuXYVarRbe3t6iRYsWYu7cuRa7dhtHAP7+++9F69athUqlEtHR0aU+653H3Cg1NVUMGDBABAUFCZVKJRo2bCgGDRpU6pqy9Z0KIcQnn3wiGjduLDw8PMrspv3xxx+L2NhY03tGRkaK5557zuY5bDAYxJtvvmm6Vtq2bSu2bt0qRo4cadbN1/i9vvXWWxb3c/r0aTFixAgRGhoqPD09Rd26dcVDDz0kvvrqK9M2hYWF4tlnnxVhYWHCx8dHdO3aVaSkpJTq9mxNUVGRCA4OFq+99prVbax1zbY2ovPXX38tWrduLby9vUWjRo3E/PnzxWeffSYAiIyMDNN21rpmV/T8/+6778To0aNFdHS08PPzE15eXqJJkyZi0qRJFkcAtue3Mz8/XwwbNkwEBAQIAGbH4bvvvhMARHp6epnlqmrchKhg6z2Srffeew8LFizA6dOnHdrIVCkMBgNq166NAQMGWKzyIfkzzstz5syZUr1IpLB9+3bEx8dj7969Zl3ZyXlee+01JCcnIz093WabKfpX//794ebmZlZ1Wh2wzUwVU1xcjIULF+Lll1+uFoFMYWFhqeqnzz//HFeuXCk1nQEpgxACS5cuRVxcnCwCGeDfRprVqXeI1KZOnYr8/HyXTuKqdH/88Qe2bt2K1157TeqiuBwzM6Rou3btwtSpU/Hoo48iKCgIv/76K5YuXYpmzZrhyJEjTpvlmBzv+vXr+Prrr7Fz50588skn2Lx5Mx5++GHJy7Ry5Uq8//770Ol0OHv2bJWeeZxIqdgAmBStUaNGqF+/PhYtWoQrV64gMDAQI0aMwLx58xjIKMzly5cxbNgwBAQE4MUXX5Q8kDGWadKkSWjVqhWSk5MZyBDJFDMzREREpGh8zCAiIiJFYzBDREREilbl28wYDAZkZmbC39+/Wk26RUREpGRCCFy7dg3h4eE226tV+WAmMzPTrkkEiYiISH7Onz9vc8LcKh/MGIf3Pn/+PNRqtcSlISIiInvodDrUr1+/zGk6jKp8MGOsWlKr1QxmiIiIFMaeJiJsAExERESKxmCGiIiIFI3BDBERESkagxkiIiJSNAYzREREpGgMZoiIiEjRGMwQERGRojGYISIiIkVjMENERESKxmCGiIiIFK3KT2dAVFnaAj1y8vXQFRZD7eOJ4Jpe0Ph6SV0sIiL6fwxmiMqQmXcDL6w/hr3pOaZlsVHBmJfYGuEBPhKWjIiIjFjNRGSFtkBfKpABgD3pOZix/hi0BXqJSkZERLdjMENkRU6+vlQgY7QnPQc5+QxmiIjkgMEMkRW6wuIy11+zsZ6IiFyDwQyRFWpvzzLX+9tYT0RErsFghsiKYD8vxEYFW1wXGxWMYD/2aCIikgMGM0RWaHy9MC+xdamAJjYqGPMTW7N7tsS0BXqcvpSP1HNXcfpyPhtkE1Vj7JpNVIbwAB8sHtoWOfl6XCsshr+3J4L9OM6M1Nhlnohux8wMkQ0aXy9E1vFDmwa1EFnHj4GMxNhlnojuxGCGiBSFXeaJ6E4MZohIUdhlnojuxGCGiBSFXeaJ6E4MZohIUZzVZZ69o4iUi72ZSPY4azXdzthlfsb6Y9hzR2+minaZZ+8oImVzE0IIqQvhTDqdDhqNBlqtFmq1WuriUDnxJkPWGIPcynaZ1xboMXF1qsVGxbFRwVg8tC2DZyIJlOf+zWomki12waWyOKrLPHtHESkfq5lItuy5yfCJmSqLvaOch1XE5CoMZki2eJMhV2DvKOdgFTG5EquZSLZ4kyFX4ISijscqYnI1BjMkW7zJkCtwQlHHYzskcjVJg5mSkhLMmjULERER8PHxQWRkJF577TXc3sFKCIHZs2cjLCwMPj4+iI+PR3p6uoSlJlfhTYZcxTih6PZpcdg0vgu2T4vD4qFtEcbqkAphFTG5mqRtZubPn48PP/wQy5cvR4sWLXD48GGMGjUKGo0GkydPBgAsWLAAixYtwvLlyxEREYFZs2YhISEBJ0+ehLe3t5TFVySlNcjjrNXVj1TnqMaX55WjsIqYXE3SYObnn39Gv3790KdPHwBAo0aNsHr1avzyyy8AbmVl3nvvPbz88svo168fAODzzz9HSEgINm3ahCFDhkhWdiVSaoM83mSqD6Weo2TOWEW8x8rYPawiJkeTtJqpS5cu2L59O/766y8AwNGjR7Fv3z707t0bAJCRkYGsrCzEx8ebXqPRaNCpUyekpKRY3GdRURF0Op3ZH7FBHskfz1F5K890D6wiJleTNDMzY8YM6HQ6REdHw8PDAyUlJXjjjTcwfPhwAEBWVhYAICQkxOx1ISEhpnV3SkpKwty5c51bcAXimC0kdzxH5asiGTNWEZMrSZqZWbt2LVauXIlVq1bh119/xfLly/H2229j+fLlFd7nzJkzodVqTX/nz593YImViw3ySO54jspTZTJmjhqlmcgWSTMzzz33HGbMmGFq+9KqVSucPXsWSUlJGDlyJEJDQwEA2dnZCAsLM70uOzsbbdq0sbhPlUoFlUrl9LIrjdIa5CmtoTJVntLO0eqCGTNSAkkzMwUFBXB3Ny+Ch4cHDAYDACAiIgKhoaHYvn27ab1Op8PBgwcRExPj0rIqnZLGbMnMu4GJq1PRc+FuPLLkZ/R8ZzcmrU5FZt4NqYtGTqSkc7Q6YcaMlEDSYKZv375444038M033+DMmTPYuHEjFi5ciEceeQQA4ObmhilTpuD111/H119/jePHj2PEiBEIDw9H//79pSy64iilQR4bgVZfSjlHqxtmzEgJJK1mWrx4MWbNmoXx48fj0qVLCA8Px5NPPonZs2ebtnn++edx/fp1jBs3Dnl5eejWrRu2bdvGMWYqQAkN8uSc0mbVl/Mp4RytbtjNmpTATdw+3G4VpNPpoNFooNVqoVarpS4O2ZB67ioeWfKz1fWbxndBmwa1XFiiWzj+CVVnmXk3MGP9MbOAxpgx4yjJ5CzluX9z1mySFXtT2q7Mktiq+lo8tC0zB1SlMWNGcsdghmTFnpS2q7Mkcq76InIVjsRNcsZZsxWqPKNxKomtRqAAXN5AWOm9OarquUJEZMTMjAJV9fYbZaW0T1/Kd3mWRMm9Oar6uUJEBDAzozjVpeuytZFDpciSKHX8k+pyrhARMTOjMDn5ehw5exUT72uCtvUDUHTTAG9PD/x67io+25dR7syE0robS5ElMVZ9WevNIdfjzbY+RFRdMJhRmPyiYiwa2hbJ+zPw3x2nTMu7NgnCoqFtcb3I/syEEqsgpBrzwhG9OVx9vJXe1oeIyF6sZlKYAB8vJO/PwP5TuWbL95/KRfL+DGh87Lu52lsFUdHGo85qdCrlKLGVmTRPiiofJbf1qU7YQJuo8piZURh9iaFUIGO0/1Qu9CUGu/ZjTxXEdX1JhTIJzs5AKHHMCymqfOQ2cqvSqjRdQYnZUSI5YmZGYfKLbpa5/rqN9Ua2qiC0N4orlElwVQaiMlkSKUhR5SOnuY44eWhpbKBN5DjMzCiMo6oObO3H18ujQpmEimQgqsMTu1RVPnLIYlWFEZSdcY6ygTaR4zCYcRFH/Rg6qurA1n7c3d3KfL21TEJ5MxDVJc0uZZWP1CO3Kv2m7axzlA20iRyH1Uwu4MgUu6OqDmztp4aNYMZaJqE8GYjqlGaXU5WPqyn5pu3Mc5QNtIkch5kZJ3NGit1RVQdl7UdboK9QJqE8GQilP7GXlxyqfKSg5Ju2M89RuTXQJlIyZmaczJ4fw4pwVANYa/upaCahPK9T8hN7RSmt4bIjKHUEZcC552h1ztYRORozM06m5Bt2RTMJ9r5OyU/sZD9HjqDsas4+R6trto7I0RjMOJnSb9gVbTxqz+uYZq8+lHrTdsU5KnUDbaKqgNVMTqbkFLuzMc1evSixio3nKJEyuAkhhNSFcCadTgeNRgOtVgu1Wi1JGTLzblhNsYdVoe7HFWXstn6tsBg1VTXg5eGOvBt6+HlXzTFnSHluP0edmVWqDmMuEdmrPPdvBjMu4qofQyWrLmPOEFnC85/IHIOZ28glmKGyaQv0mLg61WLPr9ioYEWMEktUUTz/Sc6kyhiW5/7NBsAkC9VtzBmi2/H8J7lSSsaQDYBJFpTchZ2osnj+kxwpaZR2ZmZIFqTsws5GlyQ1pQ/hQFWTkjKGDGZchDfMskk15oxSUqhUtXHMJZIjJWUMWc3kAo6caLKqkmI8j4qmULUFepy+lI/Uc1dx+nK+bFKtci0X2cbxbEiOlJQxZGbGyZwx0WRV5epRYiuSQpVrJkeu5SL7KXWUZKq6lJQxZGbGyWzdMC9dK3JxieTNlaPEljeFKtfGcHItF5WfEkdJpqpLSRlDZmaczNYN89yVAtRU1ajw0zPb4lRceVOocm0MJ9dyUWm8XklplJIxZDDjJMYfLVUN28mvilY3sWqhcsqbQpVrYzi5lovM8XolpVLCZKisZnKC2xv8fnsiC12bBFncrmuTIKSezzM9PZcHqxYqr7wpVLk2hpNruehfvF6JnIuZGQe780frs30ZWDS0LQBg/6lc03ZdmwRhVNcITF6dCqD8T8+sWnCM8qRQ5doYTq7lon/xeiVyLgYzDnbnj1aBvgSTV6didLcIjO4agZqqGrhedBOp5/MweXUqCvQlAMr/9MyqBcexN4VqzORYmwFdqpuRXMuldI5s38Lrlci5GMw4mKUfrQJ9Cf674xQAYMnwezB+5a9m6yvy9MyqBWnItTGcXMulVI5u38Lrlci5GMw4mK0frQAf8/UVfXr2866B7lHBVmfZdXXVQnXqpSHXxnByLZfSOGNsKFYFEjkXgxkHs/WjFVnHD9unxVXq6Tkz7wZmbz6BkV0awSCEWVscKaoW2EuDqhJntG9hVSCRc7kJIYRUb96oUSOcPXu21PLx48fjgw8+QGFhIZ599lmsWbMGRUVFSEhIwJIlSxASEmL3e+h0Omg0Gmi1WqjVakcW36rMvBtWf7TCbru5VySboS3QY+LqVOxNz4GvlwdGd4tA2/oBKLppQICPJyLr+CFE7W13WSubUbm9PHeKjQrmCMekOKnnruKRJT9bXb9pfBe0aVCrQvs2Xm+sCiSyrTz3b0kzM4cOHUJJSYnp3ydOnECvXr3w6KOPAgCmTp2Kb775BuvWrYNGo8HEiRMxYMAA7N+/X6oi28We9gsVzWbc/tR4e1sco+3T4hBiZ8zmiIwKe2lQVePM9i2sCiRyDkmDmdq1a5v9e968eYiMjERcXBy0Wi2WLl2KVatW4b777gMAJCcno1mzZjhw4AA6d+4sRZHtZulHy/hUpr2hR9FNA+6uH4AjZ68CgCnD8sdFHa4X3UQdf5XFHz1H9YpwVLsA9tKgqobtW4iURzZtZvR6PVasWIFp06bBzc0NR44cQXFxMeLj403bREdHo0GDBkhJSbEazBQVFaGo6N/5jnQ6ndPLbou2QI+rBcWYtek49t4x1sx/h7WFG9zw6b6/zbIs1jIkjnpqdFRGhb00yladGkZXFWzfQqQ8sglmNm3ahLy8PDzxxBMAgKysLHh5eSEgIMBsu5CQEGRlZVndT1JSEubOnevEkpZPZt4N7P7rMrYeyzRrqAvcGkTPHUDvVmGl1lnLkDjqqdFRGRU+xVrHhtHKxa7uRMoim+kMli5dit69eyM8PLxS+5k5cya0Wq3p7/z58w4qYfkZq3Lq+KtKBStGe0/lWm2wa2maA0fNYlqejIq2QI/Tl/KReu4qTl/ONxt6XUmzqjqLpePD4euVjzNYEymHLDIzZ8+exU8//YQNGzaYloWGhkKv1yMvL88sO5OdnY3Q0FCr+1KpVFCpVM4srt2MVTlDOzYoc7uimwar6yxlSBzx1GhvRsWe7IJcnmKlqNKxdnxe6tOMDaOJiFxEFsFMcnIy6tSpgz59+piWtWvXDp6enti+fTsSExMBAGlpaTh37hxiYmKkKmq5GKtybM2cXdZ6a21OKtsrwp52AeVpJCx1Lw0pqnTKOj4jr94o87VsGE1E5DiSBzMGgwHJyckYOXIkatT4tzgajQZjxozBtGnTEBgYCLVajUmTJiEmJkb2PZmMjFU5qefz0LVJkMWqpu5Rwbh0rajUcsD5bU5sZVQc0UjYFdkSZ4zYao+yjo8t1b1hNBGRI0kezPz00084d+4cRo8eXWrdu+++C3d3dyQmJpoNmqcUxqqcO2fONg5216VxEFQ13OHv7YmkAa3w2taTpoknXdXmpKyMSmUbCbsqWyLVWDdlHZ/U83k2p5tgTyciIseQdARgV5BiBODbGUcDPnz2KkZ3i0D7BrUQHuCD17b+btZNOzYqGK/2awndDT1qquTRc+L0pXz0XLjb6vrt0+IQWcfP4jpXjgzszBFby1LW8fH18sC3k7tj9uYTFqvxAOB59nQiIrJKMSMAVwd3VuXU8vXCy5tOmAUywK0MwuzNJ2Q1/H9lul27Mlsi1Vg3ZR2f9g1roZavp8VqPAAWAz1nV4sREVVVsumaXZXd3sXzpkFg76myb/LWlNVF2hkq0+3alSMDG4MKS5zZ7sie42Ope6+tQO9C3g2XfL9ERFUFMzMuVtGbvFQDsFW027UrsyVSjthakeNj6xw4k1uA8St/ZbUTEZGdGMw42Z2NPP1UZR9ySzd5qXrrGFWk27WrRwaWcqyb8h4fW4Gesas+q52IiOzDYMaJLGVTkga0stnL5U5KnJla4+uFpAGtcDa3AHk3iuHt6YFfz11F2kUdXu3X0inllXqsG3uVFeh1bRKE1PN5pn/L9fslIpITBjNOYi2b8trWk/jsiQ5wA+yuErm9WsLYrbtt/QAU3TTA29MDBhd2SLO3O3Fm3g3M2HDc7PN3jwpG0iOtEFbNq02sVYt1bRKEUV0jMHl1qtn2HGCPiKhsDGacxFo2pUBfgtHLDuG7yd1x0yDsqhIxVkv4enlg0dC2SN6fYTbDdvf/D4Sc3bbC3nY71gK5vek5eHHjcVabwLxa7GqBHtobxUg9n4fJq1NNYw0ZcYA9IqKysTeTk5TVyLNAX4KrBXq7J7EzVkuM7haB5P0ZpUYS3uuCyQvLM3GiPdVi9G8vt6g6flj+8xn8d8epUoFMdZ95nIjIHgxmnMSRvXmM1RJdGlueEgFwfpBQngDFld2yqwLOPE5EVDmsZnISR/fmCQ/wQZa27MkL8244L5gpT4Ai1SB2SiaXmceJiJSImRknccbTtsan7NcUFRucVtVUngBFqkHslM7SAHtERGQbMzNO5Oin7WA/L6vdurs3CYK/tyfyCoqdchMsT6ZJim7ZRERUfXGiSYU5fTkfszefMGs7c3uX3nYNazmtZ5Nx0kxLXcpv725tqdeTsVt2vUBfh5eLiIiqnvLcvxnMKMzfl/OxIfUCetxVG9obt9qppJ7Pw2f7Mkw9YRw9K/XtjOPMWMs0uXK2bCWyd5weIqLqjrNmV2FBNb1w7Hwe2tYPwJjlhy1u48xRY22NsqvE0YpdRar5tYiIqjo2AFYYY8NiW6x1f3b2zNvslm1ZecbpISKi8mFmRoHCA3xwvehmmdtY6v7siswAu2WXpi3Q46K2EEM7NsCorhH49dxVs2rB6p6xIiKqLGZmHMjZWY/b1fFXlav7s6syA+yWbS4z7wYmrk7FA+/vxfiVv2L0skNIPXcVi4a2ha+Xh2m76pqxIiJyBAYzDmK8afVcuBuPLPkZPd/ZjUmrU5GZV/ZAdxVlzzg2twdXF3WFuLt+gNkN1MiRowdzNNt/WQsg95/KRfL+DIzuFmFaVh0zVkREjsJqJgewlfUw9uBxdE+WssaxsVSl1LVJEBYNbWtxMkNHZgY4mu0tZTWG3n8qF6O73gpmqmPGiojIkRjMOIA9PXiu60uc0l7FUu+isjICADC6W4TZrNuA4zMDtno9VQe2GkMX3TRUy4yVq7AbPJFzyekaYzDjALZuWtobxXhly+82MzcVdecJZTAIuzICRswMOIetxtCNg2tW+3F3nIXd4ImcS27XGNvMOICtm5avl4fdM06Xl6W2OueuFJT5mqKbBtP/MzPgPLYaQ4dpvHncnYDd4ImcS47XGDMzDmBr3iJ3d7cyX1/R9irWTihbGgfXxKbxXaptWxZXMTaGtjYFBI+7c3DgRiLnkuM1xmDGAWzdtG4Ul5Tx6oq3V7F2QqWez0PXJkFm8zfdXiZmBFyHjaFdjwM3EjmXHK8xBjMOUtZNS1ugt3vG6fKwdkJ9ti8Di4a2hbubW6n6TGYEXI+NoV2LAzcSOZccrzEGMw5k7ablrOoGaydUgb4Ek1en4rvJ3XHTIJgRoGrFVrUvG7tXPXLqVVMdyPEaYzDjIvZUN5T3gizrhGrfsBYCfD15QVO1w7ZK1YvcetVUB3K8xtyEEMLl7+pC5ZlC3BnsDVCMF+SRs1cxulsE2tYPAADUr+WLELXK6smRmXfD6gkVxguZqjHjtcfMZNWlLdBj4upUi20HY6OCOfSBkzn7GivP/ZuZGSey94nB2CvpyNlbc/Yk788wG9SurKcMNjC1H1PR1QvbKlV9cuxVU53I6RpjMOMk9k5xAAB5BcV4oksjTOt1Fxb+kFaqF5KtwfXkdELJFVPRRFWPHHvVkDQ4aJ6T2PPEANy6yb686TjGLD+MK9f12GuhO/Wdr6HykeMAT0RUeXLsVUPSYDDjJPY8MZhusv8fwNw+Mq+111D52RtYEpGy2Bplmz3Xqg8GM05izxPDnTdZVY2yvw4+ZVQMU9FEVZOxV82dAQ17rlU/bDPjJPb0w/8757rZclsj9/Ipo2KYiiaqutgJggAZZGYuXLiAxx57DEFBQfDx8UGrVq1w+PBh03ohBGbPno2wsDD4+PggPj4e6enpEpbYPvY8Mdx5k/1sXwZGdY1A1yZBVl8jJ9oCPU5fykfquas4fTlftm1PmIomqto0vl6IrOOHNg1qIbKOn+x+K8n5JB1n5urVq2jbti3uvfdePP3006hduzbS09MRGRmJyMhIAMD8+fORlJSE5cuXIyIiArNmzcLx48dx8uRJeHt723wPuYwzY+mJQVugx6TVqWbZG18vD4zuFoEujYPg7ekBjY88nzKU1juI4/EQESlLee7fkgYzM2bMwP79+7F3716L64UQCA8Px7PPPovp06cDALRaLUJCQrBs2TIMGTLE5ntIHczYosSbrFIHquIgakTVA8eUqhoUM2je119/jYSEBDz66KPYvXs36tati/Hjx2Ps2LEAgIyMDGRlZSE+Pt70Go1Gg06dOiElJcViMFNUVISioiLTv3U6nfM/SCUY63tzr+tRYhAoMQgU6G+ioLgE2gJ5Dvik1IGqOB4PUdWntKwxOYakbWb+/vtvfPjhh4iKisL333+Pp59+GpMnT8by5csBAFlZWQCAkJAQs9eFhISY1t0pKSkJGo3G9Fe/fn3nfggH0Ph6wdvTA69uPYkH3t+LAR+moOc7uzFpdSoy825IXbxS2DuIiOSIY0pVX5IGMwaDAffccw/efPNNtG3bFuPGjcPYsWPx0UcfVXifM2fOhFarNf2dP3/egSV2DqVdgK7oHaSUxsVEJB8cU6r6krSaKSwsDM2bNzdb1qxZM6xfvx4AEBoaCgDIzs5GWFiYaZvs7Gy0adPG4j5VKhVUKpVzCuwkSqu2cfb070wTE1FFMGtcfUmamenatSvS0tLMlv31119o2LAhACAiIgKhoaHYvn27ab1Op8PBgwcRExPj0rI6k9IuQGcOVKW0LBURyQfHlKq+JM3MTJ06FV26dMGbb76JQYMG4ZdffsH//vc//O9//wMAuLm5YcqUKXj99dcRFRVl6podHh6O/v37S1l0h1LiBeisgaqUlqUiIvlwdtaY5EvSYKZDhw7YuHEjZs6ciVdffRURERF47733MHz4cNM2zz//PK5fv45x48YhLy8P3bp1w7Zt2+waY0YplHoBOqN3kFyyVOzaSaQ8xqyxteEueA1XXZKOM+MKch9nxkiJ4804w+lL+ei5cLfV9dunxSGyjp9Ty8A2O0TKxjGlqgbFjDND/3L1/CJyzTxInaWy1WZHrgMCEtG/OKZU9cNgRkZcdQHKOfMgdZq4Mm125BogErkSrwOSAoOZakYJmQcpZ8GtaJsdOQeIRK7C64CkIvms2WQ/Rwwkp5RBpcqaBdeZA+pVpGcZu5MT8TogaTEzoxCOeuKRS2+hinL2k19F2uywOzkRrwOSFjMzCuDIJx4ljmlj5Ionv4oMCKj0AJHIEXgdkJSYmVEARz7xSN1bqDJc9eRX3jY7Sg4QiRyF1wFJiZkZBXDkE48zpyJwNlc++ZXVZudOxgDRErkHiESOwuuApMTMjAI4+olHyt5ClSHXJz+pu5MTyQGvA5ISgxkFcEbVkBIHlZJzFZlSA0QiR+J1QFLhdAaV5KoBojjdwS08DkRE1UN57t8MZirB1QNEcb6RW3gciIiqPgYzt3FWMKMt0GPi6lSLvWtio4LtGkmXw34TERFZxokmXaCy3YQ57DcREZFjsGt2BWgL9Ci6WYIlw+/BZ090wMT7msDXy8Nsm7K6CXPYbyIiIsdhZqacLGVUujYJwqKhbTF5dSoK9CUAyu4mLNWw36zWIqo6eD0T/YvBTDlYy6jsP5ULABjdLQL/3XHKZjdhKYb9ZrUWUdVx4WoBzuYWIO9GMbw9PbD9z0tIu6jD3H4teT1XYQxgrWMwUw5lZVT2n8rF6K4Rdg0Q5erB32xVa9nTWJmI5OGfKwV4YcMx00MUcCs7PKprBOZsPoG3H72b13MVxAfSsrHNTDnYyqhofDyxeGhbm+OduHrYb3uqtYhI/rQFesy8I5ABbj1MJe/PQNMwNa/nKojtLG1jMFMOtjIqtewcVdfV8yNxNluiqiEnX4+9dwQyRvtP5aJt/QBez1UQH0htYzVTOThyOH1XDvst1zmNiKh8bD2YFN008HqugvhAahszM+Xg6IxKeWZmrgzOZktUNdh6MAnw8eT1XAXxgdQ2ZmbKSYkTqXE2W6KqoazscLcmQWgY5MvruQqS8yS7csHpDJxMTl3pOKcRkfJZmmy1e1Qwkh5phXqBvhKWjJypOk6yy7mZbiNlMMOudETkDHwwqZ6q2/fOYOY2UgUzjpiIkoiIqLoqz/2bDYCdhF3piIiIXIMNgJ3EFV3p5NQeh4iISCoMZpzE2V3p2B6HiIjoFlYzOYkzx3bh0NZERET/YjDjJM6csoDtcYiIiP7FaiYnctYAexzamoiI6F8MZirJViNcjZ2TT5YHh7YmIiL6F4OZSpCqES6HtiYiIvoX28xUkJSNcJ3ZHoeIiEhpmJmpIHsa4TozqFDihJdERETOIGlm5pVXXoGbm5vZX3R0tGl9YWEhJkyYgKCgIPj5+SExMRHZ2dkSlvhfcmiEq/H1QmQdP7RpUAuRdfwYyBARUbUkeTVTixYtcPHiRdPfvn37TOumTp2KLVu2YN26ddi9ezcyMzMxYMAACUv7LzbCJSIikgfJq5lq1KiB0NDQUsu1Wi2WLl2KVatW4b777gMAJCcno1mzZjhw4AA6d+7s6qKaYSNcIiIieZA8M5Oeno7w8HA0btwYw4cPx7lz5wAAR44cQXFxMeLj403bRkdHo0GDBkhJSbG6v6KiIuh0OrM/Z2AjXCIiInkoV2ZGp9Ph4MGD0Ov16NixI2rXrl2pN+/UqROWLVuGpk2b4uLFi5g7dy66d++OEydOICsrC15eXggICDB7TUhICLKysqzuMykpCXPnzq1UuezFRrhERETSszuY+e233/Dggw8iOzsbQgj4+/tj7dq1SEhIqPCb9+7d2/T/rVu3RqdOndCwYUOsXbsWPj4VG6dl5syZmDZtmunfOp0O9evXr3AZbXHGoHhERERkP7urmV544QVERERg3759OHLkCHr27ImJEyc6tDABAQG46667cOrUKYSGhkKv1yMvL89sm+zsbIttbIxUKhXUarXZHxEREVVddgczR44cweLFixETE4O2bdvis88+w+nTpx3aJiU/Px+nT59GWFgY2rVrB09PT2zfvt20Pi0tDefOnUNMTIzD3pOIiIiUze5qpitXrqBevXqmfwcEBKBmzZrIzc2tcPZj+vTp6Nu3Lxo2bIjMzEzMmTMHHh4eGDp0KDQaDcaMGYNp06YhMDAQarUakyZNQkxMjOQ9mYiIiEg+ytUA+OTJk2aNb4UQ+OOPP3Dt2jXTstatW9u9v3/++QdDhw5Fbm4uateujW7duuHAgQOmhsXvvvsu3N3dkZiYiKKiIiQkJGDJkiXlKTIRERFVcW5CCGHPhu7u7nBzc0NZm7u5uaGkpMRhhXMEnU4HjUYDrVbL9jNEREQKUZ77t92ZmYyMDJvb3J6hISJSMm2BHjn5eugKi6H28URwTfZcJJIru4OZhg0bWlx+7do1rF69GkuXLsXhw4dll5khIiqvzLwbeGH9MbPJZGOjgjEvsTXCAyo2bAQROU+FRwDes2cPRo4cibCwMLz99tu49957ceDAAUeWjYjI5bQF+lKBDADsSc/BjPXHoC3QS1QyIrKmXA2As7KysGzZMixduhQ6nQ6DBg1CUVERNm3ahObNmzurjERELpOTry8VyBjtSc9BTr6e1U1EMmN3ZqZv375o2rQpjh07hvfeew+ZmZlYvHixM8tGRORyusLiMtdfs7GeiFzP7szMd999h8mTJ+Ppp59GVFSUM8tERCQZtbdnmev9bawnItezOzOzb98+XLt2De3atUOnTp3w3//+Fzk5llOxRERKFeznhdioYIvrYqOCEezHKiYiubE7mOncuTM++eQTXLx4EU8++STWrFmD8PBwGAwG/Pjjj+yWTURVgsbXC/MSW5cKaGKjgjE/sTXbyxDJkN2D5lmSlpaGpUuX4osvvkBeXh569eqFr7/+2pHlqzQOmkdEFWEcZ+ZaYTH8vT0R7MdxZohcqTz37wp3zQaApk2bYsGCBfjnn3+wevXqyuyKiEhWNL5eiKzjhzYNaiGyjh8DGSIZq1RmRgmYmSEiIlIel2VmiIiIiKTGYIaIiIgUrVwjABMREREnIpUbBjNERETlwIlI5YfVTERERHbiRKTyxGCGiIjITvZMREqux2omIqJqhG09KocTkcoTgxkiomqCbT0qjxORyhOrmYiIqgG29XAMTkQqTwxmiIiqAbb1cAxORCpPrGYiIqoG2NbDccIDfLB4aFtORCojDGaIiKoBtvVwLI0vgxc5YTUTEVE1wLYeVJUxmCEiqgbY1oOqMlYzERFVE2zrQVUVgxkiomqEbT2oKmI1ExERESkagxkiIiJSNAYzREREpGgMZoiIiEjRGMwQERGRojGYISIiIkVjMENERESKxmCGiIiIFE02wcy8efPg5uaGKVOmmJYVFhZiwoQJCAoKgp+fHxITE5GdnS1dIYmIiEh2ZBHMHDp0CB9//DFat25ttnzq1KnYsmUL1q1bh927dyMzMxMDBgyQqJREREQkR5IHM/n5+Rg+fDg++eQT1KpVy7Rcq9Vi6dKlWLhwIe677z60a9cOycnJ+Pnnn3HgwAEJS0xERERyInkwM2HCBPTp0wfx8fFmy48cOYLi4mKz5dHR0WjQoAFSUlKs7q+oqAg6nc7sj4iIiKouSSeaXLNmDX799VccOnSo1LqsrCx4eXkhICDAbHlISAiysrKs7jMpKQlz5851dFGJiIhIpiTLzJw/fx7PPPMMVq5cCW9vb4ftd+bMmdBqtaa/8+fPO2zfREREJD+SBTNHjhzBpUuXcM8996BGjRqoUaMGdu/ejUWLFqFGjRoICQmBXq9HXl6e2euys7MRGhpqdb8qlQpqtdrsj4iIiKouyaqZevbsiePHj5stGzVqFKKjo/HCCy+gfv368PT0xPbt25GYmAgASEtLw7lz5xATEyNFkYmIiEiGJAtm/P390bJlS7NlNWvWRFBQkGn5mDFjMG3aNAQGBkKtVmPSpEmIiYlB586dpSgyERERyZCkDYBteffdd+Hu7o7ExEQUFRUhISEBS5YskbpYREREJCNuQgghdSGcSafTQaPRQKvVsv0MERGRQpTn/i35ODNERERElcFghoiIiBSNwQwREREpGoMZIiIiUjQGM0RERKRoDGaIiIhI0RjMEBERkaIxmCEiIiJFYzBDREREisZghoiIiBSNwQwREREpGoMZIiIiUjQGM0RERKRoDGaIiIhI0RjMEBERkaIxmCEiIiJFYzBDREREisZghoiIiBSNwQwREREpGoMZIiIiUjQGM0RERKRoDGaIiIhI0RjMEBERkaIxmCEiIiJFYzBDREREisZghoiIiBSNwQwREREpGoMZIiIiUjQGM0RERKRoDGaIiIhI0RjMEBERkaIxmCEiIiJFYzBDREREisZghoiIiBSNwQwREREpGoMZIiIiUjRJg5kPP/wQrVu3hlqthlqtRkxMDL777jvT+sLCQkyYMAFBQUHw8/NDYmIisrOzJSwxERERyY2kwUy9evUwb948HDlyBIcPH8Z9992Hfv364ffffwcATJ06FVu2bMG6deuwe/duZGZmYsCAAVIWWVLaAj1OX8pH6rmrOH05H9oCvdRFIiIikpybEEJIXYjbBQYG4q233sLAgQNRu3ZtrFq1CgMHDgQA/Pnnn2jWrBlSUlLQuXNnu/an0+mg0Wig1WqhVqudWXSnysy7gRfWH8Pe9BzTstioYMxLbI3wAB8JS0ZEROR45bl/y6bNTElJCdasWYPr168jJiYGR44cQXFxMeLj403bREdHo0GDBkhJSbG6n6KiIuh0OrM/pdMW6EsFMgCwJz0HM9YfY4aGiIiqNcmDmePHj8PPzw8qlQpPPfUUNm7ciObNmyMrKwteXl4ICAgw2z4kJARZWVlW95eUlASNRmP6q1+/vpM/gfPl5OtLBTJGe9JzkJPPYIaIiKovyYOZpk2b4rfffsPBgwfx9NNPY+TIkTh58mSF9zdz5kxotVrT3/nz5x1YWmnoCovLXH/NxnoiIqKqrIbUBfDy8kKTJk0AAO3atcOhQ4fw/vvvY/DgwdDr9cjLyzPLzmRnZyM0NNTq/lQqFVQqlbOL7VJqb88y1/vbWE9ERFSVSZ6ZuZPBYEBRURHatWsHT09PbN++3bQuLS0N586dQ0xMjIQldL1gPy/ERgVbXBcbFYxgPy8Xl4iIiEg+JM3MzJw5E71790aDBg1w7do1rFq1Crt27cL3338PjUaDMWPGYNq0aQgMDIRarcakSZMQExNjd0+mqkLj64V5ia0xY/0x7LmjN9P8xNbQ+DKYISKi6kvSYObSpUsYMWIELl68CI1Gg9atW+P7779Hr169AADvvvsu3N3dkZiYiKKiIiQkJGDJkiVSFlky4QE+WDy0LXLy9bhWWAx/b08E+3kxkCEiompPduPMOFpVGWeGiIioOlHkODNEREREFcFghoiIiBSNwQwREREpGoMZIiIiUjQGM0RERKRoDGaIiIhI0RjMEBERkaIxmCEiIiJFYzBDREREisZghoiIiBSNwQwREREpmqQTTZJ8aQv0yMnXQ1dYDLWPJ4JrclJLIiKSJwYzVEpm3g28sP4Y9qbnmJbFRgVjXmJrhAf4SFgyIiKi0ljNRGa0BfpSgQwA7EnPwYz1x6At0EtUMiIiIssYzJCZnHx9qUDGaE96DnLyGcwQEZG8MJghM7rC4jLXX7OxnoiIyNUYzJAZtbdnmev9bawnIiJyNQYzZCbYzwuxUcEW18VGBSPYjz2aiIhIXhjMkBmNrxfmJbYuFdDERgVjfmJrds8mIiLZYddsKiU8wAeLh7ZFTr4e1wqL4e/tiWA/jjNDRETyxGCGLNL4MnghIiJlYDUTERERKRqDGSIiIlI0VjMR2cB5qoiI5I3BDFEZOE8VEZH8sZqJyArOU0VEpAwMZois4DxVRETKwGCGyArOU0VEpAwMZois4DxVRETKwGCGyArOU0VEpAwMZois4DxVRETKwK7ZRGXgPFVERPLHYIbIBs5TRUQkb6xmIiIiIkVjMENERESKJmkwk5SUhA4dOsDf3x916tRB//79kZaWZrZNYWEhJkyYgKCgIPj5+SExMRHZ2dkSlZiIiIjkRtJgZvfu3ZgwYQIOHDiAH3/8EcXFxbj//vtx/fp10zZTp07Fli1bsG7dOuzevRuZmZkYMGCAhKUmIiIiOXETQgipC2F0+fJl1KlTB7t370ZsbCy0Wi1q166NVatWYeDAgQCAP//8E82aNUNKSgo6d+5sc586nQ4ajQZarRZqtdrZH4GIiIgcoDz3b1m1mdFqtQCAwMBAAMCRI0dQXFyM+Ph40zbR0dFo0KABUlJSJCkjERERyYtsumYbDAZMmTIFXbt2RcuWLQEAWVlZ8PLyQkBAgNm2ISEhyMrKsrifoqIiFBUVmf6t0+mcVmYiIiKSnmwyMxMmTMCJEyewZs2aSu0nKSkJGo3G9Fe/fn0HlZCIiIjkSBbBzMSJE7F161bs3LkT9erVMy0PDQ2FXq9HXl6e2fbZ2dkIDQ21uK+ZM2dCq9Wa/s6fP+/MohMREZHEJA1mhBCYOHEiNm7ciB07diAiIsJsfbt27eDp6Ynt27eblqWlpeHcuXOIiYmxuE+VSgW1Wm32R0RERFWXpG1mJkyYgFWrVmHz5s3w9/c3tYPRaDTw8fGBRqPBmDFjMG3aNAQGBkKtVmPSpEmIiYmxqycTERERVX2Sds12c3OzuDw5ORlPPPEEgFuD5j377LNYvXo1ioqKkJCQgCVLllitZroTu2YTEREpT3nu37IaZ8YZGMwQEREpj2LHmSEiIiIqLwYzREREpGgMZoiIiEjRGMwQERGRojGYISIiIkVjMENERESKxmCGiIiIFI3BDBERESkagxkiIiJSNAYzREREpGgMZoiIiEjRGMwQERGRojGYISIiIkVjMENERESKxmCGiIiIFI3BDBERESkagxkiIiJSNAYzREREpGgMZoiIiEjRGMwQERGRojGYISIiIkVjMENERESKxmCGiIiIFI3BDBERESkagxkiIiJStBpSF4CIiEjJtAV65OTroSsshtrHE8E1vaDx9ZK6WNUKgxkiIqIKysy7gRfWH8Pe9BzTstioYMxLbI3wAB8JS1a9sJqJiIioArQF+lKBDADsSc/BjPXHoC3QS1Sy6ofBDBERUQXk5OtLBTJGe9JzkJPPYMZVGMwQERFVgK6wuMz112ysJ8dhMENERFQBam/PMtf721hPjsNghoiIqAKC/bwQGxVscV1sVDCC/dijyVUYzBAREVWAxtcL8xJblwpoYqOCMT+xNbtnuxC7ZhMREVVQeIAPFg9ti5x8Pa4VFsPf2xPBfhxnxtUYzBAREVWCxpfBi9RYzURERESKJmkws2fPHvTt2xfh4eFwc3PDpk2bzNYLITB79myEhYXBx8cH8fHxSE9Pl6awREREJEuSBjPXr1/H3XffjQ8++MDi+gULFmDRokX46KOPcPDgQdSsWRMJCQkoLCx0cUmJiIhIriRtM9O7d2/07t3b4johBN577z28/PLL6NevHwDg888/R0hICDZt2oQhQ4a4sqhEREQkU7JtM5ORkYGsrCzEx8eblmk0GnTq1AkpKSlWX1dUVASdTmf2R0RERFWXbIOZrKwsAEBISIjZ8pCQENM6S5KSkqDRaEx/9evXd2o5iYiISFqyDWYqaubMmdBqtaa/8+fPS10kIiIiciLZBjOhoaEAgOzsbLPl2dnZpnWWqFQqqNVqsz8iIiKqumQbzERERCA0NBTbt283LdPpdDh48CBiYmIkLBkRERHJiaS9mfLz83Hq1CnTvzMyMvDbb78hMDAQDRo0wJQpU/D6668jKioKERERmDVrFsLDw9G/f3/pCk1ERESyImkwc/jwYdx7772mf0+bNg0AMHLkSCxbtgzPP/88rl+/jnHjxiEvLw/dunXDtm3b4O3tbfd7CCEAgL2aiIiIFMR43zbex8viJuzZSsH++ecf9mgiIiJSqPPnz6NevXplblPlgxmDwYDMzEz4+/vDzc2t0vvT6XSoX78+zp8/z8bFTsTj7Bo8zq7B4+waPM6u4arjLITAtWvXEB4eDnf3spv4VvlZs93d3W1GdBXBnlKuwePsGjzOrsHj7Bo8zq7hiuOs0Wjs2k62vZmIiIiI7MFghoiIiBSNwUw5qVQqzJkzByqVSuqiVGk8zq7B4+waPM6uwePsGnI8zlW+ATARERFVbczMEBERkaIxmCEiIiJFYzBDREREisZghoiIiBSNwUw5fPDBB2jUqBG8vb3RqVMn/PLLL1IXSTZeeeUVuLm5mf1FR0eb1hcWFmLChAkICgqCn58fEhMTkZ2dbbaPc+fOoU+fPvD19UWdOnXw3HPP4ebNm2bb7Nq1C/fccw9UKhWaNGmCZcuWlSpLVfqe9uzZg759+yI8PBxubm7YtGmT2XohBGbPno2wsDD4+PggPj4e6enpZttcuXIFw4cPh1qtRkBAAMaMGYP8/HyzbY4dO4bu3bvD29sb9evXx4IFC0qVZd26dYiOjoa3tzdatWqFb7/9ttxlkStbx/mJJ54odX4/8MADZtvwONuWlJSEDh06wN/fH3Xq1EH//v2RlpZmto2cfivsKYsc2XOce/ToUeqcfuqpp8y2UdRxFmSXNWvWCC8vL/HZZ5+J33//XYwdO1YEBASI7OxsqYsmC3PmzBEtWrQQFy9eNP1dvnzZtP6pp54S9evXF9u3bxeHDx8WnTt3Fl26dDGtv3nzpmjZsqWIj48Xqamp4ttvvxXBwcFi5syZpm3+/vtv4evrK6ZNmyZOnjwpFi9eLDw8PMS2bdtM21S17+nbb78VL730ktiwYYMAIDZu3Gi2ft68eUKj0YhNmzaJo0ePiocfflhERESIGzdumLZ54IEHxN133y0OHDgg9u7dK5o0aSKGDh1qWq/VakVISIgYPny4OHHihFi9erXw8fERH3/8sWmb/fv3Cw8PD7FgwQJx8uRJ8fLLLwtPT09x/PjxcpVFrmwd55EjR4oHHnjA7Py+cuWK2TY8zrYlJCSI5ORkceLECfHbb7+JBx98UDRo0EDk5+ebtpHTb4WtssiVPcc5Li5OjB071uyc1mq1pvVKO84MZuzUsWNHMWHCBNO/S0pKRHh4uEhKSpKwVPIxZ84ccffdd1tcl5eXJzw9PcW6detMy/744w8BQKSkpAghbt1M3N3dRVZWlmmbDz/8UKjValFUVCSEEOL5558XLVq0MNv34MGDRUJCgunfVfl7uvMmazAYRGhoqHjrrbdMy/Ly8oRKpRKrV68WQghx8uRJAUAcOnTItM13330n3NzcxIULF4QQQixZskTUqlXLdJyFEOKFF14QTZs2Nf170KBBok+fPmbl6dSpk3jyySftLotSWAtm+vXrZ/U1PM4Vc+nSJQFA7N69Wwghr98Ke8qiFHceZyFuBTPPPPOM1dco7TizmskOer0eR44cQXx8vGmZu7s74uPjkZKSImHJ5CU9PR3h4eFo3Lgxhg8fjnPnzgEAjhw5guLiYrPjFx0djQYNGpiOX0pKClq1aoWQkBDTNgkJCdDpdPj9999N29y+D+M2xn1Ut+8pIyMDWVlZZp9Xo9GgU6dOZsc1ICAA7du3N20THx8Pd3d3HDx40LRNbGwsvLy8TNskJCQgLS0NV69eNW1T1rG3pyxKt2vXLtSpUwdNmzbF008/jdzcXNM6HueK0Wq1AIDAwEAA8vqtsKcsSnHncTZauXIlgoOD0bJlS8ycORMFBQWmdUo7zlV+oklHyMnJQUlJidmXCgAhISH4888/JSqVvHTq1AnLli1D06ZNcfHiRcydOxfdu3fHiRMnkJWVBS8vLwQEBJi9JiQkBFlZWQCArKwsi8fXuK6sbXQ6HW7cuIGrV69Wq+/JeFwsfd7bj1mdOnXM1teoUQOBgYFm20RERJTah3FdrVq1rB772/dhqyxK9sADD2DAgAGIiIjA6dOn8eKLL6J3795ISUmBh4cHj3MFGAwGTJkyBV27dkXLli0BQFa/FfaURQksHWcAGDZsGBo2bIjw8HAcO3YML7zwAtLS0rBhwwYAyjvODGbIIXr37m36/9atW6NTp05o2LAh1q5dCx8fHwlLRlR5Q4YMMf1/q1at0Lp1a0RGRmLXrl3o2bOnhCVTrgkTJuDEiRPYt2+f1EWp0qwd53Hjxpn+v1WrVggLC0PPnj1x+vRpREZGurqYlcZqJjsEBwfDw8OjVOvq7OxshIaGSlQqeQsICMBdd92FU6dOITQ0FHq9Hnl5eWbb3H78QkNDLR5f47qytlGr1fDx8al235PxM5X1eUNDQ3Hp0iWz9Tdv3sSVK1cccuxvX2+rLFVJ48aNERwcjFOnTgHgcS6viRMnYuvWrdi5cyfq1atnWi6n3wp7yiJ31o6zJZ06dQIAs3NaSceZwYwdvLy80K5dO2zfvt20zGAwYPv27YiJiZGwZPKVn5+P06dPIywsDO3atYOnp6fZ8UtLS8O5c+dMxy8mJgbHjx83uyH8+OOPUKvVaN68uWmb2/dh3Ma4j+r2PUVERCA0NNTs8+p0Ohw8eNDsuObl5eHIkSOmbXbs2AGDwWD68YqJicGePXtQXFxs2ubHH39E06ZNUatWLdM2ZR17e8pSlfzzzz/Izc1FWFgYAB5newkhMHHiRGzcuBE7duwoVe0mp98Ke8oiV7aOsyW//fYbAJid04o6znY3Fa7m1qxZI1QqlVi2bJk4efKkGDdunAgICDBr6V2dPfvss2LXrl0iIyND7N+/X8THx4vg4GBx6dIlIcStrncNGjQQO3bsEIcPHxYxMTEiJibG9HpjN8D7779f/Pbbb2Lbtm2idu3aFrsBPvfcc+KPP/4QH3zwgcVugFXpe7p27ZpITU0VqampAoBYuHChSE1NFWfPnhVC3OqmGxAQIDZv3iyOHTsm+vXrZ7Frdtu2bcXBgwfFvn37RFRUlFmX4by8PBESEiIef/xxceLECbFmzRrh6+tbqstwjRo1xNtvvy3++OMPMWfOHItdhm2VRa7KOs7Xrl0T06dPFykpKSIjI0P89NNP4p577hFRUVGisLDQtA8eZ9uefvppodFoxK5du8y6BBcUFJi2kdNvha2yyJWt43zq1Cnx6quvisOHD4uMjAyxefNm0bhxYxEbG2vah9KOM4OZcli8eLFo0KCB8PLyEh07dhQHDhyQukiyMXjwYBEWFia8vLxE3bp1xeDBg8WpU6dM62/cuCHGjx8vatWqJXx9fcUjjzwiLl68aLaPM2fOiN69ewsfHx8RHBwsnn32WVFcXGy2zc6dO0WbNm2El5eXaNy4sUhOTi5Vlqr0Pe3cuVMAKPU3cuRIIcStrrqzZs0SISEhQqVSiZ49e4q0tDSzfeTm5oqhQ4cKPz8/oVarxahRo8S1a9fMtjl69Kjo1q2bUKlUom7dumLevHmlyrJ27Vpx1113CS8vL9GiRQvxzTffmK23pyxyVdZxLigoEPfff7+oXbu28PT0FA0bNhRjx44tFSDzONtm6RgDMLuO5fRbYU9Z5MjWcT537pyIjY0VgYGBQqVSiSZNmojnnnvObJwZIZR1nN3+/4MTERERKRLbzBAREZGiMZghIiIiRWMwQ0RERIrGYIaIiIgUjcEMERERKRqDGSIiIlI0BjNERESkaAxmiIiISNEYzBCRbKSkpMDDwwN9+vSRuihEpCAcAZiIZOM///kP/Pz8sHTpUqSlpSE8PNzidkIIlJSUoEaNGi4uIRHJETMzRCQL+fn5+PLLL/H000+jT58+WLZsmWndrl274Obmhu+++w7t2rWDSqXCvn37YDAYkJSUhIiICPj4+ODuu+/GV199ZXpdSUkJxowZY1rftGlTvP/++xJ8OiJyJj7WEJEsrF27FtHR0WjatCkee+wxTJkyBTNnzoSbm5tpmxkzZuDtt99G48aNUatWLSQlJWHFihX46KOPEBUVhT179uCxxx5D7dq1ERcXB4PBgHr16mHdunUICgrCzz//jHHjxiEsLAyDBg2S8NMSkSOxmomIZKFr164YNGgQnnnmGdy8eRNhYWFYt24devTogV27duHee+/Fpk2b0K9fPwBAUVERAgMD8dNPPyEmJsa0n//85z8oKCjAqlWrLL7PxIkTkZWVZZbBISJlY2aGiCSXlpaGX375BRs3bgQA1KhRA4MHD8bSpUvRo0cP03bt27c3/f+pU6dQUFCAXr16me1Lr9ejbdu2pn9/8MEH+Oyzz3Du3DncuHEDer0ebdq0cernISLXYjBDRJJbunQpbt68adbgVwgBlUqF//73v6ZlNWvWNP1/fn4+AOCbb75B3bp1zfanUqkAAGvWrMH06dPxzjvvICYmBv7+/njrrbdw8OBBZ34cInIxBjNEJKmbN2/i888/xzvvvIP777/fbF3//v2xevVqREdHl3pd8+bNoVKpcO7cOcTFxVnc9/79+9GlSxeMHz/etOz06dOO/QBEJDkGM0Qkqa1bt+Lq1asYM2YMNBqN2brExEQsXboUb731VqnX+fv7Y/r06Zg6dSoMBgO6desGrVaL/fv3Q61WY+TIkYiKisLnn3+O77//HhEREfjiiy9w6NAhREREuOrjEZELsGs2EUlq6dKliI+PLxXIALeCmcOHD+PYsWMWX/vaa69h1qxZSEpKQrNmzfDAAw/gm2++MQUrTz75JAYMGIDBgwejU6dOyM3NNcvSEFHVwN5MREREpGjMzBAREZGiMZghIiIiRWMwQ0RERIrGYIaIiIgUjcEMERERKRqDGSIiIlI0BjNERESkaAxmiIiISNEYzBAREZGiMZghIiIiRWMwQ0RERIrGYIaIiIgU7f8AqRMprLE0v48AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "train_object_data = [ (key, item[0], item[1]) for key, item in object_area_ap.items()]\n",
    "\n",
    "area_ap_df = pd.DataFrame(pd.DataFrame(list(train_object_data),\n",
    "               columns =['Object', 'AP', 'Area']))\n",
    "\n",
    "ax = sns.scatterplot(data=area_ap_df, x=\"Area\", y=\"AP\")\n",
    "ax.set(title='b) Average Percision by Object\\'s area (Train Set)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b950a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt_object_data = [ (key, item) for key, item in eval_results['object_counts'].items()]\n",
    "\n",
    "cnt_ap_df = pd.DataFrame(pd.DataFrame(list(cnt_object_data),\n",
    "               columns =['Object', 'Count']))\n",
    "cnt_ap_df = cnt_ap_df.sort_values(by=\"Count\", ascending=False)\n",
    "ax= sns.catplot(x=\"Object\", y=\"Count\", kind=\"bar\", palette=\"ch:.25\",data=cnt_ap_df)\n",
    "ax.set(xticklabels=[])\n",
    "ax.set(xticks=[])\n",
    "\n",
    "ax.set(title='b) Number of objects by class (Train Set)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e803bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst Performing Objects: ['road' 'computer' 'coat' 'face' 'airplane' 'street' 'mountain' 'shoe'\n",
      " 'post' 'camera' 'sand' 'plane' 'trees' 'jeans' 'watch' 'cone'\n",
      " 'sunglasses' 'hand' 'luggage' 'ramp']\n"
     ]
    }
   ],
   "source": [
    "rank_ap = np.argsort([item[1] for item in train_object_data])\n",
    "ranked_object = np.array([item[0] for item in train_object_data])[rank_ap]\n",
    "print(\"Worst Performing Objects:\", ranked_object[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6500c",
   "metadata": {},
   "source": [
    "### [Optional] Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bf6b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import (\n",
    "    DatasetCatalog, DatasetMapper,\n",
    "    build_detection_train_loader,\n",
    "    build_detection_test_loader,    \n",
    ")\n",
    "from config import get_vrd_cfg\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "cfg = get_vrd_cfg()\n",
    "\n",
    "# DEPRECARTED: Old Dataloader Code\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .6\n",
    "# cfg.DATASETS.TRAIN = (\"vrd_val\", )\n",
    "# test_dataloader = build_detection_train_loader(cfg,\n",
    "#     mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "#         T.Resize((800, 800))\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bfbba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = RelTransR(cfg)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "#Run only once\n",
    "model.eval()\n",
    "\n",
    "#Load Model\n",
    "chkpoint_path = '../checkpoint/'\n",
    "model_name = 'vrd2_model_transr_23000.pt'\n",
    "chkpoint_full_path = os.path.join(chkpoint_path, model_name)\n",
    "it, start_epoch, losses = load_checkpoint(model, chkpoint_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a22bb41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get triples that are in the training set\n",
    "import os, json\n",
    "\n",
    "trained_triples_path = '../generated/trained_triples.json'\n",
    "trained_triples = {}\n",
    "if (os.path.exists(trained_triples_path)):\n",
    "    with open(trained_triples_path, 'r') as file:\n",
    "        trained_triples = json.load(file)\n",
    "else:\n",
    "    iter_dataloader = iter(train_dataloader)\n",
    "    n_iters = len(train_dataloader.dataset.dataset)\n",
    "    for i in range(n_iters):\n",
    "        print(i)\n",
    "        data = next(iter_dataloader)[0]\n",
    "        relationships = data['relationships']\n",
    "        for j in range(len(relationships['subj_classes'])):\n",
    "            subj_cls = model.object_classes[relationships['subj_classes'][j]]\n",
    "            pred_cls = model.predicate_classes[relationships['pred_classes'][j]]\n",
    "            obj_cls = model.object_classes[relationships['obj_classes'][j]]\n",
    "            trained_triples['{}-{}-{}'.format(subj_cls, pred_cls, obj_cls)] = 1\n",
    "\n",
    "    with open(trained_triples_path, 'w') as file:\n",
    "        file.write(json.dumps(trained_triples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d7489",
   "metadata": {},
   "source": [
    "#### EVAL 1/ RELATIONSHIP PREDICTION TASK (NO GROUND TRUTH LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea9eded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "\n",
    "eval_config = [\n",
    "    'triple_dist', #multiply subj_dist, pred_dist, obj_dist\n",
    "    'pred_dist', #only pred_dist\n",
    "    'pred_subtract_dist', # multiply pred_dist with subtract_dist\n",
    "    'pred_transe_dist', # multiply pred_dist with transe_dist\n",
    "    'triple_subtract_dist', #multiply triple_dist with subtract_dist\n",
    "    'triple_transe_dist', #multiply triple_dist with transe_dist\n",
    "]\n",
    "\n",
    "def get_top_nre_relationships(data, nre=50, config='triple_dist'):\n",
    "    \"\"\"\n",
    "        Get the top 50 or 100 relationships for a given input data (image)\n",
    "        Input:\n",
    "            data (Dictionary): input dictionary to the model\n",
    "            nre (Integer): select top n scored relationships\n",
    "            use_detector_score (Boolean): use the detectron2 detector score\n",
    "            config (String Selection): see eval_config above\n",
    "        Return:\n",
    "            top_n_relationships (Dictionary): top nre relationships based on the `config` scoring metric\n",
    "    \"\"\"\n",
    "    if (config not in eval_config):\n",
    "        print(\"Error: ensure that the distance metric is one of the following options: \", eval_config)\n",
    "    \n",
    "    relationships = data['relationships']\n",
    "    predicate_distances, subject_distances, object_distances, predicate_subtract_distances, transe_distances = model.get_predicate_distances(data, is_rel_eval=True)\n",
    "    all_possible_relationships = collections.defaultdict(list) # {scores: [], subj_classes:[], pred_classes:[], obj_classes:[], subj_bboxes:[], obj_bboxes:[]}\n",
    "    top_n_relationships = collections.defaultdict(list) # \n",
    "\n",
    "    # Iterate through the possible relationships\n",
    "    for j, (pred_dists, subj_dists, obj_dists) in enumerate(zip(predicate_distances, subject_distances, object_distances)):\n",
    "\n",
    "        #labels for subject and object\n",
    "        subj_class = relationships[\"subj_classes\"][j]\n",
    "        obj_class = relationships[\"obj_classes\"][j]\n",
    "        subj_bbox = relationships[\"subj_bboxes\"][j]\n",
    "        obj_bbox = relationships[\"obj_bboxes\"][j]\n",
    "        subj_detection_score = relationships[\"subj_scores\"][j]\n",
    "        obj_detection_score = relationships[\"obj_scores\"][j]\n",
    "        \n",
    "        #multiply pred, subj, obj\n",
    "        #Compute scores for the 71 possible predicate and compute the score\n",
    "        scoring_distances = []\n",
    "        if (config == 'triple_dist'):\n",
    "            scoring_distances = [item_pred.cpu()*item_subj.cpu()*item_obj.cpu() for item_pred, item_subj, item_obj in zip(pred_dists, subj_dists, obj_dists)]\n",
    "        elif (config == 'pred_dist'):\n",
    "            scoring_distances = [item_pred.cpu() for item_pred in pred_dists]\n",
    "        elif (config == 'pred_subtract_dist'):\n",
    "            scoring_distances = [item_pred.cpu()*item_subtract.cpu() for item_pred, item_subtract in zip(pred_dists, predicate_subtract_distances[j])]\n",
    "        elif (config == 'pred_transe_dist'):\n",
    "            scoring_distances = [item_pred.cpu()*item_transe.cpu() for item_pred, item_transe in zip(pred_dists, transe_distances[j])]\n",
    "        elif (config == 'triple_subtract_dist'):\n",
    "            scoring_distances = [item_pred.cpu()*item_subj.cpu()*item_obj.cpu()*item_subtract.cpu() for item_pred, item_subj, item_obj, item_subtract in zip(pred_dists, subj_dists, obj_dists, predicate_subtract_distances[j])]\n",
    "        elif (config == 'triple_transe_dist'):\n",
    "            scoring_distances = [item_pred.cpu()*item_subj.cpu()*item_obj.cpu()*item_transe.cpu() for item_pred, item_subj, item_obj, item_transe in zip(pred_dists, subj_dists, obj_dists, transe_distances[j])]\n",
    "            \n",
    "        #Adding to all_possible_relationships to rank\n",
    "        for ind, distance_score in enumerate(scoring_distances):\n",
    "            all_possible_relationships[\"subj_classes\"].append(subj_class.cpu())\n",
    "            all_possible_relationships[\"pred_classes\"].append(ind)\n",
    "            all_possible_relationships[\"obj_classes\"].append(obj_class.cpu())\n",
    "            all_possible_relationships[\"subj_bboxes\"].append(subj_bbox)\n",
    "            all_possible_relationships[\"obj_bboxes\"].append(obj_bbox)\n",
    "            all_possible_relationships[\"distance_scores\"].append(distance_score)\n",
    "            all_possible_relationships[\"subj_detection_scores\"].append(subj_detection_score.detach().cpu().numpy())\n",
    "            all_possible_relationships[\"obj_detection_scores\"].append(obj_detection_score.detach().cpu().numpy())\n",
    "\n",
    "    #rank to get the top 50, 100 relationships\n",
    "    sorted_by_distance = np.argsort(all_possible_relationships[\"distance_scores\"])\n",
    "    \n",
    "    for key, val in all_possible_relationships.items():\n",
    "        sorted_val = np.array(val)[sorted_by_distance]\n",
    "        \n",
    "        top_n_relationships[key] = sorted_val\n",
    "        if (nre is not None and len(val) > nre):\n",
    "            top_n_relationships[key] = sorted_val[:nre]\n",
    "            \n",
    "    return top_n_relationships\n",
    "\n",
    "def eval_per_image(gt_relationships, pred_relationships, gt_thr=0.5, trained_triples=None, model=None):\n",
    "    \"\"\"\n",
    "        Iterate through the ground truth relationship of the image and check\n",
    "            whether they are in the top 50 or 100, while ensuring that the IoU threshhold is above 50%\n",
    "            \n",
    "        Input:\n",
    "            gt_relationships: ground truth relationships\n",
    "            pred_relationships: top @n predicted relationships with bounding boxes, etc.\n",
    "            gr_thr: IoU threshold\n",
    "            trained_triples: if specified, perform zeroshot evaluation\n",
    "        Return:\n",
    "            tp: number of true positives\n",
    "            n_gt_labels: number of ground truth labels to compute recall\n",
    "    \"\"\"\n",
    "    n_pred_labels = len(pred_relationships[\"subj_bboxes\"])\n",
    "    n_gt_labels = len(gt_relationships[\"subj_bboxes\"])\n",
    "    n_zeroshot_gt_labels_set = set() # only used if trained_triples is specified\n",
    "    visited = set() #track if ground truth has be visited\n",
    "    \n",
    "    tp = np.zeros((n_pred_labels,1))\n",
    "    fp = np.zeros((n_pred_labels,1))\n",
    "    for j in range(n_pred_labels):\n",
    "        pred_triple_cls = np.array((pred_relationships[\"subj_classes\"][j], pred_relationships[\"pred_classes\"][j], pred_relationships[\"obj_classes\"][j]))\n",
    "        obj_bboxes = pred_relationships[\"obj_bboxes\"][j][0]\n",
    "        subj_bboxes = pred_relationships[\"subj_bboxes\"][j][0]\n",
    "        \n",
    "        max_iou = gt_thr #track best overlap\n",
    "        kmax = -1 #track visited\n",
    "\n",
    "        for k in range(n_gt_labels):\n",
    "            gt_triple_cls = np.array((gt_relationships[\"subj_classes\"][k], gt_relationships[\"pred_classes\"][k], gt_relationships[\"obj_classes\"][k]))\n",
    "            \n",
    "            if (trained_triples is not None):\n",
    "                gt_subj_cls = model.object_classes[gt_relationships['subj_classes'][k]]\n",
    "                gt_pred_cls = model.predicate_classes[gt_relationships['pred_classes'][k]]\n",
    "                gt_obj_cls = model.object_classes[gt_relationships['obj_classes'][k]]\n",
    "                gt_triple_cls_label = '-'.join((gt_subj_cls, gt_pred_cls, gt_obj_cls))\n",
    "\n",
    "                # if the label is in the training dataset, ignore\n",
    "                if (gt_triple_cls_label in trained_triples):\n",
    "                    continue\n",
    "                    \n",
    "                n_zeroshot_gt_labels_set.add(k)\n",
    "                \n",
    "            # Verify prediction labels match ground truth labels\n",
    "            if (np.linalg.norm(gt_triple_cls - pred_triple_cls) != 0):\n",
    "                continue\n",
    "            # Verify that the ground truth labels has not been visited before\n",
    "            if (k in visited):\n",
    "                continue\n",
    "            \n",
    "            # Check IoU to make sure that the predicted bbox > threshold of 50%\n",
    "            gt_subj_bboxes = gt_relationships[\"subj_bboxes\"][k][0]\n",
    "            gt_obj_bboxes = gt_relationships[\"obj_bboxes\"][k][0]\n",
    "            \n",
    "            # Intersection between predicted bbox and gt_bbox\n",
    "            subj_intersection_bbox = np.array([max(subj_bboxes[0],gt_subj_bboxes[0]),\n",
    "                                                max(subj_bboxes[1],gt_subj_bboxes[1]),\n",
    "                                                min(subj_bboxes[2],gt_subj_bboxes[2]),\n",
    "                                                min(subj_bboxes[3],gt_subj_bboxes[3])])\n",
    "            \n",
    "            obj_intersection_bbox = np.array([max(obj_bboxes[0],gt_obj_bboxes[0]),\n",
    "                                                max(obj_bboxes[1],gt_obj_bboxes[1]),\n",
    "                                                min(obj_bboxes[2],gt_obj_bboxes[2]),\n",
    "                                                min(obj_bboxes[3],gt_obj_bboxes[3])])\n",
    "            \n",
    "            subj_intersection_bbox_width = subj_intersection_bbox[2] - subj_intersection_bbox[0] + 1;\n",
    "            subj_intersection_bbox_height = subj_intersection_bbox[3] - subj_intersection_bbox[1] + 1;\n",
    "\n",
    "            obj_intersection_bbox_width = obj_intersection_bbox[2] - obj_intersection_bbox[0] + 1;\n",
    "            obj_intersection_bbox_height = obj_intersection_bbox[3] - obj_intersection_bbox[1] + 1;\n",
    "                        \n",
    "            # Check overlapping\n",
    "            if (subj_intersection_bbox_width > 0 and subj_intersection_bbox_height > 0 and\n",
    "                obj_intersection_bbox_width > 0 and obj_intersection_bbox_height > 0):\n",
    "                \n",
    "                # [subject] compute overlap as area of intersection / area of union\n",
    "                subj_intersection_area = subj_intersection_bbox_width * subj_intersection_bbox_height\n",
    "                subj_union_area = (subj_bboxes[2] - subj_bboxes[0] + 1) * (subj_bboxes[3] - subj_bboxes[1] + 1) + \\\n",
    "                                (gt_subj_bboxes[2] - gt_subj_bboxes[0] + 1) * (gt_subj_bboxes[3] - gt_subj_bboxes[1] + 1) - subj_intersection_area\n",
    "                subj_iou = subj_intersection_area / subj_union_area\n",
    "\n",
    "                # [object] compute overlap as area of intersection / area of union\n",
    "                obj_intersection_area = obj_intersection_bbox_width * obj_intersection_bbox_height\n",
    "                obj_union_area = (obj_bboxes[2] - obj_bboxes[0] + 1) * (obj_bboxes[3] - obj_bboxes[1] + 1) + \\\n",
    "                                (gt_obj_bboxes[2] - gt_obj_bboxes[0] + 1) * (gt_obj_bboxes[3] - gt_obj_bboxes[1] + 1) - obj_intersection_area\n",
    "                obj_iou = obj_intersection_area / obj_union_area\n",
    "\n",
    "                 \n",
    "                # only need to evaluate the minimum overlap ratio to test against the threshold\n",
    "                min_iou = min(subj_iou, obj_iou)\n",
    "\n",
    "                # makes sure that this object is detected according\n",
    "                # to its individual threshold\n",
    "                if (min_iou >= max_iou):\n",
    "                    max_iou = min_iou;\n",
    "                    kmax = k;\n",
    "            \n",
    "        if (kmax > -1):\n",
    "            visited.add(kmax)\n",
    "            tp[j] = 1\n",
    "        else:\n",
    "            fp[j] = 1;\n",
    "\n",
    "    if (trained_triples is not None):\n",
    "        return tp.sum(), len(n_zeroshot_gt_labels_set)\n",
    "            \n",
    "    return tp.sum(), n_gt_labels\n",
    "\n",
    "\n",
    "def eval_dataset(dataloader, model, nre=50, config='triple_dist', trained_triples=None):\n",
    "    total_true_positive = 0\n",
    "    total_relationships = 0\n",
    "\n",
    "    cumulative_recall = []\n",
    "    recall = 0\n",
    "    \n",
    "    n_examples = len(dataloader.dataset)\n",
    "    test_data_iter = iter(dataloader)\n",
    "    for i in range(n_examples):\n",
    "        data = next(test_data_iter)[0]\n",
    "        gt_relationships = data[\"relationships\"].copy()\n",
    "        relationships = None\n",
    "        top_predicted_relationships = {}\n",
    "\n",
    "        if (len(gt_relationships['subj_bboxes']) == 0):\n",
    "            #no relationship annotations for the given image\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #get predicted relationships from the object detector\n",
    "            relationships = model.get_predicted_relationships(data)\n",
    "            data['relationships'] = relationships\n",
    "\n",
    "        if (len(relationships['subj_bboxes']) == 0):\n",
    "            #no relationship annotations for the given image\n",
    "            continue\n",
    "\n",
    "        if (i % 20 == 0 and i > 0):\n",
    "            recall =  total_true_positive / total_relationships * 100\n",
    "            cumulative_recall.append(recall)\n",
    "            print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "            print(\"| TOP {} |               Recall {:5.2f}               |\".format(nre, recall))\n",
    "\n",
    "        # Get the top \n",
    "        top_predicted_relationships = get_top_nre_relationships(data, nre, config=config)\n",
    "        image_true_positive, image_relationships = eval_per_image(gt_relationships, top_predicted_relationships, 0.5, trained_triples, model)\n",
    "        \n",
    "        total_true_positive += image_true_positive\n",
    "        total_relationships += image_relationships\n",
    "\n",
    "    recall =  total_true_positive / total_relationships * 100\n",
    "    print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "    print(\"| TOP {} |               Recall {:5.2f}               |\".format(nre, recall))\n",
    "    cumulative_recall.append(recall)\n",
    "    \n",
    "    return cumulative_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedb0b4",
   "metadata": {},
   "source": [
    "##### Test Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02345c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset\n",
    "cfg.DATASETS.TEST = (\"vrd_val\", )\n",
    "\n",
    "test_dataset = DatasetCatalog.get(\"vrd_val\")\n",
    "test_dataloader = build_detection_test_loader(dataset=test_dataset,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2768e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('is_zeroshot', 50, 'pred_dist')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_189080/134011521.py:74: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sorted_val = np.array(val)[sorted_by_distance]\n",
      "/tmp/ipykernel_189080/134011521.py:74: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sorted_val = np.array(val)[sorted_by_distance]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  3.85               |\n",
      "|----------------------Iter 80------------------------|\n",
      "| TOP 50 |               Recall  2.44               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  2.06               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  2.05               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  1.82               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  1.68               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall  2.03               |\n",
      "|----------------------Iter 240------------------------|\n",
      "| TOP 50 |               Recall  1.91               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  1.65               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  1.48               |\n",
      "|----------------------Iter 340------------------------|\n",
      "| TOP 50 |               Recall  1.32               |\n",
      "|----------------------Iter 380------------------------|\n",
      "| TOP 50 |               Recall  1.23               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  1.16               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  1.10               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  1.02               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  1.68               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  1.57               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  1.52               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  1.47               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  1.43               |\n",
      "|----------------------Iter 560------------------------|\n",
      "| TOP 50 |               Recall  1.38               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  1.48               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  1.47               |\n",
      "|----------------------Iter 640------------------------|\n",
      "| TOP 50 |               Recall  1.44               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  1.41               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall  1.34               |\n",
      "|----------------------Iter 720------------------------|\n",
      "| TOP 50 |               Recall  1.32               |\n",
      "|----------------------Iter 740------------------------|\n",
      "| TOP 50 |               Recall  1.29               |\n",
      "|----------------------Iter 760------------------------|\n",
      "| TOP 50 |               Recall  1.25               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  1.20               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  1.16               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  1.15               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  1.11               |\n",
      "|----------------------Iter 920------------------------|\n",
      "| TOP 50 |               Recall  1.18               |\n",
      "|----------------------Iter 940------------------------|\n",
      "| TOP 50 |               Recall  1.16               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  1.14               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  1.13               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  1.24               |\n",
      "('is_zeroshot', 50, 'pred_subtract_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  2.00               |\n",
      "|----------------------Iter 80------------------------|\n",
      "| TOP 50 |               Recall  1.67               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  0.96               |\n",
      "|----------------------Iter 160------------------------|\n",
      "| TOP 50 |               Recall  0.89               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  0.79               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  0.69               |\n",
      "|----------------------Iter 240------------------------|\n",
      "| TOP 50 |               Recall  1.22               |\n",
      "|----------------------Iter 260------------------------|\n",
      "| TOP 50 |               Recall  1.16               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  1.12               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  1.05               |\n",
      "|----------------------Iter 320------------------------|\n",
      "| TOP 50 |               Recall  1.85               |\n",
      "|----------------------Iter 340------------------------|\n",
      "| TOP 50 |               Recall  1.65               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  1.51               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  1.39               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  1.33               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  1.24               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  1.44               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  1.37               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  1.55               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  1.48               |\n",
      "|----------------------Iter 560------------------------|\n",
      "| TOP 50 |               Recall  1.36               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall  1.30               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  1.25               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  1.58               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall  1.65               |\n",
      "|----------------------Iter 720------------------------|\n",
      "| TOP 50 |               Recall  1.60               |\n",
      "|----------------------Iter 740------------------------|\n",
      "| TOP 50 |               Recall  1.57               |\n",
      "|----------------------Iter 760------------------------|\n",
      "| TOP 50 |               Recall  1.49               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  1.59               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  1.49               |\n",
      "|----------------------Iter 840------------------------|\n",
      "| TOP 50 |               Recall  1.46               |\n",
      "|----------------------Iter 880------------------------|\n",
      "| TOP 50 |               Recall  1.53               |\n",
      "|----------------------Iter 900------------------------|\n",
      "| TOP 50 |               Recall  1.50               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  1.55               |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  1.64               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  1.61               |\n",
      "('is_zeroshot', 50, 'pred_transe_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 80------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 120------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 160------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall  0.51               |\n",
      "|----------------------Iter 240------------------------|\n",
      "| TOP 50 |               Recall  0.92               |\n",
      "|----------------------Iter 260------------------------|\n",
      "| TOP 50 |               Recall  0.81               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  1.09               |\n",
      "|----------------------Iter 320------------------------|\n",
      "| TOP 50 |               Recall  1.05               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  0.98               |\n",
      "|----------------------Iter 380------------------------|\n",
      "| TOP 50 |               Recall  0.94               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  1.18               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  1.38               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  1.33               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  1.18               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  1.11               |\n",
      "|----------------------Iter 560------------------------|\n",
      "| TOP 50 |               Recall  1.06               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  1.00               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  0.98               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  1.13               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall  1.10               |\n",
      "|----------------------Iter 720------------------------|\n",
      "| TOP 50 |               Recall  1.03               |\n",
      "|----------------------Iter 760------------------------|\n",
      "| TOP 50 |               Recall  0.97               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  0.94               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  0.92               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  0.87               |\n",
      "|----------------------Iter 880------------------------|\n",
      "| TOP 50 |               Recall  0.86               |\n",
      "|----------------------Iter 900------------------------|\n",
      "| TOP 50 |               Recall  0.84               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  1.07               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  1.02               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  1.12               |\n",
      "('is_zeroshot', 50, 'triple_subtract_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  1.15               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  1.89               |\n",
      "|----------------------Iter 160------------------------|\n",
      "| TOP 50 |               Recall  1.60               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  1.52               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  1.39               |\n",
      "|----------------------Iter 240------------------------|\n",
      "| TOP 50 |               Recall  1.20               |\n",
      "|----------------------Iter 260------------------------|\n",
      "| TOP 50 |               Recall  1.69               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  1.65               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  1.55               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  1.62               |\n",
      "|----------------------Iter 380------------------------|\n",
      "| TOP 50 |               Recall  1.55               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  1.42               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  1.27               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  1.91               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  1.86               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  2.06               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  1.96               |\n",
      "|----------------------Iter 560------------------------|\n",
      "| TOP 50 |               Recall  1.88               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall  1.83               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  1.75               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  1.69               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  1.59               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall  1.52               |\n",
      "|----------------------Iter 760------------------------|\n",
      "| TOP 50 |               Recall  1.69               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  1.65               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  1.69               |\n",
      "|----------------------Iter 840------------------------|\n",
      "| TOP 50 |               Recall  1.77               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  1.73               |\n",
      "|----------------------Iter 880------------------------|\n",
      "| TOP 50 |               Recall  1.85               |\n",
      "|----------------------Iter 900------------------------|\n",
      "| TOP 50 |               Recall  1.91               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  1.94               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  1.90               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  1.86               |\n",
      "('is_zeroshot', 50, 'triple_transe_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall 12.50               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  5.26               |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------Iter 80------------------------|\n",
      "| TOP 50 |               Recall  3.45               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  2.67               |\n",
      "|----------------------Iter 120------------------------|\n",
      "| TOP 50 |               Recall  2.08               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  1.90               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall  1.70               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  1.73               |\n",
      "|----------------------Iter 340------------------------|\n",
      "| TOP 50 |               Recall  1.53               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  1.35               |\n",
      "|----------------------Iter 380------------------------|\n",
      "| TOP 50 |               Recall  1.30               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  1.26               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  1.52               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  1.39               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  1.33               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  1.28               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  1.25               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  1.43               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  1.58               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall  1.68               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  1.63               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  1.59               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  1.52               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall  1.66               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall  1.78               |\n",
      "|----------------------Iter 720------------------------|\n",
      "| TOP 50 |               Recall  1.70               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  1.58               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  1.49               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  1.60               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  1.71               |\n",
      "|----------------------Iter 880------------------------|\n",
      "| TOP 50 |               Recall  1.81               |\n",
      "|----------------------Iter 900------------------------|\n",
      "| TOP 50 |               Recall  1.79               |\n",
      "|----------------------Iter 920------------------------|\n",
      "| TOP 50 |               Recall  1.87               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  1.79               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  1.74               |\n",
      "('is_zeroshot', 100, 'pred_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 120------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 260------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  0.43               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  0.79               |\n",
      "|----------------------Iter 320------------------------|\n",
      "| TOP 50 |               Recall  0.75               |\n",
      "|----------------------Iter 340------------------------|\n",
      "| TOP 50 |               Recall  0.71               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  0.68               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  0.89               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  0.80               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  0.75               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  0.71               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  0.69               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  1.10               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall  1.04               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  1.02               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  0.99               |\n",
      "|----------------------Iter 640------------------------|\n",
      "| TOP 50 |               Recall  0.97               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall  0.92               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall  0.89               |\n",
      "|----------------------Iter 720------------------------|\n",
      "| TOP 50 |               Recall  0.88               |\n",
      "|----------------------Iter 760------------------------|\n",
      "| TOP 50 |               Recall  0.82               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  0.81               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  0.79               |\n",
      "|----------------------Iter 840------------------------|\n",
      "| TOP 50 |               Recall  0.73               |\n",
      "|----------------------Iter 920------------------------|\n",
      "| TOP 50 |               Recall  0.94               |\n",
      "|----------------------Iter 940------------------------|\n",
      "| TOP 50 |               Recall  0.92               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  1.27               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  1.26               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  1.24               |\n",
      "('is_zeroshot', 100, 'pred_subtract_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  2.00               |\n",
      "|----------------------Iter 80------------------------|\n",
      "| TOP 50 |               Recall  3.17               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  2.44               |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------Iter 120------------------------|\n",
      "| TOP 50 |               Recall  2.27               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  1.80               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  1.37               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall  1.15               |\n",
      "|----------------------Iter 240------------------------|\n",
      "| TOP 50 |               Recall  1.04               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  1.19               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  1.12               |\n",
      "|----------------------Iter 340------------------------|\n",
      "| TOP 50 |               Recall  1.01               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  1.61               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  1.44               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  1.38               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  1.32               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  1.25               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  1.20               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  1.15               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  1.13               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall  1.04               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  0.99               |\n",
      "|----------------------Iter 640------------------------|\n",
      "| TOP 50 |               Recall  1.15               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  1.13               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall  1.05               |\n",
      "|----------------------Iter 720------------------------|\n",
      "| TOP 50 |               Recall  1.01               |\n",
      "|----------------------Iter 740------------------------|\n",
      "| TOP 50 |               Recall  1.31               |\n",
      "|----------------------Iter 760------------------------|\n",
      "| TOP 50 |               Recall  1.45               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  1.41               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  1.39               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  1.52               |\n",
      "|----------------------Iter 840------------------------|\n",
      "| TOP 50 |               Recall  1.49               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  1.44               |\n",
      "|----------------------Iter 880------------------------|\n",
      "| TOP 50 |               Recall  1.40               |\n",
      "|----------------------Iter 940------------------------|\n",
      "| TOP 50 |               Recall  1.46               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  1.55               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  1.65               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  1.61               |\n",
      "('is_zeroshot', 100, 'pred_transe_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  4.17               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  4.76               |\n",
      "|----------------------Iter 80------------------------|\n",
      "| TOP 50 |               Recall  4.05               |\n",
      "|----------------------Iter 120------------------------|\n",
      "| TOP 50 |               Recall  2.97               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  2.70               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  2.46               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  2.00               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall  1.79               |\n",
      "|----------------------Iter 240------------------------|\n",
      "| TOP 50 |               Recall  1.55               |\n",
      "|----------------------Iter 260------------------------|\n",
      "| TOP 50 |               Recall  1.46               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  1.41               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  1.32               |\n",
      "|----------------------Iter 320------------------------|\n",
      "| TOP 50 |               Recall  1.26               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  1.09               |\n",
      "|----------------------Iter 380------------------------|\n",
      "| TOP 50 |               Recall  1.03               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  0.91               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  0.87               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  0.84               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  1.30               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  1.51               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  1.66               |\n",
      "|----------------------Iter 560------------------------|\n",
      "| TOP 50 |               Recall  1.61               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall  1.53               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  1.48               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  1.45               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  1.32               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall  1.29               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall  1.26               |\n",
      "|----------------------Iter 720------------------------|\n",
      "| TOP 50 |               Recall  1.23               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  1.12               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  1.10               |\n",
      "|----------------------Iter 840------------------------|\n",
      "| TOP 50 |               Recall  1.05               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  1.02               |\n",
      "|----------------------Iter 900------------------------|\n",
      "| TOP 50 |               Recall  0.98               |\n",
      "|----------------------Iter 920------------------------|\n",
      "| TOP 50 |               Recall  0.96               |\n",
      "|----------------------Iter 940------------------------|\n",
      "| TOP 50 |               Recall  1.06               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  1.04               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  1.02               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  1.12               |\n",
      "('is_zeroshot', 100, 'triple_subtract_dist')\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  4.55               |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  3.33               |\n",
      "|----------------------Iter 80------------------------|\n",
      "| TOP 50 |               Recall  2.56               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  3.23               |\n",
      "|----------------------Iter 120------------------------|\n",
      "| TOP 50 |               Recall  2.38               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  1.61               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  1.39               |\n",
      "|----------------------Iter 240------------------------|\n",
      "| TOP 50 |               Recall  1.14               |\n",
      "|----------------------Iter 260------------------------|\n",
      "| TOP 50 |               Recall  1.50               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  1.33               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  1.20               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  1.39               |\n",
      "|----------------------Iter 380------------------------|\n",
      "| TOP 50 |               Recall  1.67               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  1.53               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  1.47               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  1.40               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  1.35               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  1.22               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  1.43               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  1.62               |\n",
      "|----------------------Iter 560------------------------|\n",
      "| TOP 50 |               Recall  1.56               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall  1.54               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  1.58               |\n",
      "|----------------------Iter 640------------------------|\n",
      "| TOP 50 |               Recall  1.54               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall  1.65               |\n",
      "|----------------------Iter 740------------------------|\n",
      "| TOP 50 |               Recall  2.18               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  2.25               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  2.20               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  2.16               |\n",
      "|----------------------Iter 840------------------------|\n",
      "| TOP 50 |               Recall  2.12               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  2.04               |\n",
      "|----------------------Iter 880------------------------|\n",
      "| TOP 50 |               Recall  2.00               |\n",
      "|----------------------Iter 900------------------------|\n",
      "| TOP 50 |               Recall  1.93               |\n",
      "|----------------------Iter 920------------------------|\n",
      "| TOP 50 |               Recall  1.89               |\n",
      "|----------------------Iter 940------------------------|\n",
      "| TOP 50 |               Recall  1.86               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  1.86               |\n",
      "('is_zeroshot', 100, 'triple_transe_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  0.00               |\n",
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  6.00               |\n",
      "|----------------------Iter 80------------------------|\n",
      "| TOP 50 |               Recall  4.29               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  3.49               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  2.68               |\n",
      "|----------------------Iter 160------------------------|\n",
      "| TOP 50 |               Recall  2.26               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  2.80               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  2.30               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall  2.11               |\n",
      "|----------------------Iter 240------------------------|\n",
      "| TOP 50 |               Recall  1.97               |\n",
      "|----------------------Iter 260------------------------|\n",
      "| TOP 50 |               Recall  1.70               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  1.63               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  1.54               |\n",
      "|----------------------Iter 320------------------------|\n",
      "| TOP 50 |               Recall  1.89               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  1.79               |\n",
      "|----------------------Iter 380------------------------|\n",
      "| TOP 50 |               Recall  1.71               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  1.64               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  1.53               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  2.22               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  2.19               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  2.05               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  1.90               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  1.87               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  2.01               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  1.95               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  1.84               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall  2.15               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall  2.09               |\n",
      "|----------------------Iter 720------------------------|\n",
      "| TOP 50 |               Recall  2.05               |\n",
      "|----------------------Iter 740------------------------|\n",
      "| TOP 50 |               Recall  2.00               |\n",
      "|----------------------Iter 760------------------------|\n",
      "| TOP 50 |               Recall  1.95               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  1.91               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  2.02               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  2.01               |\n",
      "|----------------------Iter 840------------------------|\n",
      "| TOP 50 |               Recall  1.94               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  2.01               |\n",
      "|----------------------Iter 880------------------------|\n",
      "| TOP 50 |               Recall  1.98               |\n",
      "|----------------------Iter 900------------------------|\n",
      "| TOP 50 |               Recall  1.96               |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------Iter 940------------------------|\n",
      "| TOP 50 |               Recall  1.87               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  1.82               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  1.79               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  1.74               |\n"
     ]
    }
   ],
   "source": [
    "n_examples = len(test_dataloader.dataset)\n",
    "test_data_iter = iter(test_dataloader)\n",
    "recall_results = {}\n",
    "for is_zeroshot in [True, False]:\n",
    "    for nre in [50, 100]:\n",
    "        for conf in eval_config:\n",
    "            test_dataloader = build_detection_train_loader(cfg,\n",
    "                mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "                    T.Resize((800, 800))\n",
    "                ])\n",
    "            )\n",
    "            test_data_iter = iter(test_dataloader)\n",
    "            with torch.no_grad():\n",
    "                if (is_zeroshot):\n",
    "                    print(('is_zeroshot', nre, conf))\n",
    "                    recall_results[('is_zeroshot', nre, conf)] = eval_dataset(test_data_iter, model, nre=50, config=conf, trained_triples=trained_triples)\n",
    "                else:\n",
    "                    print(('not_zeroshot', nre, conf))\n",
    "                    recall_results[('not_zeroshot',nre, conf)] = eval_dataset(test_data_iter, model, nre=50, config=conf, trained_triples=None)\n",
    "\n",
    "            test_dataloader = None\n",
    "            del test_dataloader\n",
    "            del test_data_iter\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e573f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .6\n",
    "cfg.DATASETS.TRAIN = (\"vrd_val\", )\n",
    "del test_dataloader\n",
    "del test_data_iter\n",
    "test_dataloader = build_detection_train_loader(cfg,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")\n",
    "test_data_iter = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04b681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_results_path = '../generated/results_recall_vrd2_model_transr_23000.json'\n",
    "\n",
    "with open(recall_results_path, 'w') as file:\n",
    "    file.write(json.dumps(recall_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee57204",
   "metadata": {},
   "source": [
    "##### Train Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47f584d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataset\n",
    "cfg.DATASETS.TEST = (\"vrd_train\", )\n",
    "\n",
    "train_dataset = DatasetCatalog.get(\"vrd_train\")\n",
    "train_dataloader = build_detection_test_loader(dataset=train_dataset,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4c54343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('not_zeroshot', 50, 'triple_dist')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_255143/2466572815.py:74: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sorted_val = np.array(val)[sorted_by_distance]\n",
      "/tmp/ipykernel_255143/2466572815.py:74: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sorted_val = np.array(val)[sorted_by_distance]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  9.27               |\n",
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  9.69               |\n",
      "|----------------------Iter 80------------------------|\n",
      "| TOP 50 |               Recall 10.04               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall 11.45               |\n",
      "|----------------------Iter 120------------------------|\n",
      "| TOP 50 |               Recall 11.29               |\n",
      "|----------------------Iter 160------------------------|\n",
      "| TOP 50 |               Recall 12.69               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall 13.08               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall 13.24               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall 13.24               |\n",
      "|----------------------Iter 240------------------------|\n",
      "| TOP 50 |               Recall 13.16               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall 12.84               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall 12.87               |\n",
      "|----------------------Iter 320------------------------|\n",
      "| TOP 50 |               Recall 12.65               |\n",
      "|----------------------Iter 340------------------------|\n",
      "| TOP 50 |               Recall 12.46               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall 12.57               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall 12.19               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall 11.95               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall 12.08               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall 12.08               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall 11.97               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall 11.81               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall 11.71               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall 11.82               |\n",
      "|----------------------Iter 560------------------------|\n",
      "| TOP 50 |               Recall 11.80               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall 11.69               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall 11.58               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall 11.73               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall 11.73               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall 11.72               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall 11.69               |\n",
      "|----------------------Iter 740------------------------|\n",
      "| TOP 50 |               Recall 12.22               |\n",
      "|----------------------Iter 760------------------------|\n",
      "| TOP 50 |               Recall 12.14               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall 12.04               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall 11.96               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall 11.94               |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot_zeroshot\u001b[39m\u001b[38;5;124m'\u001b[39m, nre, conf))\n\u001b[0;32m---> 16\u001b[0m         recall_results[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot_zeroshot\u001b[39m\u001b[38;5;124m'\u001b[39m,nre, conf)] \u001b[38;5;241m=\u001b[39m \u001b[43meval_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnre\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrained_triples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m train_data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m train_dataloader\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36meval_dataset\u001b[0;34m(dataloader, model, nre, config, trained_triples)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| TOP \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m |               Recall \u001b[39m\u001b[38;5;132;01m{:5.2f}\u001b[39;00m\u001b[38;5;124m               |\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(nre, recall))\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Get the top \u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m top_predicted_relationships \u001b[38;5;241m=\u001b[39m \u001b[43mget_top_nre_relationships\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m image_true_positive, image_relationships \u001b[38;5;241m=\u001b[39m eval_per_image(gt_relationships, top_predicted_relationships, \u001b[38;5;241m0.5\u001b[39m, trained_triples, model)\n\u001b[1;32m    230\u001b[0m total_true_positive \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m image_true_positive\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36mget_top_nre_relationships\u001b[0;34m(data, nre, config)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: ensure that the distance metric is one of the following options: \u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_config)\n\u001b[1;32m     27\u001b[0m relationships \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelationships\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m predicate_distances, subject_distances, object_distances, predicate_subtract_distances, transe_distances \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_predicate_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_rel_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m all_possible_relationships \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m) \u001b[38;5;66;03m# {scores: [], subj_classes:[], pred_classes:[], obj_classes:[], subj_bboxes:[], obj_bboxes:[]}\u001b[39;00m\n\u001b[1;32m     30\u001b[0m top_n_relationships \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m) \u001b[38;5;66;03m# \u001b[39;00m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mRelTransR.get_predicate_distances\u001b[0;34m(self, data, is_rel_eval)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subj, obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(relationships[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubj_classes\u001b[39m\u001b[38;5;124m'\u001b[39m], relationships[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_classes\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;66;03m#compute all potential predicate embeddings for the (subj, obj) pair\u001b[39;00m\n\u001b[0;32m--> 434\u001b[0m         all_subj_embeddings, all_predicate_embeddings, all_object_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_word_predicate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;66;03m#languge features\u001b[39;00m\n\u001b[1;32m    437\u001b[0m         fc_all_subject_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_word[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubj\u001b[39m\u001b[38;5;124m\"\u001b[39m](all_subj_embeddings)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mRelTransR._get_word_predicate_features\u001b[0;34m(self, subj, obj)\u001b[0m\n\u001b[1;32m    296\u001b[0m query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (triples_text \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embeddings):\n\u001b[0;32m--> 298\u001b[0m     query_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_bert_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubj_cls_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_cls_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj_cls_label\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embeddings[triples_text] \u001b[38;5;241m=\u001b[39m query_embeddings\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mRelTransR._get_bert_features\u001b[0;34m(self, triples)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Put the model in \"evaluation\" mode, meaning feed-forward operation.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegments_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# Evaluating the model will return a different number of objects based on\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# how it's  configured in the `from_pretrained` call earlier. In this case,\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# becase we set `output_hidden_states = True`, the third item will be the\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# hidden states from all layers. See the documentation for more details:\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# https://huggingface.co/transformers/model_doc/bert.html#bertmodel\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/dev/research_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/research_env/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    990\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    991\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    995\u001b[0m )\n\u001b[0;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/research_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/research_env/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    578\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    583\u001b[0m     )\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/dev/research_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/research_env/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    462\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m ):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/research_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dev/research_env/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:411\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    394\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ):\n\u001b[1;32m    402\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    403\u001b[0m         hidden_states,\n\u001b[1;32m    404\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m         output_attentions,\n\u001b[1;32m    410\u001b[0m     )\n\u001b[0;32m--> 411\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/dev/research_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1046\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1046\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recall_results = {}\n",
    "for is_zeroshot in [False, True]:\n",
    "    for nre in [50, 100]:\n",
    "        for conf in eval_config:\n",
    "            train_dataloader = build_detection_test_loader(dataset=train_dataset,\n",
    "                mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "                    T.Resize((800, 800))\n",
    "                ])\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                if (is_zeroshot):\n",
    "                    print(('is_zeroshot', nre, conf))\n",
    "                    recall_results[('is_zeroshot', nre, conf)] = eval_dataset(train_dataloader, model, nre=50, config=conf, trained_triples=trained_triples)\n",
    "                else:\n",
    "                    print(('not_zeroshot', nre, conf))\n",
    "                    recall_results[('not_zeroshot',nre, conf)] = eval_dataset(train_dataloader, model, nre=50, config=conf, trained_triples=None)\n",
    "\n",
    "            train_data_iter = None\n",
    "            del train_dataloader\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd2416",
   "metadata": {},
   "source": [
    "#### EVAL 2/ PREDICATE PREDICTION TASK (GROUND TRUTH BOUNDING BOXES AND LABELS PROVIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b757019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Evalutation of the models performance on detective visual predicate\n",
    "#default\n",
    "total_true_positive = 0\n",
    "total_true_positive_5 = 0\n",
    "total_relationships = 0\n",
    "\n",
    "total_true_positive_zeroshot = 0\n",
    "total_true_positive_5_zeroshot = 0\n",
    "total_relationships_zeroshot = 0\n",
    "\n",
    "#sub\n",
    "total_true_positive_sub = 0\n",
    "total_true_positive_5_sub = 0\n",
    "\n",
    "total_true_positive_sub_zeroshot = 0\n",
    "total_true_positive_5_sub_zeroshot = 0\n",
    "\n",
    "#mul\n",
    "total_true_positive_mul = 0\n",
    "total_true_positive_5_mul = 0\n",
    "\n",
    "total_true_positive_mul_zeroshot = 0\n",
    "total_true_positive_5_mul_zeroshot = 0\n",
    "\n",
    "#transe\n",
    "total_true_positive_transe = 0\n",
    "total_true_positive_5_transe = 0\n",
    "\n",
    "total_true_positive_transe_mul = 0\n",
    "total_true_positive_5_transe_mul = 0\n",
    "\n",
    "for i in range(n_examples):\n",
    "    data = next(test_data_iter)[0]\n",
    "    relationships = data[\"relationships\"]\n",
    "    \n",
    "    if (len(relationships['subj_bboxes']) == 0):\n",
    "        #no relationship annotations for the given image\n",
    "        continue\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicate_distances, predicate_subtract_distances, transe_distances = model.get_predicate_distances(data)\n",
    "        \n",
    "    #Verbose\n",
    "    if (i % 20 == 0 and i > 0):\n",
    "        recall_zeroshot_1 = 0\n",
    "        recall_zeroshot_5 = 0\n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_1 = total_true_positive_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_5 = total_true_positive_5_zeroshot / total_relationships_zeroshot * 100\n",
    "\n",
    "        recall_sub_1 = total_true_positive_sub / total_relationships * 100\n",
    "        recall_sub_5 = total_true_positive_5_sub / total_relationships * 100\n",
    "        recall_zeroshot_sub_1 = 0\n",
    "        recall_zeroshot_sub_5 = 0\n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_sub_1 = total_true_positive_sub_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_sub_5 = total_true_positive_5_sub_zeroshot / total_relationships_zeroshot * 100        \n",
    "        \n",
    "        recall_mul_1 = total_true_positive_mul / total_relationships * 100\n",
    "        recall_mul_5 = total_true_positive_5_mul / total_relationships * 100\n",
    "        recall_zeroshot_mul_1 = 0\n",
    "        recall_zeroshot_mul_5 = 0\n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_mul_1 = total_true_positive_mul_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_mul_5 = total_true_positive_5_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "        \n",
    "        recall_transe_1 = total_true_positive_transe / total_relationships * 100\n",
    "        recall_transe_5 = total_true_positive_5_transe / total_relationships * 100\n",
    "        \n",
    "        recall_transe_mul_1 = total_true_positive_transe_mul / total_relationships * 100\n",
    "        recall_transe_mul_5 = total_true_positive_5_transe_mul / total_relationships * 100\n",
    "        \n",
    "        recall_1 =  total_true_positive / total_relationships * 100\n",
    "        recall_5 = total_true_positive_5 / total_relationships * 100\n",
    "        print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "        print(\"| Recall top 1 {:5.2f} | Recall zeroshot top 1 {:5.2f} |\".format(recall_1, recall_zeroshot_1))\n",
    "        print(\"| Recall top 5 {:5.2f} | Recall zeroshot top 5 {:5.2f} |\".format(recall_5, recall_zeroshot_5))\n",
    "        print(\"| Recall sub top 1 {:5.2f} | Recall zeroshot sub top 1 {:5.2f} |\".format(recall_sub_1, recall_zeroshot_sub_1))\n",
    "        print(\"| Recall sub top 5 {:5.2f} | Recall zeroshot sub top 5 {:5.2f} |\".format(recall_sub_5, recall_zeroshot_sub_5))\n",
    "        print(\"| Recall mul top 1 {:5.2f} | Recall zeroshot mul top 1 {:5.2f} |\".format(recall_mul_1, recall_zeroshot_mul_1))\n",
    "        print(\"| Recall mul top 5 {:5.2f} | Recall zeroshot mul top 5 {:5.2f} |\".format(recall_mul_5, recall_zeroshot_mul_5))        \n",
    "        print(\"| Recall transe 1 {:5.2f} | Recall transe mul 1 {:5.2f} |\".format(recall_transe_1, recall_transe_mul_1))\n",
    "        print(\"| Recall transe 5 {:5.2f} | Recall transe mul 5 {:5.2f} |\".format(recall_transe_5, recall_transe_mul_5))\n",
    "\n",
    "    #select top 5 to calculate recall\n",
    "    \n",
    "    for j, pred_distance in enumerate(predicate_distances):\n",
    "        sorted_pred_distance = np.argsort([item.cpu() for item in pred_distance])\n",
    "        \n",
    "        top_5_pred = sorted_pred_distance[:5]\n",
    "        top_1_pred = sorted_pred_distance[0]\n",
    "        \n",
    "        #subtract\n",
    "        sorted_pred_subtract_distance = np.argsort([item.cpu() for item in predicate_subtract_distances[j]])\n",
    "        top_5_pred_sub = sorted_pred_subtract_distance[:5]\n",
    "        top_1_pred_sub = sorted_pred_subtract_distance[0]\n",
    "        \n",
    "        #transe\n",
    "        sorted_transe_distances = np.argsort([item.cpu() for item in transe_distances[j]])\n",
    "        top_5_vtranse = sorted_transe_distances[:5]\n",
    "        top_1_vtranse = sorted_transe_distances[0]\n",
    "\n",
    "        #pred * transe\n",
    "        pred_transe_multiply_distances = [ item1.cpu() * item2.cpu() for item1,item2 in zip(pred_distance, transe_distances[j])]\n",
    "        top_5_pred_transe = np.argsort(pred_transe_multiply_distances[j].cpu())[:5]\n",
    "        top_1_pred_transe = np.argsort(pred_transe_multiply_distances[j].cpu())[0]\n",
    "        \n",
    "        #pred * sub\n",
    "        pred_multiply_distances = [ item1.cpu() * item2.cpu() for item1,item2 in zip(pred_distance, predicate_subtract_distances[j])]\n",
    "        top_5_pred_mul = np.argsort(pred_multiply_distances)[:5]\n",
    "        top_1_pred_mul = np.argsort(pred_multiply_distances)[0]\n",
    "\n",
    "        gt_subj = model.object_classes[relationships['subj_classes'][j]]\n",
    "        gt_pred = model.predicate_classes[relationships['pred_classes'][j]]\n",
    "        gt_obj = model.object_classes[relationships['obj_classes'][j]]\n",
    "            \n",
    "        #compute true positive\n",
    "        if (top_1_pred == relationships['pred_classes'][j]):\n",
    "            total_true_positive += 1\n",
    "        if (relationships['pred_classes'][j] in top_5_pred):\n",
    "            total_true_positive_5 += 1\n",
    "        \n",
    "        if ('{}-{}-{}'.format(gt_subj, gt_pred, gt_obj) not in trained_triples):\n",
    "            \n",
    "            #default pred\n",
    "            if (top_1_pred == relationships['pred_classes'][j]):\n",
    "                total_true_positive_zeroshot += 1\n",
    "            if (relationships['pred_classes'][j] in top_5_pred):\n",
    "                total_true_positive_5_zeroshot += 1\n",
    "                \n",
    "            #sub\n",
    "            if (top_1_pred_sub == relationships['pred_classes'][j]):\n",
    "                total_true_positive_sub_zeroshot += 1\n",
    "            if (relationships['pred_classes'][j] in top_5_pred_sub):\n",
    "                total_true_positive_5_sub_zeroshot += 1\n",
    "                \n",
    "            #mul (pred*sub)\n",
    "            if (top_1_pred_mul == relationships['pred_classes'][j]):\n",
    "                total_true_positive_mul_zeroshot += 1\n",
    "            if (relationships['pred_classes'][j] in top_5_pred_mul):\n",
    "                total_true_positive_5_mul_zeroshot += 1\n",
    "                \n",
    "            total_relationships_zeroshot += 1\n",
    "        \n",
    "        #sub\n",
    "        if (top_1_pred_sub == relationships['pred_classes'][j]):\n",
    "            total_true_positive_sub += 1\n",
    "        if (relationships['pred_classes'][j] in top_5_pred_sub):\n",
    "            total_true_positive_5_sub += 1\n",
    "        \n",
    "        #pred * sub\n",
    "        if (top_1_pred_mul == relationships['pred_classes'][j]):\n",
    "            total_true_positive_mul += 1\n",
    "        if (relationships['pred_classes'][j] in top_5_pred_mul):\n",
    "            total_true_positive_5_mul += 1\n",
    "\n",
    "        #transe\n",
    "        if (top_1_vtranse == relationships['pred_classes'][j]):\n",
    "            total_true_positive_transe += 1\n",
    "        if (relationships['pred_classes'][j] in top_5_vtranse):\n",
    "            total_true_positive_5_transe += 1\n",
    "        \n",
    "        #pred * sub\n",
    "        if (top_1_pred_transe == relationships['pred_classes'][j]):\n",
    "            total_true_positive_transe_mul += 1\n",
    "        if (relationships['pred_classes'][j] in top_5_pred_transe):\n",
    "            total_true_positive_5_transe_mul += 1\n",
    "            \n",
    "    total_relationships += len(predicate_distances)\n",
    "\n",
    "print(\"Final recall top 1: \", total_true_positive / total_relationships * 100)\n",
    "print(\"Final recall top 5: \", total_true_positive_5 / total_relationships * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d938f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_zeroshot_1 = 0\n",
    "recall_zeroshot_5 = 0\n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_zeroshot_1 = total_true_positive_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_5 = total_true_positive_5_zeroshot / total_relationships_zeroshot * 100\n",
    "\n",
    "recall_sub_1 = total_true_positive_sub / total_relationships * 100\n",
    "recall_sub_5 = total_true_positive_5_sub / total_relationships * 100\n",
    "recall_zeroshot_sub_1 = 0\n",
    "recall_zeroshot_sub_5 = 0\n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_zeroshot_sub_1 = total_true_positive_sub_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_sub_5 = total_true_positive_5_sub_zeroshot / total_relationships_zeroshot * 100        \n",
    "\n",
    "recall_mul_1 = total_true_positive_mul / total_relationships * 100\n",
    "recall_mul_5 = total_true_positive_5_mul / total_relationships * 100\n",
    "recall_zeroshot_mul_1 = 0\n",
    "recall_zeroshot_mul_5 = 0\n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_transe_mul_1 = total_true_positive_transe_mul / total_relationships * 100\n",
    "    recall_transe_mul_5 = total_true_positive_5_transe_mul / total_relationships * 100\n",
    "\n",
    "recall_1 =  total_true_positive / total_relationships * 100\n",
    "recall_5 = total_true_positive_5 / total_relationships * 100\n",
    "print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "print(\"| Recall top 1 {:5.2f} | Recall zeroshot top 1 {:5.2f} |\".format(recall_1, recall_zeroshot_1))\n",
    "print(\"| Recall top 5 {:5.2f} | Recall zeroshot top 5 {:5.2f} |\".format(recall_5, recall_zeroshot_5))\n",
    "print(\"| Recall sub top 1 {:5.2f} | Recall zeroshot sub top 1 {:5.2f} |\".format(recall_sub_1, recall_zeroshot_sub_1))\n",
    "print(\"| Recall sub top 5 {:5.2f} | Recall zeroshot sub top 5 {:5.2f} |\".format(recall_sub_5, recall_zeroshot_sub_5))\n",
    "print(\"| Recall mul top 1 {:5.2f} | Recall zeroshot mul top 1 {:5.2f} |\".format(recall_mul_1, recall_zeroshot_mul_1))\n",
    "print(\"| Recall mul top 5 {:5.2f} | Recall zeroshot mul top 5 {:5.2f} |\".format(recall_mul_5, recall_zeroshot_mul_5))        \n",
    "print(\"| Recall transe 1 {:5.2f} | Recall transe mul 1 {:5.2f} |\".format(recall_transe_1, recall_transe_mul_1))\n",
    "print(\"| Recall transe 5 {:5.2f} | Recall transe mul 5 {:5.2f} |\".format(recall_transe_5, recall_transe_mul_5))\n",
    "recall_zeroshot_mul_1 = total_true_positive_mul_zeroshot / total_relationships_zeroshot * 100\n",
    "recall_zeroshot_mul_5 = total_true_positive_5_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "\n",
    "recall_transe_1 = total_true_positive_transe / total_relationships * 100\n",
    "recall_transe_5 = total_true_positive_5_transe / total_relationships * 100\n",
    "\n",
    "recall_transe_mul_1 = total_true_positive_transe_mul / total_relationships * 100\n",
    "recall_transe_mul_5 = total_true_positive_5_transe_mul / total_relationships * 100\n",
    "\n",
    "recall_1 =  total_true_positive / total_relationships * 100\n",
    "recall_5 = total_true_positive_5 / total_relationships * 100\n",
    "print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "print(\"| Recall top 1 {:5.2f} | Recall zeroshot top 1 {:5.2f} |\".format(recall_1, recall_zeroshot_1))\n",
    "print(\"| Recall top 5 {:5.2f} | Recall zeroshot top 5 {:5.2f} |\".format(recall_5, recall_zeroshot_5))\n",
    "print(\"| Recall sub top 1 {:5.2f} | Recall zeroshot sub top 1 {:5.2f} |\".format(recall_sub_1, recall_zeroshot_sub_1))\n",
    "print(\"| Recall sub top 5 {:5.2f} | Recall zeroshot sub top 5 {:5.2f} |\".format(recall_sub_5, recall_zeroshot_sub_5))\n",
    "print(\"| Recall mul top 1 {:5.2f} | Recall zeroshot mul top 1 {:5.2f} |\".format(recall_mul_1, recall_zeroshot_mul_1))\n",
    "print(\"| Recall mul top 5 {:5.2f} | Recall zeroshot mul top 5 {:5.2f} |\".format(recall_mul_5, recall_zeroshot_mul_5))        \n",
    "print(\"| Recall transe 1 {:5.2f} | Recall transe mul 1 {:5.2f} |\".format(recall_transe_1, recall_transe_mul_1))\n",
    "print(\"| Recall transe 5 {:5.2f} | Recall transe mul 5 {:5.2f} |\".format(recall_transe_5, recall_transe_mul_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b49ed3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evalutation of the models performance on detective visual object (no TransE)\n",
    "import numpy as np\n",
    "#total\n",
    "total_relationships = 0\n",
    "total_relationships_zeroshot = 0\n",
    "\n",
    "#default\n",
    "total_true_positive = 0\n",
    "total_true_positive_5 = 0\n",
    "total_true_positive_10 = 0\n",
    "\n",
    "total_true_positive_zeroshot = 0\n",
    "total_true_positive_5_zeroshot = 0\n",
    "total_true_positive_10_zeroshot = 0\n",
    "\n",
    "#add\n",
    "total_true_positive_add = 0\n",
    "total_true_positive_5_add = 0\n",
    "total_true_positive_10_add = 0\n",
    "\n",
    "total_true_positive_add_zeroshot = 0\n",
    "total_true_positive_5_add_zeroshot = 0\n",
    "total_true_positive_10_add_zeroshot = 0\n",
    "\n",
    "#mul\n",
    "total_true_positive_mul = 0\n",
    "total_true_positive_5_mul = 0\n",
    "total_true_positive_10_mul = 0\n",
    "\n",
    "total_true_positive_mul_zeroshot = 0\n",
    "total_true_positive_5_mul_zeroshot = 0\n",
    "total_true_positive_10_mul_zeroshot = 0\n",
    "\n",
    "for i in range(n_examples):\n",
    "    data = next(test_data_iter)[0]\n",
    "    relationships = data[\"relationships\"]\n",
    "    with torch.no_grad():\n",
    "        object_distances, object_add_distances = model.get_object_distances(data)\n",
    "        \n",
    "    #Verbose\n",
    "    if (i % 20 == 0 and i > 0):\n",
    "        recall_1 =  total_true_positive / total_relationships * 100\n",
    "        recall_5 = total_true_positive_5 / total_relationships * 100\n",
    "        recall_10 = total_true_positive_10 / total_relationships * 100\n",
    "        recall_zeroshot_1 = 0\n",
    "        recall_zeroshot_5 = 0\n",
    "        recall_zeroshot_10 = 0\n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_1 = total_true_positive_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_5 = total_true_positive_5_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_10 = total_true_positive_10_zeroshot / total_relationships_zeroshot * 100\n",
    "\n",
    "        recall_add_1 = total_true_positive_add / total_relationships * 100\n",
    "        recall_add_5 = total_true_positive_5_add / total_relationships * 100\n",
    "        recall_add_10 = total_true_positive_10_add / total_relationships * 100\n",
    "        recall_zeroshot_add_1 = 0\n",
    "        recall_zeroshot_add_5 = 0\n",
    "        recall_zeroshot_add_10 = 0        \n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_add_1 = total_true_positive_add_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_add_5 = total_true_positive_5_add_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_add_10 = total_true_positive_10_add_zeroshot / total_relationships_zeroshot * 100\n",
    "        \n",
    "        recall_mul_1 = total_true_positive_mul / total_relationships * 100\n",
    "        recall_mul_5 = total_true_positive_5_mul / total_relationships * 100\n",
    "        recall_mul_10 = total_true_positive_10_mul / total_relationships * 100\n",
    "        recall_zeroshot_mul_1 = 0\n",
    "        recall_zeroshot_mul_5 = 0\n",
    "        recall_zeroshot_mul_10 = 0\n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_mul_1 = total_true_positive_mul_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_mul_5 = total_true_positive_5_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "            recall_zeroshot_mul_10 = total_true_positive_10_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "        \n",
    "        print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "        print(\"| Recall top 1 {:5.2f} | Recall zeroshot top 1 {:5.2f} |\".format(recall_1, recall_zeroshot_1))\n",
    "        print(\"| Recall top 5 {:5.2f} | Recall zeroshot top 5 {:5.2f} |\".format(recall_5, recall_zeroshot_5))\n",
    "        print(\"| Recall top 10 {:5.2f} | Recall zeroshot top 10 {:5.2f} |\".format(recall_10, recall_zeroshot_10))\n",
    "        print(\"| Recall add top 1 {:5.2f} | Recall zeroshot sub top 1 {:5.2f} |\".format(recall_add_1, recall_zeroshot_add_1))\n",
    "        print(\"| Recall add top 5 {:5.2f} | Recall zeroshot sub top 5 {:5.2f} |\".format(recall_add_5, recall_zeroshot_add_5))\n",
    "        print(\"| Recall add top 10 {:5.2f} | Recall zeroshot sub top 10 {:5.2f} |\".format(recall_add_10, recall_zeroshot_add_10))\n",
    "        print(\"| Recall mul top 1 {:5.2f} | Recall zeroshot mul top 1 {:5.2f} |\".format(recall_mul_1, recall_zeroshot_mul_1))\n",
    "        print(\"| Recall mul top 5 {:5.2f} | Recall zeroshot mul top 5 {:5.2f} |\".format(recall_mul_5, recall_zeroshot_mul_5))        \n",
    "        print(\"| Recall mul top 10 {:5.2f} | Recall zeroshot mul top 10 {:5.2f} |\".format(recall_mul_10, recall_zeroshot_mul_10))        \n",
    "\n",
    "    #select top 5 to calculate recall\n",
    "    for j, obj_distance in enumerate(object_distances):\n",
    "        top_1_obj = np.argsort(obj_distance)[0]\n",
    "        top_5_obj = np.argsort(obj_distance)[:5]\n",
    "        top_10_obj = np.argsort(obj_distance)[:10]\n",
    "        \n",
    "        #add\n",
    "        top_1_obj_add = np.argsort(object_add_distances[j])[0]\n",
    "        top_5_obj_add = np.argsort(object_add_distances[j])[:5]\n",
    "        top_10_obj_add = np.argsort(object_add_distances[j])[:10]\n",
    "        \n",
    "        #pred * add\n",
    "        obj_multiply_distances = [ item1 * item2 for item1,item2 in zip(obj_distance, object_add_distances[j])]\n",
    "        top_1_obj_mul = np.argsort(obj_multiply_distances)[0]\n",
    "        top_5_obj_mul = np.argsort(obj_multiply_distances)[:5]\n",
    "        top_10_obj_mul = np.argsort(obj_multiply_distances)[:10]\n",
    "\n",
    "        gt_subj = model.object_classes[relationships['subj_classes'][j]]\n",
    "        gt_pred = model.predicate_classes[relationships['pred_classes'][j]]\n",
    "        gt_obj = model.object_classes[relationships['obj_classes'][j]]\n",
    "            \n",
    "        #compute true positive        \n",
    "        if ('{}-{}-{}'.format(gt_subj, gt_pred, gt_obj) not in trained_triples):\n",
    "            \n",
    "            #default pred\n",
    "            if (top_1_obj == relationships['obj_classes'][j]):\n",
    "                total_true_positive_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_5_obj):\n",
    "                total_true_positive_5_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_10_obj):\n",
    "                total_true_positive_10_zeroshot += 1\n",
    "\n",
    "            #sub\n",
    "            if (top_1_obj_add == relationships['obj_classes'][j]):\n",
    "                total_true_positive_add_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_5_obj_add):\n",
    "                total_true_positive_5_add_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_10_obj_add):\n",
    "                total_true_positive_10_add_zeroshot += 1\n",
    "                \n",
    "            #mul (pred*sub)\n",
    "            if (top_1_obj_mul == relationships['obj_classes'][j]):\n",
    "                total_true_positive_mul_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_5_obj_mul):\n",
    "                total_true_positive_5_mul_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_10_obj_mul):\n",
    "                total_true_positive_10_mul_zeroshot += 1\n",
    "                \n",
    "            total_relationships_zeroshot += 1\n",
    "        \n",
    "        #normal\n",
    "        if (top_1_obj == relationships['obj_classes'][j]):\n",
    "            total_true_positive += 1\n",
    "        if (relationships['obj_classes'][j] in top_5_obj):\n",
    "            total_true_positive_5 += 1\n",
    "        if (relationships['obj_classes'][j] in top_10_obj):\n",
    "            total_true_positive_10 += 1\n",
    "            \n",
    "        #add\n",
    "        if (top_1_obj_add == relationships['obj_classes'][j]):\n",
    "            total_true_positive_add += 1\n",
    "        if (relationships['obj_classes'][j] in top_5_obj_add):\n",
    "            total_true_positive_5_add += 1\n",
    "        if (relationships['obj_classes'][j] in top_10_obj_add):\n",
    "            total_true_positive_10_add += 1\n",
    "        \n",
    "        #pred * add\n",
    "        if (top_1_obj_mul == relationships['obj_classes'][j]):\n",
    "            total_true_positive_mul += 1\n",
    "        if (relationships['obj_classes'][j] in top_5_obj_mul):\n",
    "            total_true_positive_5_mul += 1\n",
    "        if (relationships['obj_classes'][j] in top_10_obj_mul):\n",
    "            total_true_positive_10_mul += 1\n",
    "    total_relationships += len(object_distances)\n",
    "\n",
    "print(\"Final recall top 1: \", total_true_positive / total_relationships * 100)\n",
    "print(\"Final recall top 5: \", total_true_positive_5 / total_relationships * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa87a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_1 =  total_true_positive / total_relationships * 100\n",
    "recall_5 = total_true_positive_5 / total_relationships * 100\n",
    "recall_10 = total_true_positive_10 / total_relationships * 100\n",
    "recall_zeroshot_1 = 0\n",
    "recall_zeroshot_5 = 0\n",
    "recall_zeroshot_10 = 0\n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_zeroshot_1 = total_true_positive_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_5 = total_true_positive_5_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_10 = total_true_positive_10_zeroshot / total_relationships_zeroshot * 100\n",
    "\n",
    "recall_add_1 = total_true_positive_add / total_relationships * 100\n",
    "recall_add_5 = total_true_positive_5_add / total_relationships * 100\n",
    "recall_add_10 = total_true_positive_10_add / total_relationships * 100\n",
    "recall_zeroshot_add_1 = 0\n",
    "recall_zeroshot_add_5 = 0\n",
    "recall_zeroshot_add_10 = 0        \n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_zeroshot_add_1 = total_true_positive_add_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_add_5 = total_true_positive_5_add_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_add_10 = total_true_positive_10_add_zeroshot / total_relationships_zeroshot * 100\n",
    "\n",
    "recall_mul_1 = total_true_positive_mul / total_relationships * 100\n",
    "recall_mul_5 = total_true_positive_5_mul / total_relationships * 100\n",
    "recall_mul_10 = total_true_positive_10_mul / total_relationships * 100\n",
    "recall_zeroshot_mul_1 = 0\n",
    "recall_zeroshot_mul_5 = 0\n",
    "recall_zeroshot_mul_10 = 0\n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_zeroshot_mul_1 = total_true_positive_mul_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_mul_5 = total_true_positive_5_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "    recall_zeroshot_mul_10 = total_true_positive_10_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "\n",
    "print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "print(\"| Recall top 1 {:5.2f} | Recall zeroshot top 1 {:5.2f} |\".format(recall_1, recall_zeroshot_1))\n",
    "print(\"| Recall top 5 {:5.2f} | Recall zeroshot top 5 {:5.2f} |\".format(recall_5, recall_zeroshot_5))\n",
    "print(\"| Recall top 10 {:5.2f} | Recall zeroshot top 10 {:5.2f} |\".format(recall_10, recall_zeroshot_10))\n",
    "print(\"| Recall add top 1 {:5.2f} | Recall zeroshot sub top 1 {:5.2f} |\".format(recall_add_1, recall_zeroshot_add_1))\n",
    "print(\"| Recall add top 5 {:5.2f} | Recall zeroshot sub top 5 {:5.2f} |\".format(recall_add_5, recall_zeroshot_add_5))\n",
    "print(\"| Recall add top 10 {:5.2f} | Recall zeroshot sub top 10 {:5.2f} |\".format(recall_add_10, recall_zeroshot_add_10))\n",
    "print(\"| Recall mul top 1 {:5.2f} | Recall zeroshot mul top 1 {:5.2f} |\".format(recall_mul_1, recall_zeroshot_mul_1))\n",
    "print(\"| Recall mul top 5 {:5.2f} | Recall zeroshot mul top 5 {:5.2f} |\".format(recall_mul_5, recall_zeroshot_mul_5))        \n",
    "print(\"| Recall mul top 10 {:5.2f} | Recall zeroshot mul top 10 {:5.2f} |\".format(recall_mul_10, recall_zeroshot_mul_10))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_dataloader)\n",
    "data = next(data_iter)[0]\n",
    "# model = RelTransR(cfg)\n",
    "# negative_examples = model.generate_negative_examples(data)\n",
    "# results = model(data,negative_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130cd43b",
   "metadata": {},
   "source": [
    "### Training Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import (\n",
    "    DatasetCatalog, DatasetMapper,\n",
    "    build_detection_train_loader\n",
    ")\n",
    "from config import get_vrd_cfg\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "cfg = get_vrd_cfg()\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .7\n",
    "batch_size = 1\n",
    "\n",
    "# Dataloaders\n",
    "if ('test_dataloader' in vars()):\n",
    "    del test_dataloader\n",
    "\n",
    "train_dataloader = build_detection_train_loader(cfg,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1f29772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetCatalog.get(\"vrd_train\")\n",
    "train_dataloader = build_detection_train_loader(cfg,dataset=train_dataset,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "43a28c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': '../data/vrd/train_images/000000002658.jpg',\n",
       " 'image_id': 2658,\n",
       " 'relationships': {'subj_bboxes': [array([[306, 141, 590, 262]]),\n",
       "   array([[306, 141, 590, 262]]),\n",
       "   array([[539, 397, 680, 595]]),\n",
       "   array([[539, 397, 680, 595]]),\n",
       "   array([[425, 409, 556, 591]]),\n",
       "   array([[425, 409, 556, 591]]),\n",
       "   array([[302, 405, 420, 594]]),\n",
       "   array([[159, 408, 293, 601]]),\n",
       "   array([[ 47, 406, 156, 610]]),\n",
       "   array([[324, 504, 458, 733]])],\n",
       "  'obj_bboxes': [array([[324, 504, 458, 733]]),\n",
       "   array([[324, 504, 458, 733]]),\n",
       "   array([[306, 141, 590, 262]]),\n",
       "   array([[425, 409, 556, 591]]),\n",
       "   array([[302, 405, 420, 594]]),\n",
       "   array([[306, 141, 590, 262]]),\n",
       "   array([[159, 408, 293, 601]]),\n",
       "   array([[ 47, 406, 156, 610]]),\n",
       "   array([[159, 408, 293, 601]]),\n",
       "   array([[306, 141, 590, 262]])],\n",
       "  'union_bboxes': [array([306, 141, 590, 733]),\n",
       "   array([306, 141, 590, 733]),\n",
       "   array([306, 141, 680, 595]),\n",
       "   array([425, 397, 680, 595]),\n",
       "   array([302, 405, 556, 594]),\n",
       "   array([306, 141, 590, 591]),\n",
       "   array([159, 405, 420, 601]),\n",
       "   array([ 47, 406, 293, 610]),\n",
       "   array([ 47, 406, 293, 610]),\n",
       "   array([306, 141, 590, 733])],\n",
       "  'subj_classes': [22, 22, 7, 7, 7, 7, 7, 7, 7, 5],\n",
       "  'pred_classes': [9, 42, 15, 28, 28, 15, 28, 28, 28, 15],\n",
       "  'obj_classes': [5, 5, 22, 7, 7, 22, 7, 7, 7, 22]},\n",
       " 'height': 533,\n",
       " 'width': 800,\n",
       " 'image': tensor([[[ 1,  2,  3,  ..., 25, 25, 25],\n",
       "          [ 1,  2,  3,  ..., 25, 25, 25],\n",
       "          [ 1,  2,  3,  ..., 24, 24, 24],\n",
       "          ...,\n",
       "          [ 4,  5,  6,  ..., 11, 10,  9],\n",
       "          [ 3,  4,  6,  ..., 11, 10,  9],\n",
       "          [ 3,  4,  6,  ..., 11, 10,  9]],\n",
       " \n",
       "         [[ 7,  8,  9,  ..., 24, 24, 24],\n",
       "          [ 7,  8,  9,  ..., 24, 24, 24],\n",
       "          [ 7,  8,  9,  ..., 23, 23, 23],\n",
       "          ...,\n",
       "          [27, 28, 28,  ..., 11, 10,  9],\n",
       "          [26, 27, 28,  ..., 11, 10,  9],\n",
       "          [26, 27, 28,  ..., 11, 10,  9]],\n",
       " \n",
       "         [[12, 13, 14,  ..., 28, 28, 28],\n",
       "          [12, 13, 14,  ..., 28, 28, 28],\n",
       "          [12, 13, 14,  ..., 27, 27, 27],\n",
       "          ...,\n",
       "          [59, 60, 63,  ..., 25, 24, 23],\n",
       "          [58, 59, 63,  ..., 25, 24, 23],\n",
       "          [58, 59, 63,  ..., 25, 24, 23]]], dtype=torch.uint8),\n",
       " 'instances': Instances(num_instances=7, image_height=800, image_width=800, fields=[gt_boxes: Boxes(tensor([[306., 141., 590., 262.],\n",
       "         [324., 504., 458., 733.],\n",
       "         [539., 397., 680., 595.],\n",
       "         [425., 409., 556., 591.],\n",
       "         [302., 405., 420., 594.],\n",
       "         [159., 408., 293., 601.],\n",
       "         [ 47., 406., 156., 610.]])), gt_classes: tensor([22,  5,  7,  7,  7,  7,  7])])}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset.dataset.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbfb89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#params\n",
    "n_datapoints = len(train_dataloader.dataset.dataset)\n",
    "# n_datapoints = 3780\n",
    "n_iters = cfg.SOLVER.MAX_ITER\n",
    "num_epochs = int(n_iters / n_datapoints)\n",
    "chkpoint_it = n_datapoints #create a checkpoint every 1000 iterations\n",
    "\n",
    "#model\n",
    "model = RelTransR(cfg)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "#scheduler\n",
    "learning_rate = 0.001\n",
    "gamma = cfg.SOLVER.GAMMA\n",
    "momentum = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "#wandb\n",
    "log_interval = 20\n",
    "wandb.init(project=\"vrdtransr-project3\", entity=\"herobaby71\")\n",
    "wandb.config = {\n",
    "    \"seed\": 0,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"n_iters\": n_iters,\n",
    "    \"batch_size\": 1\n",
    "}\n",
    "wandb.watch(model, log=\"all\", log_freq=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, lr, it, epoch, loss_subj, loss_pred, loss_obj, loss_transr):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"lr\":lr, \"epoch\": epoch, \"loss\": loss, \"loss_subj\": loss_subj, \"loss_pred\": loss_pred, \"loss_obj\": loss_obj, \"loss_transr\":loss_transr}, step=it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "#checkpoint every 2000 steps\n",
    "chkpoint_it = 1000\n",
    "\n",
    "it = 0\n",
    "initial_it = 0 # checkpoint intial iteration to resume training\n",
    "start_time = time.time()\n",
    "losses = []\n",
    "\n",
    "#load checkpoint\n",
    "load_chkpoint = False\n",
    "if (load_chkpoint):\n",
    "    chkpoint_path = '../checkpoint/'\n",
    "    model_name = 'vrd2_model_18000.pt'\n",
    "    chkpoint_full_path = os.path.join(chkpoint_path, model_name)\n",
    "    it, start_epoch, losses = load_checkpoint(model, chkpoint_full_path, optimizer=optimizer)\n",
    "    initial_it = it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a04184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "trained_triples_path = '../generated/trained_triples.json'\n",
    "trained_triples = {}\n",
    "if (os.path.exists(trained_triples_path)):\n",
    "    with open(trained_triples_path) as file:\n",
    "        trained_triples = json.load(trained_triples_path)\n",
    "else:\n",
    "    iter_dataloader = iter(train_dataloader)\n",
    "    n_iters = len(train_dataloader.dataset.dataset)\n",
    "    for i in range(n_iters):\n",
    "        print(i)\n",
    "        data = next(iter_dataloader)[0]\n",
    "        relationships = data['relationships']\n",
    "        for j in range(len(relationships['subj_classes'])):\n",
    "            subj_cls = model.object_classes[relationships['subj_classes'][j]]\n",
    "            pred_cls = model.predicate_classes[relationships['pred_classes'][j]]\n",
    "            obj_cls = model.object_classes[relationships['obj_classes'][j]]\n",
    "            trained_triples['{}-{}-{}'.format(subj_cls, pred_cls, obj_cls)] = 1\n",
    "\n",
    "    with open(trained_triples_path, 'w') as file:\n",
    "        file.write(json.dumps(trained_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ea1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iter_dataloader = iter(train_dataloader)\n",
    "interval_cnt = 0\n",
    "\n",
    "#Losses\n",
    "total_loss = 0\n",
    "subj_loss = 0\n",
    "obj_loss = 0\n",
    "pred_loss = 0\n",
    "transr_loss = 0\n",
    "\n",
    "for i in range(n_iters):\n",
    "    #iterator\n",
    "    try:\n",
    "        data = next(iter_dataloader)[0]\n",
    "    except StopIteration:\n",
    "        print(\"iterator has reach its end at iteration {}. Initializing a new iterator.\".format(str(it)))\n",
    "        iter_dataloader = iter(train_dataloader)\n",
    "        data = next(iter_dataloader)[0]\n",
    "\n",
    "    #continue training from the previous checkpoint\n",
    "    if (i < initial_it % n_datapoints):\n",
    "        continue\n",
    "\n",
    "    if (len(data['relationships']['subj_bboxes']) == 1):\n",
    "        #image has only one relationship, cannot train\n",
    "        print(\"an image has been removed for this batch\")\n",
    "        continue\n",
    "        \n",
    "    #other exclusion due to bad label\n",
    "    if ('1841.jpg' in data[\"file_name\"]):\n",
    "        print(\"this image has bad label and has been removed.\")\n",
    "        continue\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #forward passes\n",
    "    negative_examples = {}\n",
    "    negative_examples = model.generate_negative_examples(data)\n",
    "#     print(\"number of objects:\", len( data['instances'].get_fields()['gt_boxes']))\n",
    "#     print(\"number of neg examples:\", len(negative_examples))\n",
    "    triplet_losses = model(data, negative_examples)\n",
    "    \n",
    "    #compute gradient backward\n",
    "    final_loss = triplet_losses['obj'] + triplet_losses['pred'] + triplet_losses['subj'] + triplet_losses['transr']\n",
    "    final_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #total loss\n",
    "    total_loss += final_loss.item()\n",
    "    subj_loss += triplet_losses['subj'].item()\n",
    "    pred_loss += triplet_losses['pred'].item()\n",
    "    obj_loss += triplet_losses['obj'].item()\n",
    "    transr_loss += triplet_losses['transr'].item()\n",
    "    \n",
    "    interval_cnt += 1\n",
    "    if (it > initial_it and it % log_interval == 0 and it > 0):\n",
    "        current_loss = total_loss / interval_cnt\n",
    "        losses.append(current_loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        epoch = it / n_datapoints\n",
    "        print('| it {} | epoch {} | lr {} | ms/batch {:5.2f} | loss {:5.2f}'.format(\n",
    "            it, int(epoch), scheduler.get_last_lr()[0], elapsed * 1000 / log_interval, current_loss))\n",
    "        train_log(current_loss, scheduler.get_last_lr()[0], it, int(epoch),\n",
    "                  loss_subj=subj_loss/interval_cnt, loss_pred=pred_loss/interval_cnt,\n",
    "                  loss_obj=obj_loss/interval_cnt, loss_transr=transr_loss/interval_cnt)\n",
    "        total_loss = 0\n",
    "        subj_loss = 0\n",
    "        pred_loss = 0\n",
    "        obj_loss = 0\n",
    "        transr_loss = 0\n",
    "        interval_cnt = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    if (it > initial_it and it % chkpoint_it == 0 and it > 0):\n",
    "        chkpnt = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"it\": it,\n",
    "            \"losses\": losses\n",
    "        }\n",
    "        torch.save(chkpnt, os.path.join(chkpoint_path, 'vrd2_model_transr_{}.pt'.format(str(it))))\n",
    "        \n",
    "    #increment total count\n",
    "    it = it + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b848e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a4722",
   "metadata": {},
   "source": [
    "### [Optional] Test Eval Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4204290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import (\n",
    "    DatasetCatalog, DatasetMapper,\n",
    "    build_detection_train_loader\n",
    ")\n",
    "\n",
    "cfg = get_vrd_cfg()\n",
    "\n",
    "#model\n",
    "# model = RelTransR(cfg)\n",
    "# device = torch.device(\"cuda\")\n",
    "# model.to(device)\n",
    "\n",
    "#test dataloader\n",
    "cfg.DATASETS.TRAIN = (\"vrd_val\", )\n",
    "test_dataloader = build_detection_train_loader(cfg,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90815f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_loss = 0\n",
    "avg_loss = 0\n",
    "cnt = 0\n",
    "\n",
    "total_loss = 0\n",
    "pred_loss = 0\n",
    "interval_cnt = 0\n",
    "\n",
    "model.eval()\n",
    "test_data_iter = iter(test_dataloader)\n",
    "broken_image = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(2000):\n",
    "        data = next(test_data_iter)[0]\n",
    "\n",
    "        #If there is only one relationship (not suitable to be evaluate with the given loss)\n",
    "        if (len(data['relationships']['subj_bboxes']) == 1):\n",
    "            #image has only one relationship, cannot train\n",
    "            print(\"an image has been removed for this batch\")\n",
    "            broken_image.append(data)\n",
    "            continue\n",
    "\n",
    "        # get negative examples and compute losses\n",
    "        negative_examples = model.generate_negative_examples(data)\n",
    "        triplet_losses = model(data, negative_examples)\n",
    "        final_loss = triplet_losses['obj'] + triplet_losses['subj'] + triplet_losses['pred']\n",
    "\n",
    "        #output interval loss\n",
    "        total_loss += final_loss.item()\n",
    "        pred_loss += triplet_losses['subj'].item()\n",
    "        interval_cnt += 1\n",
    "        if (i > 0 and i % 20 == 0):\n",
    "            current_loss = total_loss / interval_cnt\n",
    "            pred_current_loss = pred_loss / interval_cnt\n",
    "            print('| iter: {} | loss {:5.2f} | pred_loss {:5.2f}'.format(i, current_loss, pred_current_loss))\n",
    "            total_loss = 0\n",
    "            pred_loss = 0\n",
    "            interval_cnt = 0\n",
    "\n",
    "        #update global loss\n",
    "        global_loss += final_loss.item()\n",
    "        cnt += 1\n",
    "avg_loss = total_loss / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a64a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde9f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226eb1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16673bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f02afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del train_dataloader\n",
    "del test_dataloader\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a93cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "chkpoint_path = '../checkpoint/'\n",
    "\n",
    "onlyfiles = [f for f in listdir(chkpoint_path) if isfile(join(chkpoint_path, f))]\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.tensor([1,2,3])\n",
    "b= torch.tensor([45, 65, 65])\n",
    "\n",
    "c = torch.stack([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b460f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c568deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([[1,2,3], [2,3,4]])\n",
    "y = torch.tensor([[1,2,3.2]])\n",
    "\n",
    "1- F.cosine_similarity(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.tensor([[1,2,3], [2,3,4]])\n",
    "y = torch.tensor([[1.5,2.25], [2.1,3.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2187cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b95cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((x, y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8480922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
