{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f100201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from utils.register_dataset import register_vrd_dataset\n",
    "from config import parse_args, get_vrd_cfg\n",
    "from utils.trainer import CustomTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19468c06",
   "metadata": {},
   "source": [
    "### [Optional] Training Visual Backbone Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18682efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/17 08:03:44 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=101, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=400, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/17 08:03:45 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"
     ]
    }
   ],
   "source": [
    "#Train Dataset\n",
    "cfg = get_vrd_cfg()\n",
    "register_vrd_dataset('vrd')\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = CustomTrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ace061",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.OUTPUT_DIR = os.path.join(cfg.OUTPUT_DIR, 'detectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f8a938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93451a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "detectron = build_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = detectron.backbone\n",
    "b = None\n",
    "detectron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a2c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "detectron.roi_heads.box_pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.modeling.poolers import ROIPooler\n",
    "pooler = ROIPooler((7, 7), pooler_type='ROIAlignV2', scales=[1/4, 1/8, 1/16, 1/32], sampling_ratio=4)\n",
    "pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b88019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detections Output\n",
    "import random\n",
    "from utils.register_dataset import register_vrd_dataset\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "register_vrd_dataset('vrd')\n",
    "detectron_dataset = DatasetCatalog.get(\"vrd_train\")\n",
    "vrd_metadata = MetadataCatalog.get(\"vrd_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49adacf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "for d in random.sample(detectron_dataset, 3):\n",
    "    print(d[\"file_name\"])\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=vrd_metadata, scale=1)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    img = vis.get_image()\n",
    "    plt.figure(dpi=1200)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from detectron2.structures import BoxMode\n",
    "from config import ROOT_DIR, VRD_DATASET_PATH, VG200_DATASET_PATH\n",
    "\n",
    "def load_dataset_annotations(dataset_name):\n",
    "    \"\"\"\n",
    "        Load raw annotation dataset and convert it into standard detectron2 format\n",
    "        Input:\n",
    "            dataset_name: 'vrd/train' or 'vrd/val'\n",
    "    \"\"\"\n",
    "    dataset_dicts = []\n",
    "    set_name, set_type = dataset_name.split('/')\n",
    "    if (set_name == 'vrd'):\n",
    "        \n",
    "        #VRD data path\n",
    "        dataset_path = VRD_DATASET_PATH\n",
    "\n",
    "        #load annotations file\n",
    "        annotations = {}\n",
    "        file_name = \"new_annotations_{}.json\".format(set_type)\n",
    "        with open(os.path.join(dataset_path, file_name)) as annotations_file:\n",
    "            annotations = json.load(annotations_file)\n",
    "        \n",
    "        #Map 'annotations' to the correct format\n",
    "        img_file_dir = os.path.join(VRD_DATASET_PATH, '{}_images'.format(set_type))\n",
    "        for key, val in annotations.items():\n",
    "            record = {}\n",
    "\n",
    "            #populating image fields\n",
    "            img_file_path = os.path.join(img_file_dir, key)\n",
    "            height, width = cv2.imread(img_file_path).shape[:2]\n",
    "\n",
    "            record['file_name'] = img_file_path\n",
    "            record['height'] = height\n",
    "            record['width'] = width\n",
    "            record['image_id'] = int(key.split('.')[0])\n",
    "\n",
    "            #populating annotations from record\n",
    "            objs = []\n",
    "            visited = set()\n",
    "            for anno in val:\n",
    "                \n",
    "                subj = anno['subject'] #ymin, ymax, xmin, xmax\n",
    "                obj = anno['object']\n",
    "                bbox_subj = subj['bbox']\n",
    "                bbox_obj = obj['bbox']\n",
    "                new_bbox_subj = [bbox_subj[2], bbox_subj[0], bbox_subj[3], bbox_subj[1]]\n",
    "                new_bbox_obj = [bbox_obj[2], bbox_obj[0], bbox_obj[3], bbox_obj[1]]\n",
    "                \n",
    "                subj_dict = {\n",
    "                    'bbox': new_bbox_subj,\n",
    "                    'bbox_mode': BoxMode.XYXY_ABS,\n",
    "                    'category_id': subj['category']\n",
    "                }\n",
    "                if (tuple(bbox_subj) not in visited):\n",
    "                    objs.append(subj_dict)\n",
    "                    visited.add(tuple(bbox_subj))\n",
    "\n",
    "                obj_dict = {\n",
    "                    'bbox': new_bbox_obj,\n",
    "                    'bbox_mode': BoxMode.XYXY_ABS,\n",
    "                    'category_id': obj['category'],\n",
    "                }\n",
    "                if (tuple(bbox_obj) not in visited):\n",
    "                    objs.append(obj_dict)\n",
    "                    visited.add(tuple(bbox_obj))\n",
    "\n",
    "            record['annotations'] = objs\n",
    "                         \n",
    "            #add img to dataset dict\n",
    "            dataset_dicts.append(record)\n",
    "    \n",
    "    return dataset_dicts\n",
    "data = load_dataset_annotations('vrd/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418a24d",
   "metadata": {},
   "source": [
    "### [Required] DataSet and DataLoader (Step 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da5eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from torch._C import import_ir_module\n",
    "from utils.boxes import boxes_union\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from config import ROOT_DIR, VRD_DATASET_PATH, VG200_DATASET_PATH\n",
    "VRD_DATASET_PATH = \"../data/vrd/\"\n",
    "\n",
    "# custom transformation\n",
    "detectron_transform = T.Resize((800, 800))\n",
    "\n",
    "class VRDDataset(Dataset):\n",
    "    def __init__(self, set_type=\"train\", transform=detectron_transform):\n",
    "        \"\"\"\n",
    "            Note:\n",
    "                transform only applys on bounding boxes. The transformation to the image should be done by detectron2 dataloader.\n",
    "        \"\"\"\n",
    "        annotations_path = os.path.join(\n",
    "            VRD_DATASET_PATH, \"new_annotations_{}.json\".format(set_type)\n",
    "        )\n",
    "        self.images_dir = os.path.join(VRD_DATASET_PATH, \"{}_images\".format(set_type))\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(annotations_path) as fp:\n",
    "            raw_annotation = json.load(fp)\n",
    "        self.annotations = list(raw_annotation.items())\n",
    "\n",
    "        # check if the data is pre-generated\n",
    "        roidb_chkpt = os.path.join(VRD_DATASET_PATH, \"vrd_roidb_{}.json\")\n",
    "        if os.path.exists(roidb_chkpt):\n",
    "            with open(roidb_chkpt) as fp:\n",
    "                self.roidb = json.load(fp)\n",
    "        else:\n",
    "            roidb = []\n",
    "            for img_name, annotations in raw_annotation.items():\n",
    "                subj_bboxes = []\n",
    "                obj_bboxes = []\n",
    "                union_bboxes = []\n",
    "                unique_objects = {}\n",
    "                subj_classes = []\n",
    "                obj_classes = []\n",
    "                pred_classes = []\n",
    "                for anno in annotations:\n",
    "                    subj = anno[\"subject\"]\n",
    "                    obj = anno[\"object\"]\n",
    "\n",
    "                    subj[\"bbox\"] = [\n",
    "                        subj[\"bbox\"][2],\n",
    "                        subj[\"bbox\"][0],\n",
    "                        subj[\"bbox\"][3],\n",
    "                        subj[\"bbox\"][1],\n",
    "                    ]  # XYXY\n",
    "                    obj[\"bbox\"] = [\n",
    "                        obj[\"bbox\"][2],\n",
    "                        obj[\"bbox\"][0],\n",
    "                        obj[\"bbox\"][3],\n",
    "                        obj[\"bbox\"][1],\n",
    "                    ]\n",
    "\n",
    "                    union_bboxes.append(\n",
    "                        boxes_union(np.array([subj[\"bbox\"]]), np.array([obj[\"bbox\"]]))[0]\n",
    "                    )\n",
    "                    subj_bboxes.append(subj[\"bbox\"])\n",
    "                    obj_bboxes.append(obj[\"bbox\"])\n",
    "                    unique_objects[(tuple(subj[\"bbox\"]))] = {\n",
    "                        \"bbox\": subj[\"bbox\"],\n",
    "                        \"bbox_mode\": 0,  # BoxMode.XYXY_ABS\n",
    "                        \"category_id\": subj[\"category\"],\n",
    "                    }\n",
    "                    unique_objects[(tuple(obj[\"bbox\"]))] = {\n",
    "                        \"bbox\": obj[\"bbox\"],\n",
    "                        \"bbox_mode\": 0,\n",
    "                        \"category_id\": obj[\"category\"],\n",
    "                    }\n",
    "                    subj_classes.append(subj[\"category\"])\n",
    "                    obj_classes.append(obj[\"category\"])\n",
    "                    pred_classes.append(anno[\"predicate\"])\n",
    "                                \n",
    "                roidb.append(\n",
    "                    {\n",
    "                        # Detectron\n",
    "                        \"file_name\": os.path.join(self.images_dir, img_name),\n",
    "                        \"image_id\": int(img_name.split(\".\")[0]),\n",
    "                        \"annotations\": list(unique_objects.values()),\n",
    "                        # Relationships\n",
    "                        \"relationships\": {\n",
    "                            \"subj_bboxes\": subj_bboxes,\n",
    "                            \"obj_bboxes\": obj_bboxes,\n",
    "                            \"union_bboxes\": union_bboxes,\n",
    "                            \"subj_classes\": subj_classes,\n",
    "                            \"pred_classes\": pred_classes,\n",
    "                            \"obj_classes\": obj_classes,\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "            self.roidb = roidb\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.roidb)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.roidb[idx]\n",
    "        cv2.setNumThreads(0)\n",
    "        image = cv2.imread(item[\"file_name\"])\n",
    "        \n",
    "        #get transformation\n",
    "        auginput = T.AugInput(image)\n",
    "        transform = self.transform(auginput)\n",
    "        auginput2 = T.AugInput(image)\n",
    "        transform2 = self.transform(auginput2)\n",
    "        relationships = item[\"relationships\"]\n",
    "        \n",
    "        #update bboxes\n",
    "        subj_bboxes = []\n",
    "        obj_bboxes = []\n",
    "        union_bboxes = []\n",
    "        \n",
    "        for subj_bbox, obj_bbox in zip(relationships['subj_bboxes'], relationships['obj_bboxes']):\n",
    "            new_subj_box = transform.apply_box(subj_bbox)\n",
    "            new_obj_box = transform2.apply_box(obj_bbox)\n",
    "            new_union_box = boxes_union(copy.deepcopy(new_subj_box), copy.deepcopy(new_obj_box))[0]\n",
    "            \n",
    "            subj_bboxes.append(new_subj_box)\n",
    "            obj_bboxes.append(new_obj_box)\n",
    "            union_bboxes.append(new_union_box)\n",
    "        relationships['subj_bboxes'] = subj_bboxes\n",
    "        relationships['obj_bboxes'] = obj_bboxes\n",
    "        relationships['union_bboxes'] = union_bboxes\n",
    "        # add height and width\n",
    "        height, width = image.shape[:2]\n",
    "        item[\"height\"] = height\n",
    "        item[\"width\"] = width\n",
    "        \n",
    "        return item\n",
    "\n",
    "\n",
    "def get_object_classes(set_name):\n",
    "    if set_name == \"vrd\":\n",
    "        classes = []\n",
    "        with open(os.path.join(VRD_DATASET_PATH, \"objects.json\")) as fp:\n",
    "            classes = json.load(fp)\n",
    "        return classes\n",
    "    elif set_name == \"vg\":\n",
    "        # to be implemented\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_predicate_classes(set_name):\n",
    "    if set_name == \"vrd\":\n",
    "        classes = []\n",
    "        with open(os.path.join(VRD_DATASET_PATH, \"predicates.json\")) as fp:\n",
    "            classes = json.load(fp)\n",
    "        # add unknown predicate class for missing predicates\n",
    "        # classes.insert(0, 'unknown')\n",
    "        classes.append('unrelated')\n",
    "        return classes\n",
    "    elif set_name == \"vg\":\n",
    "        # to be implemented\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def visualize_image_bboxes(image, instances, object_classes=None):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            image: image tensor\n",
    "            instances: Instances object from detectron2\n",
    "    \"\"\"\n",
    "    img = copy.deepcopy(image)\n",
    "    instances_dict = instances[0].get_fields()\n",
    "    bounding_boxes = [box.tolist() for box in instances_dict['pred_boxes']]\n",
    "    labels = instances_dict['pred_classes']\n",
    "    \n",
    "    for bbox in bounding_boxes:\n",
    "        img = cv2.rectangle(img, [int(coord) for coord in bbox[0:2]], [int(coord) for coord in bbox[2:4]], (255,255,255), 2)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_bboxes(dataset):\n",
    "    pred_classes = get_predicate_classes(\"vrd\")\n",
    "    obj_classes = get_object_classes(\"vrd\")\n",
    "\n",
    "    for i in random.sample(range(len(dataset)), 60):\n",
    "        image, cropped_img, anno = dataset[i]\n",
    "        img = copy.deepcopy(image)\n",
    "        subj = anno[\"subject\"]\n",
    "        obj = anno[\"object\"]\n",
    "\n",
    "        subject_class = obj_classes[subj[\"category\"]]\n",
    "        predicate_class = pred_classes[anno[\"predicate\"]]\n",
    "        object_class = obj_classes[obj[\"category\"]]\n",
    "\n",
    "        img = cv2.rectangle(img, subj[\"bbox\"][0:2], subj[\"bbox\"][2:4], (0, 0, 255), 2)\n",
    "        img = cv2.rectangle(img, obj[\"bbox\"][0:2], obj[\"bbox\"][2:4], (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow(\n",
    "            \" \".join((subject_class, predicate_class, object_class)), cropped_img\n",
    "        )\n",
    "        cv2.waitKey(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86653a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract ROI Features\n",
    "\"\"\"\n",
    "Relevant Links:\n",
    "    https://towardsdatascience.com/faster-rcnn-object-detection-f865e5ed7fc4#:~:text=Faster%20RCNN%20is%20an%20object,SSD%20(%20Single%20Shot%20Detector).\n",
    "\"\"\"\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures import ImageList\n",
    "\n",
    "from detectron2.modeling import build_backbone\n",
    "from detectron2.modeling.poolers import ROIPooler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from config import get_vrd_cfg\n",
    "\n",
    "def get_roi_features(images, box_lists, output_size=(14,14)):\n",
    "    \"\"\"\n",
    "        Get image features from the backbone network\n",
    "        Input:\n",
    "            images: (ImageList.from_tensors) with dimension (B,C,W,H)\n",
    "            box_lists: A list of N boxes\n",
    "    \"\"\"\n",
    "    cfg = get_vrd_cfg()\n",
    "    backbone = build_backbone(cfg)\n",
    "    pooler = ROIPooler(output_size, pooler_type='ROIAlignV2', scales=[1/4, 1/8, 1/16, 1/32, 1/64], sampling_ratio=4)\n",
    "    feature_maps = backbone(images)\n",
    "    feature_maps = [feature_maps['p{}'.format(i)] for i in range(2,7)]\n",
    "    regions_feature = pooler(feature_maps, box_lists)\n",
    "    print(regions_feature.shape)\n",
    "\n",
    "    return regions_feature\n",
    "\n",
    "def generate_roi_features(dataset):\n",
    "    pass\n",
    "    # images = ImageList.from_tensors(tensors)  # preprocessed input tensor\n",
    "    # model = build_model(cfg)\n",
    "    # DetectionCheckpointer(model).load(\"output/model_final.pth\")\n",
    "    # model.eval()\n",
    "    # features = model.backbone(images.tensor)\n",
    "    # proposals, _ = model.proposal_generator(images, features)\n",
    "    # instances, _ = model.roi_heads(images, features, proposals)\n",
    "    # mask_features = [features[f] for f in model.roi_heads.in_features]\n",
    "    # mask_features = model.roi_heads.mask_pooler(mask_features, [x.pred_boxes for x in instances])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36429a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#[Only Run once] Register dataset with detectron2 instead of using my own dataloader\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from utils.annotations import get_object_classes\n",
    "\n",
    "def get_vrd_dicts(dataset_name):\n",
    "    set_name, set_type = dataset_name.split('/')\n",
    "    dataset = None\n",
    "    if (set_name == 'vrd'):\n",
    "        dataset = VRDDataset(set_type=set_type) \n",
    "    return dataset\n",
    "\n",
    "def register_vrd_dataset(set_name):\n",
    "    \"\"\"\n",
    "        Register dataset and its metadata to the detectron2 engine\n",
    "        Input:\n",
    "            set_name: 'vrd' or vg200\n",
    "    \"\"\"\n",
    "    thing_classes = get_object_classes(set_name)\n",
    "\n",
    "    #register the annotations\n",
    "    for d_type in ['train', 'val']:\n",
    "        DatasetCatalog.register('_'.join((set_name, d_type)), lambda d_type=d_type: get_vrd_dicts('/'.join((set_name, d_type))))\n",
    "        MetadataCatalog.get('_'.join((set_name, d_type))).set(thing_classes=thing_classes)\n",
    "register_vrd_dataset('vrd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf6467d",
   "metadata": {},
   "source": [
    "### [Optional] Testing/Check whether ROI is updated with Image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dataloader\n",
    "from detectron2.data import (\n",
    "    DatasetCatalog, DatasetMapper,\n",
    "    build_detection_train_loader\n",
    ")\n",
    "from config import get_vrd_cfg\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "cfg = get_vrd_cfg()\n",
    "dataloader = build_detection_train_loader(cfg,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c75ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = next(data_iter)\n",
    "train_features = train_features[0] #first element, dataloader with batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e82032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_features\n",
    "relationships = data['relationships']\n",
    "subj_boxes = relationships['subj_bboxes']\n",
    "union_boxes = relationships['union_bboxes']\n",
    "obj_boxes = relationships['obj_bboxes']\n",
    "subj_classes = relationships['subj_classes']\n",
    "pred_classes = relationships['pred_classes']\n",
    "obj_classes = relationships['obj_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.boxes import boxes_intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_line_overlap(box1, box2):\n",
    "    \"\"\"\n",
    "        box1: (xmin1, xmax1)\n",
    "        box2: (xmin2, xmax2)\n",
    "    \"\"\"\n",
    "    xmin1, xmax1 = box1\n",
    "    xmin2, xmax2 = box2\n",
    "    return xmax1 >= xmin2 and xmax2 >= xmin1\n",
    "\n",
    "def is_box_overlap(box1, box2):\n",
    "    xmin1, ymin1, xmax1, ymax1 = box1\n",
    "    xmin2, ymin2, xmax2, ymax2 = box2\n",
    "    \n",
    "    return is_line_overlap((xmin1, xmax1), (xmin2, xmax2)) and is_line_overlap((ymin1, ymax1), (ymin2, ymax2))\n",
    "\n",
    "boxes_intersect(np.array([box[0] for box in subj_boxes]), np.array([box[0] for box in obj_boxes]))\n",
    "# np.array([subj[\"bbox\"]]), np.array([obj[\"bbox\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from utils.annotations import get_object_classes\n",
    "\n",
    "#thing classes\n",
    "thing_classes = get_object_classes('vrd')\n",
    "\n",
    "#Visualizing the trained_features\n",
    "# train_features = data\n",
    "img = train_features['image']\n",
    "img = img.numpy()\n",
    "img = copy.deepcopy(np.transpose(img, (1,2, 0)))\n",
    "\n",
    "boxes = train_features['instances'].get_fields()['gt_boxes']\n",
    "classes = train_features['instances'].get_fields()['gt_classes']\n",
    "for cls, box in zip(classes, boxes):\n",
    "    int_box = [int(i) for i in box]\n",
    "    \n",
    "plt.figure(dpi=800)\n",
    "plt.imshow(img[:,:,[2,1,0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940dd72a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get Region of Interests of the ground truth bboxes of the image\n",
    "images = torch.unsqueeze(train_features['image'], axis=0)\n",
    "boxes = train_features['instances'].get_fields()['gt_boxes']\n",
    "visual_features = get_roi_features(images.float(), box_lists=[boxes], output_size=(7, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520197c",
   "metadata": {},
   "source": [
    "### [Required] BERT Modeling (extracting features from text) (Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab13229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_word_features(triples, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        triples: (Subj, Pred, Obj)\n",
    "    Return:\n",
    "        dict of [CLS, Subj, Pred, Obj, SEP] embeddings\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    marked_text = \"[CLS] \" + \" \".join(triples) + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    \n",
    "    # Save the token split to average them later on\n",
    "    token_placements = defaultdict(list)\n",
    "    triples_temp = list(triples)\n",
    "    for i, tok in enumerate(tokenized_text):\n",
    "        stip_tok = tok.replace('#', '')\n",
    "        if (stip_tok in triples_temp[0]):\n",
    "            token_placements['subj'].append(i)\n",
    "            triples_temp[0] = triples_temp[0].replace(stip_tok, '')\n",
    "        elif (stip_tok in triples_temp[1]):\n",
    "            token_placements['pred'].append(i)\n",
    "            triples_temp[1] = triples_temp[1].replace(stip_tok, '')\n",
    "        elif (stip_tok in triples_temp[2]):\n",
    "            token_placements['obj'].append(i)\n",
    "            triples_temp[2] = triples_temp[2].replace(stip_tok, '')\n",
    "        elif (not tok == '[CLS]' and not tok == '[SEP]'):\n",
    "            print(tok, triples)\n",
    "\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)  # one sentence\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on\n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
    "        # becase we set `output_hidden_states = True`, the third item will be the\n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings.size()\n",
    "\n",
    "    # remove dimension 1\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "\n",
    "    # get token embeddings (list of token embeddings)\n",
    "    token_vecs_cat = []\n",
    "    for token in token_embeddings:\n",
    "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "        token_vecs_cat.append(cat_vec)\n",
    "    results['CLS'] = token_vecs_cat[0]\n",
    "    results['SEP'] = token_vecs_cat[-1]\n",
    "\n",
    "    # average the token embeddings for word that are splitted to get word embeddings\n",
    "    for key, val in token_placements.items():\n",
    "        results[key] = token_vecs_cat[val[0]]\n",
    "        for i in range(1, len(val)):\n",
    "            results[key] += token_vecs_cat[val[i]]\n",
    "        results[key] = results[key] / len(val)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be38a58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = BertModel.from_pretrained(\n",
    "#     \"bert-base-uncased\",\n",
    "#     output_hidden_states=True,  # Whether the model returns all hidden-states.\n",
    "# )\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d045fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#object and predicate labels\n",
    "from utils.annotations import get_object_classes, get_predicate_classes\n",
    "\n",
    "object_classes = get_object_classes('vrd')\n",
    "predicate_classes = get_predicate_classes('vrd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfd42873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "triples_embeddings_path = '../generated/triples_embeddings.pt'\n",
    "def get_triples_features(set_name='vrd'):\n",
    "    triples_memo = {}\n",
    "#     triples_memo = torch.load(triples_embeddings_path)\n",
    "    if (os.path.exists(triples_embeddings_path)):\n",
    "        triples_memo = torch.load(triples_embeddings_path)\n",
    "        return triples_memo\n",
    "        \n",
    "    # initialize the model and tokenizer\n",
    "    model = BertModel.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        output_hidden_states=True,  # Whether the model returns all hidden-states.\n",
    "    )\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # iterate through the all the triples, and extract the features\n",
    "    dataset = get_vrd_dicts('vrd/train')\n",
    "    for train_feat in dataset:\n",
    "        rel = train_feat['relationships']\n",
    "        for subj_ind, pred_ind, obj_ind in zip(rel['subj_classes'], rel['pred_classes'], rel['obj_classes']):\n",
    "            for neg_obj in object_classes:\n",
    "                print((subj_ind, pred_ind, obj_ind))\n",
    "                #neg subj\n",
    "                triples_text = (neg_obj, predicate_classes[pred_ind], object_classes[obj_ind])\n",
    "                if ('-'.join(triples_text) in triples_memo):\n",
    "                    continue\n",
    "                word_feat = get_word_features(triples_text, model, tokenizer)\n",
    "                triples_memo['-'.join(triples_text)] = word_feat\n",
    "                \n",
    "                #neg obj\n",
    "                triples_text = (object_classes[subj_ind], predicate_classes[pred_ind], neg_obj)\n",
    "                if ('-'.join(triples_text) in triples_memo):\n",
    "                    continue\n",
    "                word_feat = get_word_features(triples_text, model, tokenizer)\n",
    "                triples_memo['-'.join(triples_text)] = word_feat\n",
    "                \n",
    "    \n",
    "    try:\n",
    "        torch.save(triples_memo, triples_embeddings_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return triples_memo\n",
    "token_embeddings = get_triples_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575df0db",
   "metadata": {},
   "source": [
    "### [Optional] Visualize language triples similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "def cosine_similarity(a, b):\n",
    "    return (a @ b.T) / (norm(a)*norm(b))\n",
    "\n",
    "def get_predicate_similarity_scores(test_triples_feats, token_name='pred'):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            test_triples_feat: triple feature to test\n",
    "            token: pred, obj, subj, or CLS\n",
    "        token\n",
    "    \"\"\"\n",
    "    similarity_scores = {}\n",
    "    for triples, features in triples_memo.items():\n",
    "        try:\n",
    "            similarity_scores[triples] = cosine_similarity(test_triples_feats[token_name], features[token_name])\n",
    "        except:\n",
    "            print(triples)\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cc841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_triples = 'motorcycle-on-road'\n",
    "test_triples_feats = triples_memo[test_triples]\n",
    "similarity_scores = get_predicate_similarity_scores(test_triples_feats, token_name='obj')\n",
    "dict(sorted(similarity_scores.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacks = {\n",
    "    'CLS': [],\n",
    "    'SEP': [],\n",
    "    'subj': [],\n",
    "    'pred': [],\n",
    "    'obj': [],\n",
    "    'subj-obj': [],\n",
    "    'subj+pred': [],\n",
    "}\n",
    "triples_label = []\n",
    "for key, trip in triples_memo.items():\n",
    "    for k, val in trip.items():\n",
    "        stacks[k].append(val)\n",
    "    stacks['subj-obj'].append(torch.cat((trip['subj'], trip['obj']), dim=0))\n",
    "    stacks['subj+pred'].append((trip['subj'] + trip['pred'])/2)\n",
    "    \n",
    "    triples_label.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mp\n",
    "\n",
    "#color maping for plot\n",
    "colors =  [ list(np.random.choice(range(256), size=3)/255) for _ in range(100)]\n",
    "cmap = mp.colors.ListedColormap(colors, name='from_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5491e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#t-sne 2-dim predicate\n",
    "tsne = TSNE(2, perplexity=30, verbose=1)\n",
    "predicate_stack = np.array([item.numpy() for item in stacks['pred']])\n",
    "tsne_proj = tsne.fit_transform(predicate_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab997810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subj-obj 2-dim t-sne\n",
    "tsne = TSNE(2, perplexity=30, verbose=1)\n",
    "subj_obj_stack = np.array([item.numpy() for item in stacks['subj-obj']])\n",
    "tsne_proj = tsne.fit_transform(subj_obj_stack)\n",
    "\n",
    "#plotting\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "num_categories = 100\n",
    "predicate_classes = get_predicate_classes('vrd')\n",
    "pred_labels = np.array([item.split('-')[1] for item in triples_label])\n",
    "for lab, pred in enumerate(predicate_classes):\n",
    "    indices = pred_labels == pred\n",
    "    ax.scatter(tsne_proj[indices,0],tsne_proj[indices,1], c=np.array(cmap(lab)).reshape(1,4), label = pred ,alpha=0.5)\n",
    "ax.legend(fontsize='large', markerscale=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58382dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(subj+pred)/2 2-dim t-sne\n",
    "tsne = TSNE(2, perplexity=30, verbose=1)\n",
    "subj_pred_stack = np.array([item.numpy() for item in stacks['subj+pred']])\n",
    "tsne_proj = tsne.fit_transform(subj_pred_stack)\n",
    "\n",
    "#plotting\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "num_categories = 100\n",
    "predicate_classes = get_predicate_classes('vrd')\n",
    "pred_labels = np.array([item.split('-')[1] for item in triples_label])\n",
    "for lab, pred in enumerate(predicate_classes):\n",
    "    indices = pred_labels == pred\n",
    "    ax.scatter(tsne_proj[indices,0],tsne_proj[indices,1], c=np.array(cmap(lab)).reshape(1,4), label = pred ,alpha=0.5)\n",
    "ax.legend(fontsize='large', markerscale=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obj 2-dim t-sne\n",
    "tsne = TSNE(2, perplexity=30, verbose=1)\n",
    "obj_stack = np.array([item.numpy() for item in stacks['obj']])\n",
    "tsne_proj = tsne.fit_transform(obj_stack)\n",
    "\n",
    "#plotting\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "num_categories = 100\n",
    "predicate_classes = get_predicate_classes('vrd')\n",
    "pred_labels = np.array([item.split('-')[1] for item in triples_label])\n",
    "for lab, pred in enumerate(predicate_classes):\n",
    "    indices = pred_labels == pred\n",
    "    ax.scatter(tsne_proj[indices,0],tsne_proj[indices,1], c=np.array(cmap(lab)).reshape(1,4), label = pred ,alpha=0.5)\n",
    "ax.legend(fontsize='large', markerscale=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeeba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word feature length\n",
    "obj_stack[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3288f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#roi feature length\n",
    "visual_features.reshape((6, 256*7*7)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efebc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.token_embeddings, triples_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810abf10",
   "metadata": {},
   "source": [
    "### [Required] VRDTransR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efcbd48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleDict\n",
    "\n",
    "from utils.annotations import get_object_classes, get_predicate_classes\n",
    "from utils.boxes import boxes_union\n",
    "\n",
    "from modeling.roi_features import get_roi_features\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.modeling import build_backbone\n",
    "from detectron2.modeling.poolers import ROIPooler\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures.boxes import Boxes\n",
    "import random\n",
    "\n",
    "class RelTransR(nn.Module):\n",
    "    def __init__(self, cfg, pooling_size=(7,7), training=True):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Object and Predicate Classes\n",
    "        self.object_classes = get_object_classes('vrd')\n",
    "        self.predicate_classes = get_predicate_classes('vrd')\n",
    "\n",
    "        # Embeddings dimensions\n",
    "        self.visual_feature_dim = 256*pooling_size[0]*pooling_size[1]\n",
    "        self.visual_hidden_dim = 128*pooling_size[0]*pooling_size[1]\n",
    "        self.word_feature_dim = 3072\n",
    "        self.trans_feature_dim = 256\n",
    "        self.emb_feature_dim = 64\n",
    "        \n",
    "        # Spatial Module\n",
    "        self.spatial_feature_dim = 22\n",
    "        self.spatial_hidden_dim = 64\n",
    "        self.fc_spatial = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.spatial_feature_dim, self.spatial_hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.spatial_hidden_dim, self.spatial_hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # Visual Modal\n",
    "        self.detectron = build_model(cfg)\n",
    "        if (training):\n",
    "            self._load_detectron_chkpoints(cfg)\n",
    "\n",
    "        # Seperate for predicate\n",
    "        self.backbone = copy.deepcopy(self.detectron.backbone)\n",
    "        self.pooler = copy.deepcopy(self.detectron.roi_heads.box_pooler)\n",
    "\n",
    "        # Language Modal\n",
    "        self.bert_model = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\",\n",
    "            # Whether the model returns all hidden-states.\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        self.bert_model.to('cuda')\n",
    "        self.bert_model.eval()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        # Pre-trained token embeddings (static without changes for now)\n",
    "        self.triples_embeddings_path = '../generated/triples_embeddings.pt'\n",
    "\n",
    "        # Fully connect language\n",
    "        self.fc_word = ModuleDict({\n",
    "            'subj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.word_feature_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(0.1),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "            'pred': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.word_feature_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(0.1),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "            'obj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.word_feature_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(0.1),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "        })\n",
    "        \n",
    "        # Fully connect roi\n",
    "        self.fc_rois = ModuleDict({\n",
    "            'subj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.visual_feature_dim, self.visual_hidden_dim),\n",
    "                #torch.nn.BatchNorm1d(self.visual_hidden_dim),\n",
    "                torch.nn.LeakyReLU(0.1),\n",
    "                torch.nn.Linear(self.visual_hidden_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(0.1),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.trans_feature_dim),\n",
    "            ),\n",
    "            'pred': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.visual_feature_dim + self.spatial_hidden_dim, self.visual_hidden_dim),\n",
    "                #torch.nn.BatchNorm1d(self.visual_hidden_dim),\n",
    "                torch.nn.LeakyReLU(0.1),\n",
    "                torch.nn.Linear(self.visual_hidden_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(0.1),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.trans_feature_dim),\n",
    "            ),\n",
    "            'obj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.visual_feature_dim, self.visual_hidden_dim),\n",
    "                #torch.nn.BatchNorm1d(self.visual_hidden_dim),\n",
    "                torch.nn.LeakyReLU(0.1),\n",
    "                torch.nn.Linear(self.visual_hidden_dim, self.trans_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.trans_feature_dim),\n",
    "                torch.nn.LeakyReLU(0.1),\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.trans_feature_dim),\n",
    "            ),\n",
    "        })\n",
    "        \n",
    "        self.fc_rois2 = ModuleDict({\n",
    "            'subj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.emb_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.emb_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "            'pred': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.emb_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.emb_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "            'obj': torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.trans_feature_dim, self.emb_feature_dim),\n",
    "                #torch.nn.BatchNorm1d(self.emb_feature_dim),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(self.emb_feature_dim, self.emb_feature_dim),\n",
    "            ),\n",
    "        })\n",
    "        \n",
    "        # Triplet Loss (Cosine Distance)\n",
    "        self.triplet_loss = ModuleDict({\n",
    "            'subj': nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y), margin=0.2),\n",
    "            'pred': nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y), margin=0.2),\n",
    "            'obj': nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y), margin=0.2),\n",
    "        })\n",
    "\n",
    "    def _freeze_parameters(self, cfg):\n",
    "        freeze_detectron = True\n",
    "        if freeze_detectron:\n",
    "            for param in self.detectron.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "                    \n",
    "    def _load_detectron_chkpoints(self, cfg):\n",
    "        \"\"\"\n",
    "            Extension of __init__ for modules\n",
    "        \"\"\"\n",
    "        # Load Detectron2 Pre-Trained Weights\n",
    "        if cfg.VRD_RESNETS101_PRETRAINED_WEIGHTS is not None:\n",
    "            DetectionCheckpointer(self.detectron).load(\n",
    "                os.path.join(cfg.OUTPUT_DIR, 'model_final.pth')\n",
    "            )\n",
    "    \n",
    "    def _load_words_chkpoints(self, cfg):\n",
    "        \"\"\"\n",
    "            Extension of __init__ for modules\n",
    "        \"\"\"\n",
    "        return get_triples_features(cfg.DATASETS.TRAIN[0].split('_')[0])\n",
    "\n",
    "    \n",
    "    def _save_words_chkpoints(self, cfg):\n",
    "        torch.save(self.token_embeddings, self.triples_embeddings_path)\n",
    "    \n",
    "    def _get_bert_features(self, triples):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            triples: (Subj, Pred, Obj)\n",
    "        Return:\n",
    "            dict of [CLS, Subj, Pred, Obj, SEP] embeddings\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Load pre-trained model tokenizer (vocabulary)\n",
    "        marked_text = \"[CLS] \" + \" \".join(triples) + \" [SEP]\"\n",
    "        tokenized_text = self.tokenizer.tokenize(marked_text)\n",
    "\n",
    "        # Save the token split to average them later on\n",
    "        token_placements = defaultdict(list)\n",
    "        triples_temp = list(triples)\n",
    "        for i, tok in enumerate(tokenized_text):\n",
    "            stip_tok = tok.replace('#', '')\n",
    "            if (stip_tok in triples_temp[0]):\n",
    "                token_placements['subj'].append(i)\n",
    "                triples_temp[0] = triples_temp[0].replace(stip_tok, '')\n",
    "            elif (stip_tok in triples_temp[1]):\n",
    "                token_placements['pred'].append(i)\n",
    "                triples_temp[1] = triples_temp[1].replace(stip_tok, '')\n",
    "            elif (stip_tok in triples_temp[2]):\n",
    "                token_placements['obj'].append(i)\n",
    "                triples_temp[2] = triples_temp[2].replace(stip_tok, '')\n",
    "            elif (not tok == '[CLS]' and not tok == '[SEP]'):\n",
    "                print(tok, triples)\n",
    "\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)  # one sentence\n",
    "\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')\n",
    "        segments_tensors = torch.tensor([segments_ids]).to('cuda')\n",
    "\n",
    "        # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert_model(tokens_tensor, segments_tensors)\n",
    "\n",
    "            # Evaluating the model will return a different number of objects based on\n",
    "            # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
    "            # becase we set `output_hidden_states = True`, the third item will be the\n",
    "            # hidden states from all layers. See the documentation for more details:\n",
    "            # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "            hidden_states = outputs[2]\n",
    "\n",
    "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "        token_embeddings.size()\n",
    "\n",
    "        # remove dimension 1\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "\n",
    "        # get token embeddings (list of token embeddings)\n",
    "        token_vecs_cat = []\n",
    "        for token in token_embeddings:\n",
    "            cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "            token_vecs_cat.append(cat_vec)\n",
    "        results['CLS'] = token_vecs_cat[0]\n",
    "        results['SEP'] = token_vecs_cat[-1]\n",
    "\n",
    "        # average the token embeddings for word that are splitted to get word embeddings\n",
    "        for key, val in token_placements.items():\n",
    "            results[key] = token_vecs_cat[val[0]]\n",
    "            for i in range(1, len(val)):\n",
    "                results[key] += token_vecs_cat[val[i]]\n",
    "            results[key] = results[key] / len(val)\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def _get_word_features(self, triples):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                triples: ([(subj, pred, obj)]) list of triple\n",
    "            Return:\n",
    "                resulting embeddings for subjs, preds, and objs\n",
    "        \"\"\"\n",
    "        subj_embeddings = []\n",
    "        pred_embeddings = []\n",
    "        obj_embeddings = []\n",
    "        \n",
    "        for subj, pred, obj in triples:\n",
    "            subj_cls_label = self.object_classes[subj]\n",
    "            pred_cls_label = self.predicate_classes[pred]\n",
    "            obj_cls_label = self.object_classes[obj]\n",
    "            triples_text = '-'.join((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "            query_embeddings = None\n",
    "            if (triples_text not in token_embeddings):\n",
    "                query_embeddings = self._get_bert_features((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "                token_embeddings[triples_text] = query_embeddings\n",
    "            else:\n",
    "                query_embeddings = token_embeddings[triples_text]\n",
    "            subj_embeddings.append(query_embeddings['subj'].to('cuda'))\n",
    "            pred_embeddings.append(query_embeddings['pred'].to('cuda'))\n",
    "            obj_embeddings.append(query_embeddings['obj'].to('cuda'))\n",
    "        \n",
    "        subj_embeddings = torch.stack(subj_embeddings).to('cuda')\n",
    "        pred_embeddings = torch.stack(pred_embeddings).to('cuda')\n",
    "        obj_embeddings = torch.stack(obj_embeddings).to('cuda')\n",
    "        \n",
    "        return subj_embeddings, pred_embeddings, obj_embeddings\n",
    "    \n",
    "    def _get_word_predicate_features(self, subj, obj):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                subj: subject index\n",
    "                obj: object index\n",
    "            Output:\n",
    "                A stack pred_embeddings for the above two subj and obj\n",
    "        \"\"\"\n",
    "        subj_cls_label = self.object_classes[subj]\n",
    "        obj_cls_label = self.object_classes[obj]\n",
    "\n",
    "        #predicate label\n",
    "        pred_embeddings = []\n",
    "        subj_embeddings = []\n",
    "        obj_embeddings = []\n",
    "        for pred, pred_cls_label in enumerate(self.predicate_classes):\n",
    "            triples_text = '-'.join((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "            query_embeddings = None\n",
    "            if (triples_text not in token_embeddings):\n",
    "                query_embeddings = self._get_bert_features((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "                token_embeddings[triples_text] = query_embeddings\n",
    "            else:\n",
    "                query_embeddings = token_embeddings[triples_text]\n",
    "            subj_embeddings.append(query_embeddings['subj'].to('cuda'))\n",
    "            pred_embeddings.append(query_embeddings['pred'].to('cuda'))\n",
    "            obj_embeddings.append(query_embeddings['obj'].to('cuda'))\n",
    "            \n",
    "        subj_embeddings = torch.stack(subj_embeddings).to('cuda')\n",
    "        pred_embeddings = torch.stack(pred_embeddings).to('cuda')\n",
    "        obj_embeddings = torch.stack(obj_embeddings).to('cuda')\n",
    "\n",
    "        return subj_embeddings, pred_embeddings, obj_embeddings\n",
    "    \n",
    "    def get_instances_prediction(self, data):\n",
    "        \"\"\"\n",
    "            This function gets the predicted instances from the object detector, \n",
    "            and is only relevant to relationship detection evaluation.\n",
    "            \n",
    "            Input:\n",
    "                data - data format for detectron2\n",
    "            Output:\n",
    "                instances - Instances object by the detectron2 that contain predicted instances\n",
    "        \"\"\"\n",
    "        instances = []\n",
    "        \n",
    "        images =  torch.unsqueeze(data['image'], axis=0).cuda().float()\n",
    "        images_list = ImageList(images, [(800, 800)])\n",
    "        self.detectron.eval()\n",
    "        features = self.detectron.backbone(images)\n",
    "        proposals, _ = self.detectron.proposal_generator(images_list, features)\n",
    "        instances, _ = self.detectron.roi_heads(images, features, proposals)\n",
    "        \n",
    "        return instances\n",
    "    \n",
    "    def enumerate_relationships_from_instances(self, instances):\n",
    "        \"\"\"\n",
    "            This function convert the format of instancesinto all possible combinations\n",
    "            of relationships in the detectron2 format\n",
    "            \n",
    "            Input:\n",
    "                instances - Instances object by detectron2\n",
    "            Output:\n",
    "                relationships - a dictionary of relationships in the vrdtransr input format\n",
    "        \"\"\"\n",
    "        enumerated_relationships = {\n",
    "            'subj_bboxes': [],\n",
    "            'obj_bboxes': [],\n",
    "            'union_bboxes': [],\n",
    "            'subj_classes': [],\n",
    "            'obj_classes': [],\n",
    "            'subj_scores': [],\n",
    "            'obj_scores': []\n",
    "        }\n",
    "        \n",
    "        # TO DO: implement to support batch images instead of a single image\n",
    "        instance_dict = instances[0].get_fields()\n",
    "        \n",
    "        pred_bboxes = instance_dict['pred_boxes']\n",
    "        pred_cls = instance_dict['pred_classes']\n",
    "        pred_scores = instance_dict['scores']\n",
    "        \n",
    "        for i in range(len(pred_cls)):\n",
    "            for j in range(i + 1, len(pred_cls)):\n",
    "                obj1_bbox = pred_bboxes[i].tensor.cpu().detach().numpy().astype(int)\n",
    "                obj2_bbox = pred_bboxes[j].tensor.cpu().detach().numpy().astype(int)\n",
    "                union_bbox = boxes_union(copy.deepcopy(obj1_bbox), copy.deepcopy(obj2_bbox))[0]\n",
    "                \n",
    "                obj1_label = pred_cls[i]\n",
    "                obj2_label = pred_cls[j]\n",
    "                \n",
    "                obj1_score = pred_scores[i]\n",
    "                obj2_score = pred_scores[j]\n",
    "                \n",
    "                enumerated_relationships['subj_bboxes'].append(obj1_bbox)\n",
    "                enumerated_relationships['obj_bboxes'].append(obj2_bbox)\n",
    "                enumerated_relationships['union_bboxes'].append(union_bbox)\n",
    "                enumerated_relationships['subj_classes'].append(obj1_label)\n",
    "                enumerated_relationships['obj_classes'].append(obj2_label)\n",
    "                enumerated_relationships['subj_scores'].append(obj1_score)\n",
    "                enumerated_relationships['obj_scores'].append(obj2_score)\n",
    "                \n",
    "                \n",
    "                enumerated_relationships['subj_bboxes'].append(obj2_bbox)\n",
    "                enumerated_relationships['obj_bboxes'].append(obj1_bbox)\n",
    "                enumerated_relationships['union_bboxes'].append(union_bbox)\n",
    "                enumerated_relationships['subj_classes'].append(obj2_label)\n",
    "                enumerated_relationships['obj_classes'].append(obj1_label)\n",
    "                enumerated_relationships['subj_scores'].append(obj2_score)\n",
    "                enumerated_relationships['obj_scores'].append(obj1_score)\n",
    "                \n",
    "        return enumerated_relationships\n",
    "    \n",
    "    def get_predicted_relationships(self, data):\n",
    "        \"\"\"\n",
    "            This function gets the predicted relationships from the object detector, \n",
    "            and is only relevant to relationship detection evaluation.\n",
    "            \n",
    "            Input:\n",
    "                data - data format for detectron2\n",
    "            Output:\n",
    "                relationships - Instances object by the detectron2 that contain predicted instances\n",
    "        \"\"\"\n",
    "        #get predicted objects in the given image\n",
    "        instances = self.get_instances_prediction(data)\n",
    "        \n",
    "        #enumerate the relationships with the predicted instances (bounding boxes and labels)\n",
    "        relationships = self.enumerate_relationships_from_instances(instances)\n",
    "        \n",
    "        return relationships\n",
    "\n",
    "    \n",
    "    def get_predicate_distances(self, data, is_rel_eval=False):\n",
    "        \"\"\"\n",
    "            Predict model's prediction based on the given data.\n",
    "            Return the prediction predicate, visual relationship, and phrase (to be implemented)\n",
    "            Input:\n",
    "                data: vrdtranse input format\n",
    "                is_rel_eval: boolean whether the evaluation is predicate detection or relationship detection\n",
    "        \"\"\"\n",
    "        relationships = data[\"relationships\"].copy()\n",
    "        all_predicate_distances = [] # for each (subj, obj) pair, we get a set of distances\n",
    "        all_subject_distances = [] #distance between subject visual and language\n",
    "        all_object_distances = []\n",
    "        all_predicate_subtract_distances = []\n",
    "        all_transe_visual_feature = []\n",
    "        all_transe_language_feature = []\n",
    "        all_transe_distance_feature = []\n",
    "        \n",
    "        #forward features for gt_visual and gt_text\n",
    "        fc_features = self.forward(data, None, get_fc_features=True)\n",
    "        \n",
    "        rel_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for subj, obj in zip(relationships['subj_classes'], relationships['obj_classes']):\n",
    "                #compute all potential predicate embeddings for the (subj, obj) pair\n",
    "                all_subj_embeddings, all_predicate_embeddings, all_object_embeddings = self._get_word_predicate_features(subj=subj, obj=obj)\n",
    "\n",
    "                #languge features\n",
    "                fc_all_subject_embeddings = self.fc_word[\"subj\"](all_subj_embeddings)\n",
    "                fc_all_predicate_embeddings = self.fc_word[\"pred\"](all_predicate_embeddings)\n",
    "                fc_all_object_embeddings = self.fc_word[\"obj\"](all_object_embeddings)\n",
    "                fc_pred_transe_language_feature = fc_all_subject_embeddings + fc_all_predicate_embeddings - fc_all_object_embeddings\n",
    "\n",
    "                #visual features\n",
    "                fc_pred_visual_feature = fc_features[\"visual\"][\"pred\"][rel_cnt,:]\n",
    "                fc_pred_subtract_visual_feature = fc_features[\"visual\"][\"obj\"][rel_cnt,:] - fc_features[\"visual\"][\"subj\"][rel_cnt,:]\n",
    "                fc_pred_transe_visual_feature = fc_features[\"visual\"][\"subj\"][rel_cnt,:] + fc_features[\"visual\"][\"pred\"][rel_cnt,:] - fc_features[\"visual\"][\"obj\"][rel_cnt,:]\n",
    "                fc_subj_visual_feature = fc_features[\"visual\"][\"subj\"][rel_cnt,:]\n",
    "                fc_obj_visual_feature = fc_features[\"visual\"][\"obj\"][rel_cnt,:]\n",
    "                \n",
    "                #compute distance between the fc_features[\"visual\"][\"pred\"] and fc_predicate_embeddings to get top n\n",
    "                pdist = lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    "                distance = []\n",
    "                distance_subject = []\n",
    "                distance_object = []\n",
    "                distance_subtract = []\n",
    "                distance_transe = []\n",
    "                \n",
    "                for subj_emb, pred_emb, obj_emb, pred_transe_emb in zip(fc_all_subject_embeddings, fc_all_predicate_embeddings, fc_all_object_embeddings, fc_pred_transe_language_feature):\n",
    "                    distance.append(pdist(torch.unsqueeze(fc_pred_visual_feature, dim=0), torch.unsqueeze(pred_emb, dim=0)))\n",
    "                    distance_subtract.append(pdist(torch.unsqueeze(fc_pred_subtract_visual_feature, dim=0), torch.unsqueeze(obj_emb - subj_emb, dim=0)))\n",
    "                    distance_transe.append(pdist(torch.unsqueeze(fc_pred_transe_visual_feature, dim=0), torch.unsqueeze(pred_transe_emb, dim=0)))\n",
    "                    distance_subject.append(pdist(torch.unsqueeze(fc_subj_visual_feature, dim=0), torch.unsqueeze(subj_emb, dim=0)))\n",
    "                    distance_object.append(pdist(torch.unsqueeze(fc_obj_visual_feature, dim=0), torch.unsqueeze(obj_emb, dim=0)))\n",
    "                     \n",
    "                    \n",
    "                # add set of distances to the given relationship\n",
    "                all_predicate_distances.append(distance)\n",
    "                all_subject_distances.append(distance_subject)\n",
    "                all_object_distances.append(distance_object)\n",
    "                all_predicate_subtract_distances.append(distance_subtract)\n",
    "                all_transe_visual_feature.append(fc_pred_transe_visual_feature)\n",
    "                all_transe_language_feature.append(fc_pred_transe_language_feature)\n",
    "                all_transe_distance_feature.append(distance_transe)\n",
    "                rel_cnt += 1\n",
    "        \n",
    "        if (is_rel_eval):\n",
    "            return all_predicate_distances, all_subject_distances, all_object_distances, all_predicate_subtract_distances, all_transe_distance_feature\n",
    "                \n",
    "        return all_predicate_distances, all_predicate_subtract_distances, all_transe_distance_feature\n",
    "    \n",
    "    def get_triples_distance(self, data):\n",
    "        \"\"\"\n",
    "            POINTER 3 [currently redundant]\n",
    "            Predicts the distance between subjects, predicates, objects, and subtracted distance between subject and object\n",
    "            \n",
    "            Only relevant for visual relationship detection task (evaluation)\n",
    "        \"\"\"\n",
    "        relationships = data[\"relationships\"].copy()\n",
    "        all_predicate_distances = []\n",
    "        all_subject_distances = []\n",
    "        all_object_distances = []\n",
    "        all_predicate_subtract_distances = []\n",
    "        \n",
    "        rel_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for subj, obj in zip(relationships['subj_classes'], relationships['obj_classes']):\n",
    "                #compute all potential predicate embeddings for the (subj, obj) pair\n",
    "                all_subj_embeddings, all_predicate_embeddings, all_object_embeddings = self._get_word_predicate_features(subj=subj, obj=obj)\n",
    "\n",
    "                #languge features\n",
    "                fc_all_subject_embeddings = self.fc_word[\"subj\"](all_subj_embeddings)\n",
    "                fc_all_predicate_embeddings = self.fc_word[\"pred\"](all_predicate_embeddings)\n",
    "                fc_all_object_embeddings = self.fc_word[\"obj\"](all_object_embeddings)\n",
    "                fc_pred_transe_language_feature = fc_all_subject_embeddings - fc_all_object_embeddings\n",
    "\n",
    "                #visual features\n",
    "                fc_pred_visual_feature = fc_features[\"visual\"][\"pred\"][rel_cnt,:]\n",
    "                fc_pred_subtract_visual_feature = fc_features[\"visual\"][\"obj\"][rel_cnt,:] - fc_features[\"visual\"][\"subj\"][rel_cnt,:]\n",
    "                fc_pred_transe_visual_feature = fc_features[\"visual\"][\"subj\"][rel_cnt,:] + fc_features[\"visual\"][\"pred\"][rel_cnt,:] - fc_features[\"visual\"][\"obj\"][rel_cnt,:]\n",
    "                \n",
    "                #compute distance between the fc_features[\"visual\"][\"pred\"] and fc_predicate_embeddings to get top n\n",
    "                pdist = lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    "                distance = []\n",
    "                distance_subtract = []\n",
    "                distance_transe = []\n",
    "                \n",
    "                for subj_emb, pred_emb, obj_emb, pred_transe_emb in zip(fc_all_subject_embeddings, fc_all_predicate_embeddings, fc_all_object_embeddings, fc_pred_transe_language_feature):\n",
    "                    distance.append(pdist(torch.unsqueeze(fc_pred_visual_feature, dim=0), torch.unsqueeze(pred_emb, dim=0)))\n",
    "                    distance_subtract.append(pdist(torch.unsqueeze(fc_pred_subtract_visual_feature, dim=0), torch.unsqueeze(obj_emb - subj_emb, dim=0)))\n",
    "                    distance_transe.append(pdist(torch.unsqueeze(fc_pred_transe_visual_feature, dim=0), torch.unsqueeze(pred_transe_emb, dim=0)))\n",
    "\n",
    "                # add set of distances to the given relationship\n",
    "                all_predicate_distances.append(distance)\n",
    "                all_predicate_subtract_distances.append(distance_subtract)\n",
    "                all_transe_visual_feature.append(fc_pred_transe_visual_feature)\n",
    "                all_transe_language_feature.append(fc_pred_transe_language_feature)\n",
    "                all_transe_distance_feature.append(distance_transe)\n",
    "                rel_cnt += 1\n",
    "        \n",
    "        fc_features = self.forward(data, None, get_fc_features=True)\n",
    "\n",
    "        \n",
    "        return all_predicate_distances, all_subject_distances, all_object_distances, all_predicate_subtract_distances\n",
    "    \n",
    "    def _get_word_object_features(self, subj, pred):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                subj: subject index\n",
    "                pred: predicate index\n",
    "            Output:\n",
    "                A stack pred_embeddings for the above two subj and obj\n",
    "        \"\"\"\n",
    "        subj_cls_label = self.object_classes[subj]\n",
    "        pred_cls_label = self.predicate_classes[pred]\n",
    "\n",
    "        #predicate label\n",
    "        pred_embeddings = []\n",
    "        subj_embeddings = []\n",
    "        obj_embeddings = []\n",
    "        for obj, obj_cls_label in enumerate(self.object_classes):\n",
    "            triples_text = '-'.join((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "            query_embeddings = None\n",
    "            if (triples_text not in token_embeddings):\n",
    "                query_embeddings = self._get_bert_features((subj_cls_label, pred_cls_label, obj_cls_label))\n",
    "                token_embeddings[triples_text] = query_embeddings\n",
    "            else:\n",
    "                query_embeddings = token_embeddings[triples_text]\n",
    "            subj_embeddings.append(query_embeddings['subj'].to('cuda'))\n",
    "            pred_embeddings.append(query_embeddings['pred'].to('cuda'))\n",
    "            obj_embeddings.append(query_embeddings['obj'].to('cuda'))\n",
    "            \n",
    "        subj_embeddings = torch.stack(subj_embeddings).to('cuda')\n",
    "        pred_embeddings = torch.stack(pred_embeddings).to('cuda')\n",
    "        obj_embeddings = torch.stack(obj_embeddings).to('cuda')\n",
    "\n",
    "        return subj_embeddings, pred_embeddings, obj_embeddings\n",
    "    \n",
    "    def get_object_distances(self, data):\n",
    "        \"\"\"\n",
    "            Predict model's prediction based on the given data.\n",
    "            Return the prediction predicate, phrase (to be implemented), and visual relationship (to be implemented)\n",
    "        \"\"\"\n",
    "        relationships = data[\"relationships\"]\n",
    "        all_object_distances = [] # for each (subj, pred) pair, we get a set of distances\n",
    "        all_object_add_distances = [] \n",
    "\n",
    "        #forward features for gt_visual and gt_text\n",
    "        fc_features = self.forward(data, None, get_fc_features=True, obfuscate_object=True)\n",
    "\n",
    "        rel_cnt = 0\n",
    "        with torch.no_grad():\n",
    "            for subj, pred in zip(relationships['subj_classes'], relationships['pred_classes']):\n",
    "                #compute all potential predicate embeddings for the (subj, obj) pair\n",
    "                all_subj_embeddings, all_predicate_embeddings, all_object_embeddings = self._get_word_object_features(subj=subj, pred=pred)\n",
    "\n",
    "                #languge features\n",
    "                fc_all_subject_embeddings = self.fc_word[\"subj\"](all_subj_embeddings)\n",
    "                fc_all_predicate_embeddings = self.fc_word[\"pred\"](all_predicate_embeddings)\n",
    "                fc_all_object_embeddings = self.fc_word[\"obj\"](all_object_embeddings)\n",
    "\n",
    "                #visual features\n",
    "                fc_obj_add_visual_feature = fc_features[\"visual\"][\"subj\"][rel_cnt,:] + fc_features[\"visual\"][\"pred\"][rel_cnt,:]\n",
    "\n",
    "                #compute distance between the fc_features[\"visual\"][\"pred\"] and fc_predicate_embeddings to get top n\n",
    "                pdist = lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    "                distance_add = []\n",
    "                distance = []\n",
    "                \n",
    "                for subj_emb, pred_emb, obj_emb in zip(fc_all_subject_embeddings, fc_all_predicate_embeddings, fc_all_object_embeddings):\n",
    "                    distance.append(pdist(torch.unsqueeze(fc_obj_add_visual_feature, dim=0), torch.unsqueeze(obj_emb, dim=0)))\n",
    "                    distance_add.append(pdist(torch.unsqueeze(fc_obj_add_visual_feature, dim=0), torch.unsqueeze(subj_emb + pred_emb, dim=0)))\n",
    "                    \n",
    "                # add set of distances to the given relationship\n",
    "                all_object_distances.append(distance)\n",
    "                all_object_add_distances.append(distance_add)\n",
    "                rel_cnt += 1\n",
    "\n",
    "        return all_object_distances, all_object_add_distances                \n",
    "    \n",
    "    def _get_prediced_bboxes(self, data):\n",
    "        \n",
    "        data['height'] = 800\n",
    "        data['width'] = 800\n",
    "\n",
    "        self.detectron.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.detectron([data])\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def _get_roi_features(self, images, box_lists):\n",
    "        \"\"\"\n",
    "            Get image features from the backbone network\n",
    "            Input:\n",
    "                images: (ImageList.from_tensors) with dimension (C,W,H)\n",
    "                box_lists: A list of N boxes\n",
    "            Return:\n",
    "                features:[N, 7*7*256]\n",
    "        \"\"\"\n",
    "        N = len(box_lists[0])\n",
    "        \n",
    "        cfg = self.cfg\n",
    "        feature_maps = self.backbone(images)\n",
    "        feature_maps = [feature_maps['p{}'.format(i)] for i in range(2,6)]\n",
    "        regions_feature = self.pooler(feature_maps, box_lists)\n",
    "        return regions_feature.reshape((N, self.visual_feature_dim))\n",
    "\n",
    "    def get_unrelated_instance(self, bbox, cls, gt_tuple_boxes, gt_classes, memo, other_memo=None):\n",
    "        negative_example = {}\n",
    "\n",
    "        tuple_bbox = tuple(bbox)\n",
    "        for i, neg_bbox in enumerate(gt_tuple_boxes):\n",
    "            if (other_memo is not None and neg_bbox not in other_memo and neg_bbox not in memo):\n",
    "                negative_example = {\n",
    "                    'bbox': torch.from_numpy(np.asarray(neg_bbox)).float().cuda(), #convert to tensor float\n",
    "                    'cls': gt_classes[i]\n",
    "                }\n",
    "                return negative_example\n",
    "            elif (neg_bbox != tuple_bbox and neg_bbox not in memo):\n",
    "                negative_example = {\n",
    "                    'bbox': torch.from_numpy(np.asarray(neg_bbox)).float().cuda(), #convert to tensor float\n",
    "                    'cls': gt_classes[i]\n",
    "                }\n",
    "                return negative_example\n",
    "\n",
    "        return negative_example\n",
    "    \n",
    "    def generate_negative_examples(self, data, K=3):\n",
    "        \"\"\"\n",
    "            for each triple relation in data, generate K negative examples\n",
    "\n",
    "            return: [{\n",
    "                'subj_bboxes': Boxes(tensor[[X,Y,X2,Y2],...])),\n",
    "                'union_bboxes': Boxes(tensor(([[X,Y,X2,Y2],...])),\n",
    "                'obj_bboxes': Boxes(tensor([[X,Y,X2,Y2],...])),\n",
    "                'subj_classes': [cls_subj,...],\n",
    "                'pred_classes': [cls_pred,...],\n",
    "                'obj_classes': [cls_obj,...]\n",
    "            }]\n",
    "        \"\"\"\n",
    "        boxes = data['instances'].get_fields()['gt_boxes']\n",
    "        gt_tuple_boxes = [tuple([ele.item() for ele in box]) for box in boxes] #convert ground truth boxes into tuples\n",
    "\n",
    "        classes = data['instances'].get_fields()['gt_classes']\n",
    "        gt_classes = [int(item) for item in classes]\n",
    "\n",
    "        # shuffle to random select first K\n",
    "        zip_gt_data = list(zip(gt_tuple_boxes, gt_classes))\n",
    "        random.shuffle(zip_gt_data)\n",
    "        gt_tuple_boxes, gt_classes = zip(*zip_gt_data)\n",
    "        \n",
    "        relationships = data['relationships']\n",
    "        subj_boxes = relationships['subj_bboxes']\n",
    "        union_boxes = relationships['union_bboxes']\n",
    "        obj_boxes = relationships['obj_bboxes']\n",
    "        subj_classes = relationships['subj_classes']\n",
    "        pred_classes = relationships['pred_classes']\n",
    "        obj_classes = relationships['obj_classes']\n",
    "        \n",
    "        #generate K negative examples\n",
    "        neg_examples = []\n",
    "        memo_subj = set()\n",
    "        memo_obj = set()\n",
    "        existed_predicates = dict(zip([tuple(item) for item in data['relationships']['union_bboxes']], data['relationships']['pred_classes']))\n",
    "        \n",
    "        # negative within the image\n",
    "        for i in range(min(len(gt_tuple_boxes)-1, K)):\n",
    "            neg_ex = defaultdict(list)\n",
    "            if (len(memo_subj) == len(gt_tuple_boxes)-1 or len(memo_obj) == len(gt_tuple_boxes)-1):\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                for j in range(len(subj_boxes)): #iterate through the relationships in the image\n",
    "                    #subj\n",
    "                    subj_box = subj_boxes[j]\n",
    "                    subj_cls = subj_classes[j]\n",
    "\n",
    "                    #ISSUE: in the case where the number of object in the image is actually smaller than K, it is kind useless\n",
    "                    unrelated_subj_instance = self.get_unrelated_instance(subj_box[0], subj_cls, gt_tuple_boxes, gt_classes, memo=memo_subj)\n",
    "                    neg_ex['subj_bboxes'].append(unrelated_subj_instance['bbox'])\n",
    "                    neg_ex['subj_classes'].append(unrelated_subj_instance['cls'])\n",
    "\n",
    "                    #obj\n",
    "                    obj_box = obj_boxes[j]\n",
    "                    obj_cls = obj_classes[j]\n",
    "                    other_memo = set()\n",
    "                    other_memo.add(tuple(subj_box[0]))\n",
    "                    unrelated_obj_instance = self.get_unrelated_instance(obj_box[0], obj_cls, gt_tuple_boxes, gt_classes, memo=memo_obj, other_memo=other_memo)\n",
    "                    neg_ex['obj_bboxes'].append(unrelated_obj_instance['bbox'])\n",
    "                    neg_ex['obj_classes'].append(unrelated_obj_instance['cls'])\n",
    "                    \n",
    "                    #pred\n",
    "                    new_union_box = boxes_union(copy.deepcopy(unrelated_subj_instance['bbox'].reshape(1,4).to('cpu')),\n",
    "                                                copy.deepcopy(unrelated_obj_instance['bbox'].reshape(1,4).to('cpu')))[0]\n",
    "                    new_predicate_class = len(self.predicate_classes) - 1\n",
    "                    if (tuple(new_union_box) in existed_predicates):\n",
    "                        new_predicate_class = existed_predicates[tuple(new_union_box)]\n",
    "                    neg_ex['union_bboxes'].append(torch.from_numpy(np.asarray(new_union_box)).float().cuda())\n",
    "                    neg_ex['pred_classes'].append(new_predicate_class)\n",
    "\n",
    "                for j in range(len(subj_boxes)):\n",
    "                    memo_subj.add(tuple(neg_ex['subj_bboxes'][j].tolist()))\n",
    "                    memo_obj.add(tuple(neg_ex['obj_bboxes'][j].tolist()))\n",
    "            except:\n",
    "                break\n",
    "\n",
    "            #stack the bounding boxes\n",
    "            neg_ex['subj_bboxes'] = Boxes(torch.stack(neg_ex['subj_bboxes']))\n",
    "            neg_ex['obj_bboxes'] = Boxes(torch.stack(neg_ex['obj_bboxes']))\n",
    "            neg_ex['union_bboxes'] = Boxes(torch.stack(neg_ex['union_bboxes']))\n",
    "            \n",
    "            #append to memory\n",
    "            neg_examples.append(neg_ex)\n",
    "            \n",
    "        return neg_examples\n",
    "\n",
    "    def get_spatial_features(self, relationships, is_negative=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: see data definition in forward function\n",
    "        Return:\n",
    "            spatial_features: a tensor of spatial features containing coordinates of the bounding box\n",
    "        \"\"\"\n",
    "        def spatial_delta(entity1, entity2):\n",
    "            \"\"\"\n",
    "                entity1, entity2: [X,Y,X,Y]\n",
    "            \"\"\"\n",
    "            \n",
    "            width1, height1 = entity1[2] - entity1[0], entity1[3] - entity1[1]\n",
    "            width2, height2 = entity2[2] - entity2[0], entity2[3] - entity2[1]\n",
    "            \n",
    "            delta_feat = [\n",
    "                (entity1[0] - entity2[0])/width2,\n",
    "                (entity1[1] - entity2[1])/height2,\n",
    "                np.log(width1/width2),\n",
    "                np.log(height1/height2),\n",
    "            ]\n",
    "            return delta_feat\n",
    "        \n",
    "        def spatial_coordinates(entity):\n",
    "            \"\"\"\n",
    "                entity: [X,Y,X,Y]\n",
    "            \"\"\"\n",
    "            width, height = entity[2] - entity[0], entity[3] - entity[1]\n",
    "            coordinate_feat = [\n",
    "                entity[0]/800,\n",
    "                entity[1]/800,\n",
    "                entity[2]/800,\n",
    "                entity[3]/800,\n",
    "                width*height/800*800\n",
    "            ]\n",
    "            return coordinate_feat\n",
    "        \n",
    "        spatial_features = []\n",
    "        #iterate through every relationship pair and construct an array of spatial feature    \n",
    "        for subj_bbox, obj_bbox, union_bbox in zip(relationships['subj_bboxes'], relationships['obj_bboxes'], relationships['union_bboxes']):\n",
    "            feat = []\n",
    "            #XYXY\n",
    "            if (is_negative):\n",
    "                feat.extend(spatial_delta(subj_bbox.cpu(), obj_bbox.cpu()))\n",
    "                feat.extend(spatial_delta(subj_bbox.cpu(), union_bbox.cpu()))\n",
    "                feat.extend(spatial_delta(union_bbox.cpu(), obj_bbox.cpu()))\n",
    "                feat.extend(spatial_coordinates(subj_bbox.cpu()))\n",
    "                feat.extend(spatial_coordinates(obj_bbox.cpu()))            \n",
    "            else:\n",
    "                feat.extend(spatial_delta(subj_bbox[0], obj_bbox[0]))\n",
    "                feat.extend(spatial_delta(subj_bbox[0], union_bbox))\n",
    "                feat.extend(spatial_delta(union_bbox, obj_bbox[0]))\n",
    "                feat.extend(spatial_coordinates(subj_bbox[0]))\n",
    "                feat.extend(spatial_coordinates(obj_bbox[0]))\n",
    "            \n",
    "            spatial_features.append(torch.from_numpy(np.asarray(feat)).float().cuda())\n",
    "        \n",
    "        return torch.stack(spatial_features)\n",
    "    \n",
    "    def eval_phrase_detection(self, data):\n",
    "        #add batch dim to image\n",
    "        image = torch.unsqueeze(data['image'], axis=0).cuda()\n",
    "        \n",
    "        #get prediction bounding boxes\n",
    "        output = self._get_prediced_bboxes(data)\n",
    "        bboxes = output[0]['instances'].get_fields()['pred_boxes']\n",
    "        bboxes_features = self._get_roi_features(image.float(), box_lists=[bboxes])\n",
    "        \n",
    "        bboxes_classes = output[0]['instances'].get_fields()['pred_classes']\n",
    "        conf_score = output[0]['instances'].get_fields()['scores']\n",
    "        #iterate through every pairs and compute score\n",
    "        return output, bboxes_features\n",
    "    \n",
    "    def forward(self, data, negative_examples, get_fc_features=False, obfuscate_object=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: {\n",
    "                    #Detectron\n",
    "                    'file_name': os.path.join(self.images_dir, img_name),\n",
    "                    'image_id': int(img_name.split('.')[0]),\n",
    "                    'annotations': list(unique_objects.values()),\n",
    "\n",
    "                    #Relationships\n",
    "                    'relationships': {\n",
    "                        'subj_bboxes': subj_bboxes,\n",
    "                        'obj_bboxes': obj_bboxes,\n",
    "                        'union_bboxes': union_bboxes,\n",
    "\n",
    "                        'subj_classes': subj_classes,\n",
    "                        'pred_classes': pred_classes,\n",
    "                        'obj_classes': obj_classes,\n",
    "                    }\n",
    "                }\n",
    "            context: #any external data (not implemented as of current)\n",
    "        \"\"\"\n",
    "        image = torch.unsqueeze(data['image'], axis=0).cuda()\n",
    "\n",
    "        relationships = data['relationships']\n",
    "        \n",
    "        subj_bboxes = Boxes([bbox[0] for bbox in relationships['subj_bboxes']]).to('cuda')\n",
    "        if (obfuscate_object):\n",
    "            union_bboxes = subj_bboxes\n",
    "        else:\n",
    "            union_bboxes = Boxes([bbox for bbox in relationships['union_bboxes']]).to('cuda')\n",
    "        obj_bboxes = Boxes([bbox[0] for bbox in relationships['obj_bboxes']]).to('cuda')\n",
    "\n",
    "        \n",
    "        #ground_truth features\n",
    "        gt_features = {\"visual\": {}, \"textual\": {}}\n",
    "\n",
    "        #fully connected features\n",
    "        fc_features = {\"visual\": {}, \"textual\": {}}\n",
    "\n",
    "\n",
    "        #spatial features\n",
    "        gt_spatial_features = self.fc_spatial(self.get_spatial_features(relationships))\n",
    "        \n",
    "        #extract visual features from backbone and ROIPool for n relations in the image\n",
    "        gt_features[\"visual\"][\"subj\"] = self._get_roi_features(image.float(), box_lists=[subj_bboxes])\n",
    "        gt_features[\"visual\"][\"pred\"] = self._get_roi_features(image.float(), box_lists=[union_bboxes])\n",
    "        gt_features[\"visual\"][\"pred\"] = torch.cat((gt_features[\"visual\"][\"pred\"], gt_spatial_features), 1)\n",
    "        gt_features[\"visual\"][\"obj\"] = self._get_roi_features(image.float(), box_lists=[obj_bboxes])\n",
    "        \n",
    "        #fc visual (rois1 and rois2)\n",
    "        fc_features[\"visual\"][\"subj\"] = self.fc_rois[\"subj\"](gt_features[\"visual\"][\"subj\"])\n",
    "        fc_features[\"visual\"][\"pred\"] = self.fc_rois[\"pred\"](gt_features[\"visual\"][\"pred\"])\n",
    "        fc_features[\"visual\"][\"obj\"] = self.fc_rois[\"obj\"](gt_features[\"visual\"][\"obj\"])\n",
    "\n",
    "        fc_features[\"visual\"][\"subj\"] = self.fc_rois2[\"subj\"](fc_features[\"visual\"][\"subj\"])\n",
    "        fc_features[\"visual\"][\"pred\"] = self.fc_rois2[\"pred\"](fc_features[\"visual\"][\"pred\"])\n",
    "        fc_features[\"visual\"][\"obj\"] = self.fc_rois2[\"obj\"](fc_features[\"visual\"][\"obj\"])\n",
    "\n",
    "        \n",
    "        if (get_fc_features):\n",
    "            return fc_features\n",
    "    \n",
    "        #extract word embeddings for n examples in the image\n",
    "        word_embeddings = self._get_word_features(list(zip(relationships['subj_classes'],\n",
    "                                                           relationships['pred_classes'],\n",
    "                                                           relationships['obj_classes'])))\n",
    "        gt_features[\"textual\"][\"subj\"] = word_embeddings[0]\n",
    "        gt_features[\"textual\"][\"pred\"] = word_embeddings[1]\n",
    "        gt_features[\"textual\"][\"obj\"] = word_embeddings[2]\n",
    "\n",
    "\n",
    "        #fc word\n",
    "        fc_features[\"textual\"][\"subj\"] = self.fc_word[\"subj\"](gt_features[\"textual\"][\"subj\"])\n",
    "        fc_features[\"textual\"][\"pred\"] = self.fc_word[\"pred\"](gt_features[\"textual\"][\"pred\"])\n",
    "        fc_features[\"textual\"][\"obj\"] = self.fc_word[\"obj\"](gt_features[\"textual\"][\"obj\"])\n",
    "\n",
    "        #Visual and Language Consistency losses triplet_loss(anchor, positive, negative)\n",
    "        triplet_losses = {\"subj\": None, \"pred\": None, \"obj\": None} #, \"transr\": None}\n",
    "        \n",
    "        #NEGATIVE EXAMPLES\n",
    "        for neg_relationships in negative_examples:\n",
    "\n",
    "            neg_subj_boxes = neg_relationships['subj_bboxes']\n",
    "            neg_union_boxes = neg_relationships['union_bboxes']\n",
    "            neg_obj_boxes = neg_relationships['obj_bboxes']\n",
    "\n",
    "            #dictionary to store gt_features\n",
    "            neg_features = {\"visual\": {}, \"textual\": {}}\n",
    "\n",
    "            #spatial features\n",
    "            neg_spatial_features = self.fc_spatial(self.get_spatial_features(neg_relationships, is_negative=True))\n",
    "\n",
    "            #extract visual features from backbone and ROIPool for n relations in the image\n",
    "            neg_features[\"visual\"][\"subj\"] = self._get_roi_features(image.float(), box_lists=[neg_subj_boxes])\n",
    "            neg_features[\"visual\"][\"pred\"] = self._get_roi_features(image.float(), box_lists=[neg_union_boxes])\n",
    "            neg_features[\"visual\"][\"pred\"] = torch.cat((neg_features[\"visual\"][\"pred\"], neg_spatial_features), 1)\n",
    "            neg_features[\"visual\"][\"obj\"] = self._get_roi_features(image.float(), box_lists=[neg_obj_boxes])\n",
    "\n",
    "            #extract word embeddings for n examples in the image\n",
    "            neg_word_embeddings = self._get_word_features(list(zip(neg_relationships['subj_classes'],\n",
    "                                                               neg_relationships['pred_classes'],\n",
    "                                                               neg_relationships['obj_classes'])))\n",
    "            neg_features[\"textual\"][\"subj\"] = neg_word_embeddings[0]\n",
    "            neg_features[\"textual\"][\"pred\"] = neg_word_embeddings[1]\n",
    "            neg_features[\"textual\"][\"obj\"] = neg_word_embeddings[2]\n",
    "\n",
    "            #neg fc visual\n",
    "            neg_fc_features = {\"visual\": {}, \"textual\": {}}\n",
    "            neg_fc_features[\"visual\"][\"subj\"] = self.fc_rois[\"subj\"](neg_features[\"visual\"][\"subj\"])\n",
    "            neg_fc_features[\"visual\"][\"pred\"] = self.fc_rois[\"pred\"](neg_features[\"visual\"][\"pred\"])\n",
    "            neg_fc_features[\"visual\"][\"obj\"] = self.fc_rois[\"obj\"](neg_features[\"visual\"][\"obj\"])\n",
    "\n",
    "            neg_fc_features[\"visual\"][\"subj\"] = self.fc_rois2[\"subj\"](neg_fc_features[\"visual\"][\"subj\"])\n",
    "            neg_fc_features[\"visual\"][\"pred\"] = self.fc_rois2[\"pred\"](neg_fc_features[\"visual\"][\"pred\"])\n",
    "            neg_fc_features[\"visual\"][\"obj\"] = self.fc_rois2[\"obj\"](neg_fc_features[\"visual\"][\"obj\"])\n",
    "            \n",
    "            #neg fc word\n",
    "            neg_fc_features[\"textual\"][\"subj\"] = self.fc_word[\"subj\"](neg_features[\"textual\"][\"subj\"])\n",
    "            neg_fc_features[\"textual\"][\"pred\"] = self.fc_word[\"pred\"](neg_features[\"textual\"][\"pred\"])\n",
    "            neg_fc_features[\"textual\"][\"obj\"] = self.fc_word[\"obj\"](neg_features[\"textual\"][\"obj\"])\n",
    "\n",
    "            if (triplet_losses[\"subj\"] is None):\n",
    "                triplet_losses[\"subj\"] = self.triplet_loss[\"subj\"](fc_features[\"visual\"][\"subj\"], fc_features[\"textual\"][\"subj\"], neg_fc_features[\"visual\"][\"subj\"]) + self.triplet_loss[\"subj\"](fc_features[\"textual\"][\"subj\"], fc_features[\"visual\"][\"subj\"], neg_fc_features[\"textual\"][\"subj\"])\n",
    "                triplet_losses[\"pred\"] = self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"pred\"], fc_features[\"textual\"][\"pred\"], neg_fc_features[\"visual\"][\"pred\"]) + self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"pred\"], fc_features[\"visual\"][\"pred\"], neg_fc_features[\"textual\"][\"pred\"])\n",
    "                triplet_losses[\"obj\"] = self.triplet_loss[\"obj\"](fc_features[\"visual\"][\"obj\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"visual\"][\"obj\"]) + self.triplet_loss[\"obj\"](fc_features[\"textual\"][\"obj\"],fc_features[\"visual\"][\"obj\"],neg_fc_features[\"textual\"][\"obj\"])\n",
    "                \n",
    "                triplet_losses[\"transr\"] = self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"subj\"] + fc_features[\"visual\"][\"pred\"], fc_features[\"visual\"][\"obj\"], neg_fc_features[\"visual\"][\"subj\"] + neg_fc_features[\"visual\"][\"pred\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"subj\"] + fc_features[\"textual\"][\"pred\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"textual\"][\"subj\"] + neg_fc_features[\"textual\"][\"pred\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"subj\"] + fc_features[\"visual\"][\"pred\"], fc_features[\"visual\"][\"obj\"], neg_fc_features[\"visual\"][\"obj\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"subj\"] + fc_features[\"textual\"][\"pred\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"textual\"][\"obj\"])\n",
    "\n",
    "            else:\n",
    "                triplet_losses[\"subj\"] += self.triplet_loss[\"subj\"](fc_features[\"visual\"][\"subj\"], fc_features[\"textual\"][\"subj\"], neg_fc_features[\"visual\"][\"subj\"]) + self.triplet_loss[\"subj\"](fc_features[\"textual\"][\"subj\"],fc_features[\"visual\"][\"subj\"], neg_fc_features[\"textual\"][\"subj\"])\n",
    "                triplet_losses[\"pred\"] += self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"pred\"], fc_features[\"textual\"][\"pred\"], neg_fc_features[\"visual\"][\"pred\"]) + self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"pred\"],fc_features[\"visual\"][\"pred\"],neg_fc_features[\"textual\"][\"pred\"])\n",
    "                triplet_losses[\"obj\"] += self.triplet_loss[\"obj\"](fc_features[\"visual\"][\"obj\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"visual\"][\"obj\"]) + self.triplet_loss[\"obj\"](fc_features[\"textual\"][\"obj\"],fc_features[\"visual\"][\"obj\"],neg_fc_features[\"textual\"][\"obj\"])\n",
    "\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"subj\"] + fc_features[\"visual\"][\"pred\"], fc_features[\"visual\"][\"obj\"], neg_fc_features[\"visual\"][\"subj\"] + neg_fc_features[\"visual\"][\"pred\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"subj\"] + fc_features[\"textual\"][\"pred\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"textual\"][\"subj\"] + neg_fc_features[\"textual\"][\"pred\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"visual\"][\"subj\"] + fc_features[\"visual\"][\"pred\"], fc_features[\"visual\"][\"obj\"], neg_fc_features[\"visual\"][\"obj\"])\n",
    "                triplet_losses[\"transr\"] += self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"subj\"] + fc_features[\"textual\"][\"pred\"], fc_features[\"textual\"][\"obj\"], neg_fc_features[\"textual\"][\"obj\"])\n",
    "\n",
    "        #get triples\n",
    "        language_negatives = []\n",
    "        for subj, pred, obj in zip(relationships['subj_classes'], relationships['pred_classes'], relationships['obj_classes']):\n",
    "            subj_neg_classes = copy.deepcopy(self.object_classes)\n",
    "            subj_neg_classes.remove(self.object_classes[subj])\n",
    "            obj_neg_classes = copy.deepcopy(self.object_classes)\n",
    "            obj_neg_classes.remove(self.object_classes[obj])\n",
    "            \n",
    "            negative_triples = [(self.object_classes.index(neg), pred, obj) for neg in subj_neg_classes]\n",
    "            negative_triples.extend([(subj, pred, self.object_classes.index(neg)) for neg in obj_neg_classes])\n",
    "            \n",
    "            language_negatives.append(negative_triples)\n",
    " \n",
    "        n_neg_lang = 50\n",
    "        for i in random.sample(range(len(language_negatives[0])), n_neg_lang):\n",
    "            neg_example = [item[i] for item in language_negatives]\n",
    "            \n",
    "            neg_word_embeddings = self._get_word_features(neg_example)\n",
    "            neg_features_subj = neg_word_embeddings[0]\n",
    "            neg_features_pred = neg_word_embeddings[1]\n",
    "            neg_features_obj = neg_word_embeddings[2]\n",
    "            \n",
    "            neg_fc_features_subj = self.fc_word[\"subj\"](neg_features_subj)\n",
    "            neg_fc_features_pred = self.fc_word[\"pred\"](neg_features_pred)\n",
    "            neg_fc_features_obj = self.fc_word[\"obj\"](neg_features_obj)\n",
    "\n",
    "            triplet_losses[\"subj\"] = self.triplet_loss[\"subj\"](fc_features[\"textual\"][\"subj\"], fc_features[\"textual\"][\"subj\"], neg_fc_features_subj) + self.triplet_loss[\"subj\"](fc_features[\"textual\"][\"subj\"], fc_features[\"visual\"][\"subj\"], neg_fc_features_subj)\n",
    "            triplet_losses[\"pred\"] = self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"pred\"], fc_features[\"textual\"][\"pred\"], neg_fc_features_pred) + self.triplet_loss[\"pred\"](fc_features[\"textual\"][\"pred\"], fc_features[\"visual\"][\"pred\"], neg_fc_features_pred)\n",
    "            triplet_losses[\"obj\"] = self.triplet_loss[\"obj\"](fc_features[\"textual\"][\"obj\"], fc_features[\"textual\"][\"obj\"], neg_fc_features_obj) + self.triplet_loss[\"obj\"](fc_features[\"textual\"][\"obj\"],fc_features[\"visual\"][\"obj\"], neg_fc_features_obj)            \n",
    "        #divide by the number of training examples\n",
    "        triplet_losses[\"subj\"] = triplet_losses[\"subj\"] / (len(negative_examples) + n_neg_lang)\n",
    "        triplet_losses[\"pred\"] = triplet_losses[\"pred\"] / (len(negative_examples) + n_neg_lang)\n",
    "        triplet_losses[\"obj\"] = triplet_losses[\"obj\"] / (len(negative_examples) + n_neg_lang)      \n",
    "        triplet_losses[\"transr\"] =  triplet_losses[\"transr\"] / len(negative_examples)\n",
    "        return triplet_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3640f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_checkpoint(model, chkpoint_path, optimizer=None):\n",
    "    chkpoint = torch.load(chkpoint_path)\n",
    "    \n",
    "    # incompatibility fixes\n",
    "    \n",
    "    model.load_state_dict(chkpoint['model'])\n",
    "    if (optimizer is not None):\n",
    "        optimizer.load_state_dict(chkpoint['optimizer'])\n",
    "    return chkpoint['it'], chkpoint['epoch'], chkpoint['losses']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6629a",
   "metadata": {},
   "source": [
    "### [Optional] Test Instance Detector (Object detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71ab5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import (\n",
    "    DatasetCatalog, DatasetMapper,\n",
    "    build_detection_train_loader,\n",
    "    build_detection_test_loader,\n",
    ")\n",
    "from config import get_vrd_cfg\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "cfg = get_vrd_cfg()\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .6\n",
    "cfg.DATASETS.TEST = (\"vrd_val\", )\n",
    "\n",
    "#test dataset\n",
    "test_dataset = DatasetCatalog.get(\"vrd_val\")\n",
    "test_dataloader = build_detection_test_loader(dataset=test_dataset,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "#train dataset\n",
    "train_dataset = DatasetCatalog.get(\"vrd_train\")\n",
    "train_dataloader = build_detection_test_loader(dataset=train_dataset,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f0c5cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RelTransR(\n",
       "  (fc_spatial): Sequential(\n",
       "    (0): Linear(in_features=22, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (detectron): GeneralizedRCNN(\n",
       "    (backbone): FPN(\n",
       "      (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (top_block): LastLevelMaxPool()\n",
       "      (bottom_up): ResNet(\n",
       "        (stem): BasicStem(\n",
       "          (conv1): Conv2d(\n",
       "            3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (res2): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (res3): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (3): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (res4): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (3): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (4): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (5): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (6): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (7): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (8): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (9): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (10): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (11): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (12): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (13): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (14): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (15): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (16): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (17): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (18): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (19): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (20): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (21): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (22): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (res5): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proposal_generator): RPN(\n",
       "      (rpn_head): StandardRPNHead(\n",
       "        (conv): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (anchor_generator): DefaultAnchorGenerator(\n",
       "        (cell_anchors): BufferList()\n",
       "      )\n",
       "    )\n",
       "    (roi_heads): StandardROIHeads(\n",
       "      (box_pooler): ROIPooler(\n",
       "        (level_poolers): ModuleList(\n",
       "          (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "          (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "          (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "          (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "        )\n",
       "      )\n",
       "      (box_head): FastRCNNConvFCHead(\n",
       "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "        (fc_relu1): ReLU()\n",
       "        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (fc_relu2): ReLU()\n",
       "      )\n",
       "      (box_predictor): FastRCNNOutputLayers(\n",
       "        (cls_score): Linear(in_features=1024, out_features=101, bias=True)\n",
       "        (bbox_pred): Linear(in_features=1024, out_features=400, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (backbone): FPN(\n",
       "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (top_block): LastLevelMaxPool()\n",
       "    (bottom_up): ResNet(\n",
       "      (stem): BasicStem(\n",
       "        (conv1): Conv2d(\n",
       "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (res2): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res3): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res4): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (6): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (7): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (8): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (9): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (10): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (11): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (12): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (13): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (14): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (15): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (16): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (17): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (18): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (19): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (20): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (21): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (22): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res5): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): ROIPooler(\n",
       "    (level_poolers): ModuleList(\n",
       "      (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "      (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "      (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "      (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "    )\n",
       "  )\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc_word): ModuleDict(\n",
       "    (subj): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "      (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "    )\n",
       "    (pred): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "      (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "    )\n",
       "    (obj): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "      (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_rois): ModuleDict(\n",
       "    (subj): Sequential(\n",
       "      (0): Linear(in_features=12544, out_features=6272, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "      (2): Linear(in_features=6272, out_features=256, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.1)\n",
       "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (pred): Sequential(\n",
       "      (0): Linear(in_features=12608, out_features=6272, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "      (2): Linear(in_features=6272, out_features=256, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.1)\n",
       "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (obj): Sequential(\n",
       "      (0): Linear(in_features=12544, out_features=6272, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "      (2): Linear(in_features=6272, out_features=256, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.1)\n",
       "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc_rois2): ModuleDict(\n",
       "    (subj): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (pred): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (obj): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (triplet_loss): ModuleDict(\n",
       "    (subj): TripletMarginWithDistanceLoss()\n",
       "    (pred): TripletMarginWithDistanceLoss()\n",
       "    (obj): TripletMarginWithDistanceLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RelTransR(cfg)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "#Run only once\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf347f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpoint_path = '../checkpoint/'\n",
    "model_name = 'vltranse_langcon_model_8000.pt'\n",
    "chkpoint_full_path = os.path.join(chkpoint_path, model_name)\n",
    "it, start_epoch, losses = load_checkpoint(model, chkpoint_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1a2c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from detectron2.evaluation import COCOEvaluator, DatasetEvaluator, DatasetEvaluators, inference_on_dataset\n",
    "\n",
    "object_classes = model.object_classes\n",
    "\n",
    "class Counter(DatasetEvaluator):\n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        self.gt_count = 0\n",
    "        self.object_areas = defaultdict(list)\n",
    "        \n",
    "    def process(self, inputs, outputs):\n",
    "        for output in outputs:\n",
    "            self.count += len(output[\"instances\"])\n",
    "        \n",
    "        for inp in inputs:\n",
    "            self.gt_count += len(inp[\"instances\"])\n",
    "\n",
    "            annotations = inp['instances'].get_fields()    \n",
    "            for box, cls in zip(annotations['gt_boxes'], annotations['gt_classes']):\n",
    "                object_width = box[2] - box[0] + 1\n",
    "                object_height = box[3] - box[1] + 1\n",
    "                object_area = object_width * object_height\n",
    "                self.object_areas['area-{}'.format(object_classes[cls])].append(object_area)\n",
    "                \n",
    "    def evaluate(self):\n",
    "        # save self.count somewhere, or print it, or return it.\n",
    "        resulting_dict = {\"count\": self.count, \"gt_count\": self.gt_count, \"object_average_area\": {}}\n",
    "        \n",
    "        for key, areas in self.object_areas.items():\n",
    "            resulting_dict[\"object_average_area\"][key] = sum(areas) / len(areas)\n",
    "        \n",
    "        return resulting_dict\n",
    "    \n",
    "def beatify_detectron2_results(eval_results):\n",
    "    \"\"\"\n",
    "        Beautify the results output by detectron2\n",
    "    \"\"\"\n",
    "    object_areas = eval_results['object_average_area']\n",
    "    object_area_ap = {}\n",
    "    \n",
    "    for eval_method, eval_result in eval_results.items():\n",
    "        if (eval_method == 'count'):\n",
    "            print(\"Total Objects Detected:\", eval_result)\n",
    "        elif (eval_method == 'gt_count'):\n",
    "            print(\"Total Labeled Objects:\", eval_result)\n",
    "        elif (eval_method == 'object_average_area'):\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Evaluation results for {}\".format(eval_method))\n",
    "\n",
    "            resulting_string = \"\"\n",
    "            for i, (key, res) in enumerate(eval_result.items()):\n",
    "                resulting_string = \"\".join((resulting_string, \"|   {:>16}\\t->\\t{:5.2f}\".format(key, res)))\n",
    "                if ((i + 1) <= 6):\n",
    "                    resulting_string = \"\".join((resulting_string, \"   |\"))\n",
    "                if ((i + 1) == 6):\n",
    "                    resulting_string = \"\".join((resulting_string, \"\\nEvaluation results by object category\\n\"))\n",
    "                elif ((i + 1) > 6):\n",
    "                    object_cls = key.split('-')[1]\n",
    "                    area_key = 'area-{}'.format(object_cls)\n",
    "                    object_area = object_areas[area_key]\n",
    "                    \n",
    "                    resulting_string = \"\".join((resulting_string, \"( {:5.2f} area )\\t|\".format(object_area)))\n",
    "                    object_area_ap[object_cls] = (res, object_area)\n",
    "                    \n",
    "                if ((i + 1) % 2 == 0):\n",
    "                    resulting_string = \"\".join((resulting_string, \"\\n\"))\n",
    "            print(resulting_string)\n",
    "            \n",
    "    return object_area_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08971bfd",
   "metadata": {},
   "source": [
    "##### Object Detector Evaluation for Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce338ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = inference_on_dataset(\n",
    "    model.detectron,\n",
    "    test_dataloader,\n",
    "    DatasetEvaluators([COCOEvaluator('vrd_val', output_dir=\"../generated/coco_evaluations_val\"), Counter()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d3444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "object_area_ap = beatify_detectron2_results(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0aa3d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "test_object_data = [ (key, item[0], item[1]) for key, item in object_area_ap.items()]\n",
    "\n",
    "area_ap_df = pd.DataFrame(pd.DataFrame(list(test_object_data),\n",
    "               columns =['Object', 'AP', 'Area']))\n",
    "\n",
    "sns.scatterplot(data=area_ap_df, x=\"Area\", y=\"AP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ed32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_ap = np.argsort([item[1] for item in test_object_data])\n",
    "ranked_object = np.array([item[0] for item in test_object_data])[rank_ap]\n",
    "print(\"Worst Performing Objects:\", ranked_object[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99ba7d",
   "metadata": {},
   "source": [
    "##### Object detector performance for Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2b890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_results = inference_on_dataset(\n",
    "    model.detectron,\n",
    "    train_dataloader,\n",
    "    DatasetEvaluators([COCOEvaluator('vrd_train', output_dir=\"../generated/coco_evaluations_train/\"), Counter()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa74ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_area_ap = beatify_detectron2_results(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e666328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "train_object_data = [ (key, item[0], item[1]) for key, item in object_area_ap.items()]\n",
    "\n",
    "area_ap_df = pd.DataFrame(pd.DataFrame(list(train_object_data),\n",
    "               columns =['Object', 'AP', 'Area']))\n",
    "\n",
    "sns.scatterplot(data=area_ap_df, x=\"Area\", y=\"AP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e803bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_ap = np.argsort([item[1] for item in train_object_data])\n",
    "ranked_object = np.array([item[0] for item in train_object_data])[rank_ap]\n",
    "print(\"Worst Performing Objects:\", ranked_object[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a6500c",
   "metadata": {},
   "source": [
    "### [Optional] Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bf6b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import (\n",
    "    DatasetCatalog, DatasetMapper,\n",
    "    build_detection_train_loader,\n",
    "    build_detection_test_loader,    \n",
    ")\n",
    "from config import get_vrd_cfg\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "cfg = get_vrd_cfg()\n",
    "\n",
    "# DEPRECARTED: Old Dataloader Code\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .6\n",
    "# cfg.DATASETS.TRAIN = (\"vrd_val\", )\n",
    "# test_dataloader = build_detection_train_loader(cfg,\n",
    "#     mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "#         T.Resize((800, 800))\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RelTransR(cfg)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "#Run only once\n",
    "model.eval()\n",
    "\n",
    "#Load Model\n",
    "chkpoint_path = '../checkpoint/'\n",
    "model_name = 'vrd2_model_transr_23000.pt'\n",
    "chkpoint_full_path = os.path.join(chkpoint_path, model_name)\n",
    "it, start_epoch, losses = load_checkpoint(model, chkpoint_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a22bb41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get triples that are in the training set\n",
    "import os, json\n",
    "\n",
    "trained_triples_path = '../generated/trained_triples.json'\n",
    "trained_triples = {}\n",
    "if (os.path.exists(trained_triples_path)):\n",
    "    with open(trained_triples_path, 'r') as file:\n",
    "        trained_triples = json.load(file)\n",
    "else:\n",
    "    iter_dataloader = iter(train_dataloader)\n",
    "    n_iters = len(train_dataloader.dataset.dataset)\n",
    "    for i in range(n_iters):\n",
    "        print(i)\n",
    "        data = next(iter_dataloader)[0]\n",
    "        relationships = data['relationships']\n",
    "        for j in range(len(relationships['subj_classes'])):\n",
    "            subj_cls = model.object_classes[relationships['subj_classes'][j]]\n",
    "            pred_cls = model.predicate_classes[relationships['pred_classes'][j]]\n",
    "            obj_cls = model.object_classes[relationships['obj_classes'][j]]\n",
    "            trained_triples['{}-{}-{}'.format(subj_cls, pred_cls, obj_cls)] = 1\n",
    "\n",
    "    with open(trained_triples_path, 'w') as file:\n",
    "        file.write(json.dumps(trained_triples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d7489",
   "metadata": {},
   "source": [
    "#### EVAL 1/ RELATIONSHIP PREDICTION TASK (NO GROUND TRUTH LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedb0b4",
   "metadata": {},
   "source": [
    "##### Test Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2768e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('not_zeroshot', 50, 'triple_subtract_dist')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhtran/dev/research_env/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/thanhtran/dev/research_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:43: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "/home/thanhtran/dev/research_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n",
      "/home/thanhtran/dev/vrdtransr/src/utils/eval_helpers.py:122: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sorted_val = np.array(val)[sorted_by_distance]\n",
      "/home/thanhtran/dev/vrdtransr/src/utils/eval_helpers.py:122: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sorted_val = np.array(val)[sorted_by_distance]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  3.54               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  4.35               |\n",
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  6.10               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  4.50               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  4.97               |\n",
      "|----------------------Iter 160------------------------|\n",
      "| TOP 50 |               Recall  5.00               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  5.05               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  5.55               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall  5.59               |\n",
      "|----------------------Iter 260------------------------|\n",
      "| TOP 50 |               Recall  5.78               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  5.91               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  6.02               |\n",
      "|----------------------Iter 320------------------------|\n",
      "| TOP 50 |               Recall  5.91               |\n",
      "|----------------------Iter 340------------------------|\n",
      "| TOP 50 |               Recall  5.64               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  5.73               |\n",
      "|----------------------Iter 380------------------------|\n",
      "| TOP 50 |               Recall  5.75               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  5.71               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  5.76               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  5.62               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  5.56               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  5.55               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  5.65               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  5.54               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  5.45               |\n",
      "|----------------------Iter 560------------------------|\n",
      "| TOP 50 |               Recall  5.50               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall  5.53               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  5.45               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  5.44               |\n",
      "|----------------------Iter 640------------------------|\n",
      "| TOP 50 |               Recall  5.44               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  5.53               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall  5.49               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall  5.36               |\n",
      "|----------------------Iter 740------------------------|\n",
      "| TOP 50 |               Recall  5.35               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  5.43               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  5.45               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  5.43               |\n",
      "|----------------------Iter 840------------------------|\n",
      "| TOP 50 |               Recall  5.39               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  5.48               |\n",
      "|----------------------Iter 880------------------------|\n",
      "| TOP 50 |               Recall  5.41               |\n",
      "|----------------------Iter 900------------------------|\n",
      "| TOP 50 |               Recall  5.33               |\n",
      "|----------------------Iter 920------------------------|\n",
      "| TOP 50 |               Recall  5.25               |\n",
      "|----------------------Iter 940------------------------|\n",
      "| TOP 50 |               Recall  5.21               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  5.25               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  5.20               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  5.16               |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('not_zeroshot', 50, 'triple_transe_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  3.54               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  4.78               |\n",
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  6.98               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  5.11               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  5.28               |\n",
      "|----------------------Iter 160------------------------|\n",
      "| TOP 50 |               Recall  5.45               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  5.36               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  5.76               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall  5.92               |\n",
      "|----------------------Iter 260------------------------|\n",
      "| TOP 50 |               Recall  6.33               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  6.53               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  6.60               |\n",
      "|----------------------Iter 320------------------------|\n",
      "| TOP 50 |               Recall  6.60               |\n",
      "|----------------------Iter 340------------------------|\n",
      "| TOP 50 |               Recall  6.37               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  6.41               |\n",
      "|----------------------Iter 380------------------------|\n",
      "| TOP 50 |               Recall  6.39               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  6.32               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  6.38               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  6.29               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  6.19               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  6.22               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  6.34               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  6.22               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  6.18               |\n",
      "|----------------------Iter 560------------------------|\n",
      "| TOP 50 |               Recall  6.25               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall  6.25               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  6.15               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  6.14               |\n",
      "|----------------------Iter 640------------------------|\n",
      "| TOP 50 |               Recall  6.14               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  6.24               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall  6.20               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall  6.02               |\n",
      "|----------------------Iter 740------------------------|\n",
      "| TOP 50 |               Recall  6.03               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  6.13               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  6.16               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  6.10               |\n",
      "|----------------------Iter 840------------------------|\n",
      "| TOP 50 |               Recall  6.03               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  6.09               |\n",
      "|----------------------Iter 880------------------------|\n",
      "| TOP 50 |               Recall  6.01               |\n",
      "|----------------------Iter 900------------------------|\n",
      "| TOP 50 |               Recall  5.97               |\n",
      "|----------------------Iter 920------------------------|\n",
      "| TOP 50 |               Recall  5.87               |\n",
      "|----------------------Iter 940------------------------|\n",
      "| TOP 50 |               Recall  5.88               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  5.90               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  5.84               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  5.78               |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('not_zeroshot', 50, 'pred_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  6.19               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  5.65               |\n",
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  7.27               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  5.26               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  4.76               |\n",
      "|----------------------Iter 160------------------------|\n",
      "| TOP 50 |               Recall  5.09               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  4.97               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  5.62               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall  5.59               |\n",
      "|----------------------Iter 260------------------------|\n",
      "| TOP 50 |               Recall  6.06               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  6.22               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  6.36               |\n",
      "|----------------------Iter 320------------------------|\n",
      "| TOP 50 |               Recall  6.41               |\n",
      "|----------------------Iter 340------------------------|\n",
      "| TOP 50 |               Recall  6.33               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  6.33               |\n",
      "|----------------------Iter 380------------------------|\n",
      "| TOP 50 |               Recall  6.28               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  6.10               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  6.11               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  5.99               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  5.84               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  5.95               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  6.07               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  5.94               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  5.86               |\n",
      "|----------------------Iter 560------------------------|\n",
      "| TOP 50 |               Recall  5.89               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall  5.83               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  5.74               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  5.72               |\n",
      "|----------------------Iter 640------------------------|\n",
      "| TOP 50 |               Recall  5.76               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  5.93               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall  5.92               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall  5.81               |\n",
      "|----------------------Iter 740------------------------|\n",
      "| TOP 50 |               Recall  5.82               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  5.85               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  5.86               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  5.80               |\n",
      "|----------------------Iter 840------------------------|\n",
      "| TOP 50 |               Recall  5.70               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  5.77               |\n",
      "|----------------------Iter 880------------------------|\n",
      "| TOP 50 |               Recall  5.68               |\n",
      "|----------------------Iter 900------------------------|\n",
      "| TOP 50 |               Recall  5.65               |\n",
      "|----------------------Iter 920------------------------|\n",
      "| TOP 50 |               Recall  5.57               |\n",
      "|----------------------Iter 940------------------------|\n",
      "| TOP 50 |               Recall  5.58               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  5.53               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  5.45               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  5.44               |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('not_zeroshot', 50, 'pred_subtract_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  7.08               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  6.52               |\n",
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  7.27               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  4.95               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  5.07               |\n",
      "|----------------------Iter 160------------------------|\n",
      "| TOP 50 |               Recall  5.36               |\n",
      "|----------------------Iter 180------------------------|\n",
      "| TOP 50 |               Recall  5.44               |\n",
      "|----------------------Iter 200------------------------|\n",
      "| TOP 50 |               Recall  6.20               |\n",
      "|----------------------Iter 220------------------------|\n",
      "| TOP 50 |               Recall  6.11               |\n",
      "|----------------------Iter 260------------------------|\n",
      "| TOP 50 |               Recall  6.17               |\n",
      "|----------------------Iter 280------------------------|\n",
      "| TOP 50 |               Recall  6.27               |\n",
      "|----------------------Iter 300------------------------|\n",
      "| TOP 50 |               Recall  6.50               |\n",
      "|----------------------Iter 320------------------------|\n",
      "| TOP 50 |               Recall  6.37               |\n",
      "|----------------------Iter 340------------------------|\n",
      "| TOP 50 |               Recall  6.15               |\n",
      "|----------------------Iter 360------------------------|\n",
      "| TOP 50 |               Recall  6.21               |\n",
      "|----------------------Iter 380------------------------|\n",
      "| TOP 50 |               Recall  6.28               |\n",
      "|----------------------Iter 400------------------------|\n",
      "| TOP 50 |               Recall  6.18               |\n",
      "|----------------------Iter 420------------------------|\n",
      "| TOP 50 |               Recall  6.28               |\n",
      "|----------------------Iter 440------------------------|\n",
      "| TOP 50 |               Recall  6.12               |\n",
      "|----------------------Iter 460------------------------|\n",
      "| TOP 50 |               Recall  6.10               |\n",
      "|----------------------Iter 480------------------------|\n",
      "| TOP 50 |               Recall  6.16               |\n",
      "|----------------------Iter 500------------------------|\n",
      "| TOP 50 |               Recall  6.34               |\n",
      "|----------------------Iter 520------------------------|\n",
      "| TOP 50 |               Recall  6.17               |\n",
      "|----------------------Iter 540------------------------|\n",
      "| TOP 50 |               Recall  6.05               |\n",
      "|----------------------Iter 560------------------------|\n",
      "| TOP 50 |               Recall  6.05               |\n",
      "|----------------------Iter 580------------------------|\n",
      "| TOP 50 |               Recall  6.00               |\n",
      "|----------------------Iter 600------------------------|\n",
      "| TOP 50 |               Recall  5.91               |\n",
      "|----------------------Iter 620------------------------|\n",
      "| TOP 50 |               Recall  5.93               |\n",
      "|----------------------Iter 640------------------------|\n",
      "| TOP 50 |               Recall  5.92               |\n",
      "|----------------------Iter 660------------------------|\n",
      "| TOP 50 |               Recall  6.06               |\n",
      "|----------------------Iter 680------------------------|\n",
      "| TOP 50 |               Recall  6.05               |\n",
      "|----------------------Iter 700------------------------|\n",
      "| TOP 50 |               Recall  5.91               |\n",
      "|----------------------Iter 740------------------------|\n",
      "| TOP 50 |               Recall  6.03               |\n",
      "|----------------------Iter 780------------------------|\n",
      "| TOP 50 |               Recall  6.11               |\n",
      "|----------------------Iter 800------------------------|\n",
      "| TOP 50 |               Recall  6.07               |\n",
      "|----------------------Iter 820------------------------|\n",
      "| TOP 50 |               Recall  6.04               |\n",
      "|----------------------Iter 840------------------------|\n",
      "| TOP 50 |               Recall  5.97               |\n",
      "|----------------------Iter 860------------------------|\n",
      "| TOP 50 |               Recall  6.07               |\n",
      "|----------------------Iter 880------------------------|\n",
      "| TOP 50 |               Recall  5.98               |\n",
      "|----------------------Iter 900------------------------|\n",
      "| TOP 50 |               Recall  5.94               |\n",
      "|----------------------Iter 920------------------------|\n",
      "| TOP 50 |               Recall  5.87               |\n",
      "|----------------------Iter 940------------------------|\n",
      "| TOP 50 |               Recall  5.86               |\n",
      "|----------------------Iter 960------------------------|\n",
      "| TOP 50 |               Recall  5.86               |\n",
      "|----------------------Iter 980------------------------|\n",
      "| TOP 50 |               Recall  5.80               |\n",
      "|----------------------Iter 999------------------------|\n",
      "| TOP 50 |               Recall  5.78               |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('not_zeroshot', 50, 'pred_transe_dist')\n",
      "|----------------------Iter 20------------------------|\n",
      "| TOP 50 |               Recall  4.42               |\n",
      "|----------------------Iter 40------------------------|\n",
      "| TOP 50 |               Recall  5.22               |\n",
      "|----------------------Iter 60------------------------|\n",
      "| TOP 50 |               Recall  6.98               |\n",
      "|----------------------Iter 100------------------------|\n",
      "| TOP 50 |               Recall  5.11               |\n",
      "|----------------------Iter 140------------------------|\n",
      "| TOP 50 |               Recall  4.87               |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(recall_results_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    117\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(recall_results))\n\u001b[0;32m--> 120\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvltranse_langcon_model_8000.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(checkpoint_model_name, dataset_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot_zeroshot\u001b[39m\u001b[38;5;124m\"\u001b[39m, nre, conf))\n\u001b[0;32m---> 97\u001b[0m     recall_results[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot_zeroshot\u001b[39m\u001b[38;5;124m\"\u001b[39m, nre, conf)] \u001b[38;5;241m=\u001b[39m \u001b[43meval_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnre\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnre\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrained_triples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m test_dataloader\n",
      "File \u001b[0;32m~/dev/vrdtransr/src/utils/eval_helpers.py:329\u001b[0m, in \u001b[0;36meval_dataset\u001b[0;34m(dataloader, model, nre, config, trained_triples)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| TOP \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m |               Recall \u001b[39m\u001b[38;5;132;01m{:5.2f}\u001b[39;00m\u001b[38;5;124m               |\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    324\u001b[0m             nre, recall\n\u001b[1;32m    325\u001b[0m         )\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Get the top\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m top_predicted_relationships \u001b[38;5;241m=\u001b[39m \u001b[43mget_top_nre_relationships\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m image_true_positive, image_relationships \u001b[38;5;241m=\u001b[39m eval_per_image(\n\u001b[1;32m    333\u001b[0m     gt_relationships, top_predicted_relationships, \u001b[38;5;241m0.5\u001b[39m, trained_triples, model\n\u001b[1;32m    334\u001b[0m )\n\u001b[1;32m    336\u001b[0m total_true_positive \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m image_true_positive\n",
      "File \u001b[0;32m~/dev/vrdtransr/src/utils/eval_helpers.py:45\u001b[0m, in \u001b[0;36mget_top_nre_relationships\u001b[0;34m(model, data, nre, config)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: ensure that the distance metric is one of the following options: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m         eval_config,\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     38\u001b[0m relationships \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelationships\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     39\u001b[0m (\n\u001b[1;32m     40\u001b[0m     predicate_distances,\n\u001b[1;32m     41\u001b[0m     subject_distances,\n\u001b[1;32m     42\u001b[0m     object_distances,\n\u001b[1;32m     43\u001b[0m     predicate_subtract_distances,\n\u001b[1;32m     44\u001b[0m     transe_distances,\n\u001b[0;32m---> 45\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_predicate_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_rel_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m all_possible_relationships \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mlist\u001b[39m\n\u001b[1;32m     48\u001b[0m )  \u001b[38;5;66;03m# {scores: [], subj_classes:[], pred_classes:[], obj_classes:[], subj_bboxes:[], obj_bboxes:[]}\u001b[39;00m\n\u001b[1;32m     49\u001b[0m top_n_relationships \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mRelTransR.get_predicate_distances\u001b[0;34m(self, data, is_rel_eval)\u001b[0m\n\u001b[1;32m    456\u001b[0m distance_transe \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subj_emb, pred_emb, obj_emb, pred_transe_emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(fc_all_subject_embeddings, fc_all_predicate_embeddings, fc_all_object_embeddings, fc_pred_transe_language_feature):\n\u001b[0;32m--> 459\u001b[0m     distance\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfc_pred_visual_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    460\u001b[0m     distance_subtract\u001b[38;5;241m.\u001b[39mappend(pdist(torch\u001b[38;5;241m.\u001b[39munsqueeze(fc_pred_subtract_visual_feature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39munsqueeze(obj_emb \u001b[38;5;241m-\u001b[39m subj_emb, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m    461\u001b[0m     distance_transe\u001b[38;5;241m.\u001b[39mappend(pdist(torch\u001b[38;5;241m.\u001b[39munsqueeze(fc_pred_transe_visual_feature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39munsqueeze(pred_transe_emb, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mRelTransR.get_predicate_distances.<locals>.<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    448\u001b[0m fc_obj_visual_feature \u001b[38;5;241m=\u001b[39m fc_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisual\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj\u001b[39m\u001b[38;5;124m\"\u001b[39m][rel_cnt,:]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m#compute distance between the fc_features[\"visual\"][\"pred\"] and fc_predicate_embeddings to get top n\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m pdist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x, y: \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m distance \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    453\u001b[0m distance_subject \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from utils.register_dataset import register_vrd_dataset\n",
    "from config import GENERATED_DIR, get_vrd_cfg, CHECKPOINT_DIR\n",
    "\n",
    "from modeling.word_features import get_triples_features, get_trained_triples_memo\n",
    "\n",
    "from utils.annotations import get_object_classes, get_predicate_classes\n",
    "from utils.trainer import load_checkpoint\n",
    "from utils.eval_helpers import eval_dataset, eval_config\n",
    "\n",
    "from detectron2.data import (\n",
    "    DatasetCatalog,\n",
    "    DatasetMapper,\n",
    "    build_detection_train_loader,\n",
    "    build_detection_test_loader,\n",
    ")\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "\n",
    "def load_model(cfg, checkpoint_model_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = RelTransR(cfg)\n",
    "    model.to(device)\n",
    "\n",
    "    chkpoint_full_path = os.path.join(CHECKPOINT_DIR, checkpoint_model_name)\n",
    "    _, _, _ = load_checkpoint(model, chkpoint_full_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(checkpoint_model_name, dataset_name=\"vrd_val\"):\n",
    "    cfg = get_vrd_cfg()\n",
    "    cfg.DATASETS.TEST = (\"dataset_name\",)\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4\n",
    "\n",
    "    # [Only Run once] Register dataset with detectron2 instead of using my own dataloader\n",
    "#     register_vrd_dataset(dataset_name.split(\"_\")[0])\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(cfg, checkpoint_model_name)\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Parallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # For zeroshot\n",
    "    trained_triples = get_trained_triples_memo()\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    # test_dataset = DatasetCatalog.get(dataset_name)\n",
    "    # test_dataloader = build_detection_test_loader(\n",
    "    #     dataset=test_dataset,\n",
    "    #     mapper=DatasetMapper(cfg, is_train=True, augmentations=[T.Resize((800, 800))]),\n",
    "    # )\n",
    "\n",
    "    # Iterate and Test\n",
    "    recall_results = {}\n",
    "    i = 0\n",
    "    for is_zeroshot in [False, True]:\n",
    "        for nre in [50, 100]:\n",
    "            for conf in eval_config:\n",
    "                if i == 0:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                test_dataset = DatasetCatalog.get(dataset_name)\n",
    "                test_dataloader = build_detection_test_loader(\n",
    "                    dataset=test_dataset,\n",
    "                    mapper=DatasetMapper(\n",
    "                        cfg, is_train=True, augmentations=[T.Resize((800, 800))]\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                if is_zeroshot:\n",
    "                    print((\"is_zeroshot\", nre, conf))\n",
    "                    recall_results[(\"is_zeroshot\", nre, conf)] = eval_dataset(\n",
    "                        test_dataloader,\n",
    "                        model,\n",
    "                        nre=nre,\n",
    "                        config=conf,\n",
    "                        trained_triples=trained_triples,\n",
    "                    )\n",
    "                else:\n",
    "                    print((\"not_zeroshot\", nre, conf))\n",
    "                    recall_results[(\"not_zeroshot\", nre, conf)] = eval_dataset(\n",
    "                        test_dataloader,\n",
    "                        model,\n",
    "                        nre=nre,\n",
    "                        config=conf,\n",
    "                        trained_triples=None,\n",
    "                    )\n",
    "\n",
    "                test_dataloader = None\n",
    "                del test_dataloader\n",
    "                del test_dataset\n",
    "                del model\n",
    "\n",
    "                model = load_model(cfg, checkpoint_model_name)\n",
    "                # Set model to evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "    recall_results_path = f\"{GENERATED_DIR}/{checkpoint_model_name.split('.')[0]}.json\"\n",
    "\n",
    "    with open(recall_results_path, \"w\") as file:\n",
    "        file.write(json.dumps(recall_results))\n",
    "\n",
    "\n",
    "test_model(\"vltranse_langcon_model_8000.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2e573f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/17 09:00:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [Resize(shape=(800, 800))]\n",
      "\u001b[32m[03/17 09:00:10 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"
     ]
    }
   ],
   "source": [
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .4\n",
    "cfg.DATASETS.TRAIN = (\"vrd_val\", )\n",
    "\n",
    "test_dataloader = build_detection_train_loader(cfg,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")\n",
    "test_data_iter = iter(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04b681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_results_path = '../generated/results_recall_vrd2_model_transr_23000.json'\n",
    "\n",
    "with open(recall_results_path, 'w') as file:\n",
    "    file.write(json.dumps(recall_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee57204",
   "metadata": {},
   "source": [
    "##### Train Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47f584d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/17 08:05:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [Resize(shape=(800, 800))]\n"
     ]
    }
   ],
   "source": [
    "#train dataset\n",
    "cfg.DATASETS.TEST = (\"vrd_train\", )\n",
    "\n",
    "train_dataset = DatasetCatalog.get(\"vrd_train\")\n",
    "train_dataloader = build_detection_test_loader(dataset=train_dataset,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f1a099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': '/home/thanhtran/dev/vrdtransr/data/vrd/train_images/000000002658.jpg',\n",
       " 'image_id': 2658,\n",
       " 'annotations': [{'bbox': [306, 94, 590, 175],\n",
       "   'bbox_mode': 0,\n",
       "   'category_id': 22},\n",
       "  {'bbox': [324, 336, 458, 489], 'bbox_mode': 0, 'category_id': 5},\n",
       "  {'bbox': [539, 265, 680, 397], 'bbox_mode': 0, 'category_id': 7},\n",
       "  {'bbox': [425, 273, 556, 394], 'bbox_mode': 0, 'category_id': 7},\n",
       "  {'bbox': [302, 270, 420, 396], 'bbox_mode': 0, 'category_id': 7},\n",
       "  {'bbox': [159, 272, 293, 401], 'bbox_mode': 0, 'category_id': 7},\n",
       "  {'bbox': [47, 271, 156, 407], 'bbox_mode': 0, 'category_id': 7}],\n",
       " 'relationships': {'subj_bboxes': [array([[306, 141, 590, 262]]),\n",
       "   array([[306, 141, 590, 262]]),\n",
       "   array([[539, 397, 680, 595]]),\n",
       "   array([[539, 397, 680, 595]]),\n",
       "   array([[425, 409, 556, 591]]),\n",
       "   array([[425, 409, 556, 591]]),\n",
       "   array([[302, 405, 420, 594]]),\n",
       "   array([[159, 408, 293, 601]]),\n",
       "   array([[ 47, 406, 156, 610]]),\n",
       "   array([[324, 504, 458, 733]])],\n",
       "  'obj_bboxes': [array([[324, 504, 458, 733]]),\n",
       "   array([[324, 504, 458, 733]]),\n",
       "   array([[306, 141, 590, 262]]),\n",
       "   array([[425, 409, 556, 591]]),\n",
       "   array([[302, 405, 420, 594]]),\n",
       "   array([[306, 141, 590, 262]]),\n",
       "   array([[159, 408, 293, 601]]),\n",
       "   array([[ 47, 406, 156, 610]]),\n",
       "   array([[159, 408, 293, 601]]),\n",
       "   array([[306, 141, 590, 262]])],\n",
       "  'union_bboxes': [array([306, 141, 590, 733]),\n",
       "   array([306, 141, 590, 733]),\n",
       "   array([306, 141, 680, 595]),\n",
       "   array([425, 397, 680, 595]),\n",
       "   array([302, 405, 556, 594]),\n",
       "   array([306, 141, 590, 591]),\n",
       "   array([159, 405, 420, 601]),\n",
       "   array([ 47, 406, 293, 610]),\n",
       "   array([ 47, 406, 293, 610]),\n",
       "   array([306, 141, 590, 733])],\n",
       "  'subj_classes': [22, 22, 7, 7, 7, 7, 7, 7, 7, 5],\n",
       "  'pred_classes': [9, 42, 15, 28, 28, 15, 28, 28, 28, 15],\n",
       "  'obj_classes': [5, 5, 22, 7, 7, 22, 7, 7, 7, 22]},\n",
       " 'height': 533,\n",
       " 'width': 800}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = {}\n",
    "train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c54343",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_results = {}\n",
    "for is_zeroshot in [False, True]:\n",
    "    for nre in [50, 100]:\n",
    "        for conf in eval_config:\n",
    "            train_dataloader = build_detection_test_loader(dataset=train_dataset,\n",
    "                mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "                    T.Resize((800, 800))\n",
    "                ])\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                if (is_zeroshot):\n",
    "                    print(('is_zeroshot', nre, conf))\n",
    "                    recall_results[('is_zeroshot', nre, conf)] = eval_dataset(train_dataloader, model, nre=50, config=conf, trained_triples=trained_triples)\n",
    "                else:\n",
    "                    print(('not_zeroshot', nre, conf))\n",
    "                    recall_results[('not_zeroshot',nre, conf)] = eval_dataset(train_dataloader, model, nre=50, config=conf, trained_triples=None)\n",
    "\n",
    "            train_data_iter = None\n",
    "            del train_dataloader\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd2416",
   "metadata": {},
   "source": [
    "#### EVAL 2/ PREDICATE PREDICTION TASK (GROUND TRUTH BOUNDING BOXES AND LABELS PROVIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b757019b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 44\u001b[0m     predicate_distances, predicate_subtract_distances, transe_distances \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mget_predicate_distances(data)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#Verbose\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Evalutation of the models performance on detective visual predicate\n",
    "#default\n",
    "total_true_positive = 0\n",
    "total_true_positive_5 = 0\n",
    "total_relationships = 0\n",
    "\n",
    "total_true_positive_zeroshot = 0\n",
    "total_true_positive_5_zeroshot = 0\n",
    "total_relationships_zeroshot = 0\n",
    "\n",
    "#sub\n",
    "total_true_positive_sub = 0\n",
    "total_true_positive_5_sub = 0\n",
    "\n",
    "total_true_positive_sub_zeroshot = 0\n",
    "total_true_positive_5_sub_zeroshot = 0\n",
    "\n",
    "#mul\n",
    "total_true_positive_mul = 0\n",
    "total_true_positive_5_mul = 0\n",
    "\n",
    "total_true_positive_mul_zeroshot = 0\n",
    "total_true_positive_5_mul_zeroshot = 0\n",
    "\n",
    "#transe\n",
    "total_true_positive_transe = 0\n",
    "total_true_positive_5_transe = 0\n",
    "\n",
    "total_true_positive_transe_mul = 0\n",
    "total_true_positive_5_transe_mul = 0\n",
    "\n",
    "n_examples = len(test_dataloader.dataset.dataset.dataset)\n",
    "\n",
    "for i in range(n_examples):\n",
    "    data = next(test_data_iter)[0]\n",
    "    relationships = data[\"relationships\"]\n",
    "    \n",
    "    if (len(relationships['subj_bboxes']) == 0):\n",
    "        #no relationship annotations for the given image\n",
    "        continue\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicate_distances, predicate_subtract_distances, transe_distances = model.get_predicate_distances(data)\n",
    "        \n",
    "    #Verbose\n",
    "    if (i % 20 == 0 and i > 0):\n",
    "        recall_zeroshot_1 = 0\n",
    "        recall_zeroshot_5 = 0\n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_1 = total_true_positive_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_5 = total_true_positive_5_zeroshot / total_relationships_zeroshot * 100\n",
    "\n",
    "        recall_sub_1 = total_true_positive_sub / total_relationships * 100\n",
    "        recall_sub_5 = total_true_positive_5_sub / total_relationships * 100\n",
    "        recall_zeroshot_sub_1 = 0\n",
    "        recall_zeroshot_sub_5 = 0\n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_sub_1 = total_true_positive_sub_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_sub_5 = total_true_positive_5_sub_zeroshot / total_relationships_zeroshot * 100        \n",
    "        \n",
    "        recall_mul_1 = total_true_positive_mul / total_relationships * 100\n",
    "        recall_mul_5 = total_true_positive_5_mul / total_relationships * 100\n",
    "        recall_zeroshot_mul_1 = 0\n",
    "        recall_zeroshot_mul_5 = 0\n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_mul_1 = total_true_positive_mul_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_mul_5 = total_true_positive_5_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "        \n",
    "        recall_transe_1 = total_true_positive_transe / total_relationships * 100\n",
    "        recall_transe_5 = total_true_positive_5_transe / total_relationships * 100\n",
    "        \n",
    "        recall_transe_mul_1 = total_true_positive_transe_mul / total_relationships * 100\n",
    "        recall_transe_mul_5 = total_true_positive_5_transe_mul / total_relationships * 100\n",
    "        \n",
    "        recall_1 =  total_true_positive / total_relationships * 100\n",
    "        recall_5 = total_true_positive_5 / total_relationships * 100\n",
    "        print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "        print(\"| Recall top 1 {:5.2f} | Recall zeroshot top 1 {:5.2f} |\".format(recall_1, recall_zeroshot_1))\n",
    "        print(\"| Recall top 5 {:5.2f} | Recall zeroshot top 5 {:5.2f} |\".format(recall_5, recall_zeroshot_5))\n",
    "        print(\"| Recall sub top 1 {:5.2f} | Recall zeroshot sub top 1 {:5.2f} |\".format(recall_sub_1, recall_zeroshot_sub_1))\n",
    "        print(\"| Recall sub top 5 {:5.2f} | Recall zeroshot sub top 5 {:5.2f} |\".format(recall_sub_5, recall_zeroshot_sub_5))\n",
    "        print(\"| Recall mul top 1 {:5.2f} | Recall zeroshot mul top 1 {:5.2f} |\".format(recall_mul_1, recall_zeroshot_mul_1))\n",
    "        print(\"| Recall mul top 5 {:5.2f} | Recall zeroshot mul top 5 {:5.2f} |\".format(recall_mul_5, recall_zeroshot_mul_5))        \n",
    "        print(\"| Recall transe 1 {:5.2f} | Recall transe mul 1 {:5.2f} |\".format(recall_transe_1, recall_transe_mul_1))\n",
    "        print(\"| Recall transe 5 {:5.2f} | Recall transe mul 5 {:5.2f} |\".format(recall_transe_5, recall_transe_mul_5))\n",
    "\n",
    "    #select top 5 to calculate recall\n",
    "    \n",
    "    for j, pred_distance in enumerate(predicate_distances):\n",
    "        sorted_pred_distance = np.argsort([item.cpu() for item in pred_distance])\n",
    "        \n",
    "        top_5_pred = sorted_pred_distance[:5]\n",
    "        top_1_pred = sorted_pred_distance[0]\n",
    "        \n",
    "        #subtract\n",
    "        sorted_pred_subtract_distance = np.argsort([item.cpu() for item in predicate_subtract_distances[j]])\n",
    "        top_5_pred_sub = sorted_pred_subtract_distance[:5]\n",
    "        top_1_pred_sub = sorted_pred_subtract_distance[0]\n",
    "        \n",
    "        #transe\n",
    "        sorted_transe_distances = np.argsort([item.cpu() for item in transe_distances[j]])\n",
    "        top_5_vtranse = sorted_transe_distances[:5]\n",
    "        top_1_vtranse = sorted_transe_distances[0]\n",
    "\n",
    "        #pred * transe\n",
    "        pred_transe_multiply_distances = [ item1.cpu() * item2.cpu() for item1,item2 in zip(pred_distance, transe_distances[j])]\n",
    "        top_5_pred_transe = np.argsort(pred_transe_multiply_distances[j].cpu())[:5]\n",
    "        top_1_pred_transe = np.argsort(pred_transe_multiply_distances[j].cpu())[0]\n",
    "        \n",
    "        #pred * sub\n",
    "        pred_multiply_distances = [ item1.cpu() * item2.cpu() for item1,item2 in zip(pred_distance, predicate_subtract_distances[j])]\n",
    "        top_5_pred_mul = np.argsort(pred_multiply_distances)[:5]\n",
    "        top_1_pred_mul = np.argsort(pred_multiply_distances)[0]\n",
    "\n",
    "        gt_subj = model.object_classes[relationships['subj_classes'][j]]\n",
    "        gt_pred = model.predicate_classes[relationships['pred_classes'][j]]\n",
    "        gt_obj = model.object_classes[relationships['obj_classes'][j]]\n",
    "            \n",
    "        #compute true positive\n",
    "        if (top_1_pred == relationships['pred_classes'][j]):\n",
    "            total_true_positive += 1\n",
    "        if (relationships['pred_classes'][j] in top_5_pred):\n",
    "            total_true_positive_5 += 1\n",
    "        \n",
    "        if ('{}-{}-{}'.format(gt_subj, gt_pred, gt_obj) not in trained_triples):\n",
    "            \n",
    "            #default pred\n",
    "            if (top_1_pred == relationships['pred_classes'][j]):\n",
    "                total_true_positive_zeroshot += 1\n",
    "            if (relationships['pred_classes'][j] in top_5_pred):\n",
    "                total_true_positive_5_zeroshot += 1\n",
    "                \n",
    "            #sub\n",
    "            if (top_1_pred_sub == relationships['pred_classes'][j]):\n",
    "                total_true_positive_sub_zeroshot += 1\n",
    "            if (relationships['pred_classes'][j] in top_5_pred_sub):\n",
    "                total_true_positive_5_sub_zeroshot += 1\n",
    "                \n",
    "            #mul (pred*sub)\n",
    "            if (top_1_pred_mul == relationships['pred_classes'][j]):\n",
    "                total_true_positive_mul_zeroshot += 1\n",
    "            if (relationships['pred_classes'][j] in top_5_pred_mul):\n",
    "                total_true_positive_5_mul_zeroshot += 1\n",
    "                \n",
    "            total_relationships_zeroshot += 1\n",
    "        \n",
    "        #sub\n",
    "        if (top_1_pred_sub == relationships['pred_classes'][j]):\n",
    "            total_true_positive_sub += 1\n",
    "        if (relationships['pred_classes'][j] in top_5_pred_sub):\n",
    "            total_true_positive_5_sub += 1\n",
    "        \n",
    "        #pred * sub\n",
    "        if (top_1_pred_mul == relationships['pred_classes'][j]):\n",
    "            total_true_positive_mul += 1\n",
    "        if (relationships['pred_classes'][j] in top_5_pred_mul):\n",
    "            total_true_positive_5_mul += 1\n",
    "\n",
    "        #transe\n",
    "        if (top_1_vtranse == relationships['pred_classes'][j]):\n",
    "            total_true_positive_transe += 1\n",
    "        if (relationships['pred_classes'][j] in top_5_vtranse):\n",
    "            total_true_positive_5_transe += 1\n",
    "        \n",
    "        #pred * sub\n",
    "        if (top_1_pred_transe == relationships['pred_classes'][j]):\n",
    "            total_true_positive_transe_mul += 1\n",
    "        if (relationships['pred_classes'][j] in top_5_pred_transe):\n",
    "            total_true_positive_5_transe_mul += 1\n",
    "            \n",
    "    total_relationships += len(predicate_distances)\n",
    "\n",
    "print(\"Final recall top 1: \", total_true_positive / total_relationships * 100)\n",
    "print(\"Final recall top 5: \", total_true_positive_5 / total_relationships * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d938f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_zeroshot_1 = 0\n",
    "recall_zeroshot_5 = 0\n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_zeroshot_1 = total_true_positive_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_5 = total_true_positive_5_zeroshot / total_relationships_zeroshot * 100\n",
    "\n",
    "recall_sub_1 = total_true_positive_sub / total_relationships * 100\n",
    "recall_sub_5 = total_true_positive_5_sub / total_relationships * 100\n",
    "recall_zeroshot_sub_1 = 0\n",
    "recall_zeroshot_sub_5 = 0\n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_zeroshot_sub_1 = total_true_positive_sub_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_sub_5 = total_true_positive_5_sub_zeroshot / total_relationships_zeroshot * 100        \n",
    "\n",
    "recall_mul_1 = total_true_positive_mul / total_relationships * 100\n",
    "recall_mul_5 = total_true_positive_5_mul / total_relationships * 100\n",
    "recall_zeroshot_mul_1 = 0\n",
    "recall_zeroshot_mul_5 = 0\n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_transe_mul_1 = total_true_positive_transe_mul / total_relationships * 100\n",
    "    recall_transe_mul_5 = total_true_positive_5_transe_mul / total_relationships * 100\n",
    "\n",
    "recall_1 =  total_true_positive / total_relationships * 100\n",
    "recall_5 = total_true_positive_5 / total_relationships * 100\n",
    "print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "print(\"| Recall top 1 {:5.2f} | Recall zeroshot top 1 {:5.2f} |\".format(recall_1, recall_zeroshot_1))\n",
    "print(\"| Recall top 5 {:5.2f} | Recall zeroshot top 5 {:5.2f} |\".format(recall_5, recall_zeroshot_5))\n",
    "print(\"| Recall sub top 1 {:5.2f} | Recall zeroshot sub top 1 {:5.2f} |\".format(recall_sub_1, recall_zeroshot_sub_1))\n",
    "print(\"| Recall sub top 5 {:5.2f} | Recall zeroshot sub top 5 {:5.2f} |\".format(recall_sub_5, recall_zeroshot_sub_5))\n",
    "print(\"| Recall mul top 1 {:5.2f} | Recall zeroshot mul top 1 {:5.2f} |\".format(recall_mul_1, recall_zeroshot_mul_1))\n",
    "print(\"| Recall mul top 5 {:5.2f} | Recall zeroshot mul top 5 {:5.2f} |\".format(recall_mul_5, recall_zeroshot_mul_5))        \n",
    "print(\"| Recall transe 1 {:5.2f} | Recall transe mul 1 {:5.2f} |\".format(recall_transe_1, recall_transe_mul_1))\n",
    "print(\"| Recall transe 5 {:5.2f} | Recall transe mul 5 {:5.2f} |\".format(recall_transe_5, recall_transe_mul_5))\n",
    "recall_zeroshot_mul_1 = total_true_positive_mul_zeroshot / total_relationships_zeroshot * 100\n",
    "recall_zeroshot_mul_5 = total_true_positive_5_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "\n",
    "recall_transe_1 = total_true_positive_transe / total_relationships * 100\n",
    "recall_transe_5 = total_true_positive_5_transe / total_relationships * 100\n",
    "\n",
    "recall_transe_mul_1 = total_true_positive_transe_mul / total_relationships * 100\n",
    "recall_transe_mul_5 = total_true_positive_5_transe_mul / total_relationships * 100\n",
    "\n",
    "recall_1 =  total_true_positive / total_relationships * 100\n",
    "recall_5 = total_true_positive_5 / total_relationships * 100\n",
    "print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "print(\"| Recall top 1 {:5.2f} | Recall zeroshot top 1 {:5.2f} |\".format(recall_1, recall_zeroshot_1))\n",
    "print(\"| Recall top 5 {:5.2f} | Recall zeroshot top 5 {:5.2f} |\".format(recall_5, recall_zeroshot_5))\n",
    "print(\"| Recall sub top 1 {:5.2f} | Recall zeroshot sub top 1 {:5.2f} |\".format(recall_sub_1, recall_zeroshot_sub_1))\n",
    "print(\"| Recall sub top 5 {:5.2f} | Recall zeroshot sub top 5 {:5.2f} |\".format(recall_sub_5, recall_zeroshot_sub_5))\n",
    "print(\"| Recall mul top 1 {:5.2f} | Recall zeroshot mul top 1 {:5.2f} |\".format(recall_mul_1, recall_zeroshot_mul_1))\n",
    "print(\"| Recall mul top 5 {:5.2f} | Recall zeroshot mul top 5 {:5.2f} |\".format(recall_mul_5, recall_zeroshot_mul_5))        \n",
    "print(\"| Recall transe 1 {:5.2f} | Recall transe mul 1 {:5.2f} |\".format(recall_transe_1, recall_transe_mul_1))\n",
    "print(\"| Recall transe 5 {:5.2f} | Recall transe mul 5 {:5.2f} |\".format(recall_transe_5, recall_transe_mul_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b49ed3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evalutation of the models performance on detective visual object (no TransE)\n",
    "import numpy as np\n",
    "#total\n",
    "total_relationships = 0\n",
    "total_relationships_zeroshot = 0\n",
    "\n",
    "#default\n",
    "total_true_positive = 0\n",
    "total_true_positive_5 = 0\n",
    "total_true_positive_10 = 0\n",
    "\n",
    "total_true_positive_zeroshot = 0\n",
    "total_true_positive_5_zeroshot = 0\n",
    "total_true_positive_10_zeroshot = 0\n",
    "\n",
    "#add\n",
    "total_true_positive_add = 0\n",
    "total_true_positive_5_add = 0\n",
    "total_true_positive_10_add = 0\n",
    "\n",
    "total_true_positive_add_zeroshot = 0\n",
    "total_true_positive_5_add_zeroshot = 0\n",
    "total_true_positive_10_add_zeroshot = 0\n",
    "\n",
    "#mul\n",
    "total_true_positive_mul = 0\n",
    "total_true_positive_5_mul = 0\n",
    "total_true_positive_10_mul = 0\n",
    "\n",
    "total_true_positive_mul_zeroshot = 0\n",
    "total_true_positive_5_mul_zeroshot = 0\n",
    "total_true_positive_10_mul_zeroshot = 0\n",
    "\n",
    "for i in range(n_examples):\n",
    "    data = next(test_data_iter)[0]\n",
    "    relationships = data[\"relationships\"]\n",
    "    with torch.no_grad():\n",
    "        object_distances, object_add_distances = model.get_object_distances(data)\n",
    "        \n",
    "    #Verbose\n",
    "    if (i % 20 == 0 and i > 0):\n",
    "        recall_1 =  total_true_positive / total_relationships * 100\n",
    "        recall_5 = total_true_positive_5 / total_relationships * 100\n",
    "        recall_10 = total_true_positive_10 / total_relationships * 100\n",
    "        recall_zeroshot_1 = 0\n",
    "        recall_zeroshot_5 = 0\n",
    "        recall_zeroshot_10 = 0\n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_1 = total_true_positive_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_5 = total_true_positive_5_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_10 = total_true_positive_10_zeroshot / total_relationships_zeroshot * 100\n",
    "\n",
    "        recall_add_1 = total_true_positive_add / total_relationships * 100\n",
    "        recall_add_5 = total_true_positive_5_add / total_relationships * 100\n",
    "        recall_add_10 = total_true_positive_10_add / total_relationships * 100\n",
    "        recall_zeroshot_add_1 = 0\n",
    "        recall_zeroshot_add_5 = 0\n",
    "        recall_zeroshot_add_10 = 0        \n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_add_1 = total_true_positive_add_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_add_5 = total_true_positive_5_add_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_add_10 = total_true_positive_10_add_zeroshot / total_relationships_zeroshot * 100\n",
    "        \n",
    "        recall_mul_1 = total_true_positive_mul / total_relationships * 100\n",
    "        recall_mul_5 = total_true_positive_5_mul / total_relationships * 100\n",
    "        recall_mul_10 = total_true_positive_10_mul / total_relationships * 100\n",
    "        recall_zeroshot_mul_1 = 0\n",
    "        recall_zeroshot_mul_5 = 0\n",
    "        recall_zeroshot_mul_10 = 0\n",
    "        if (total_relationships_zeroshot > 0):\n",
    "            recall_zeroshot_mul_1 = total_true_positive_mul_zeroshot / total_relationships_zeroshot * 100\n",
    "            recall_zeroshot_mul_5 = total_true_positive_5_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "            recall_zeroshot_mul_10 = total_true_positive_10_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "        \n",
    "        print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "        print(\"| Recall top 1 {:5.2f} | Recall zeroshot top 1 {:5.2f} |\".format(recall_1, recall_zeroshot_1))\n",
    "        print(\"| Recall top 5 {:5.2f} | Recall zeroshot top 5 {:5.2f} |\".format(recall_5, recall_zeroshot_5))\n",
    "        print(\"| Recall top 10 {:5.2f} | Recall zeroshot top 10 {:5.2f} |\".format(recall_10, recall_zeroshot_10))\n",
    "        print(\"| Recall add top 1 {:5.2f} | Recall zeroshot sub top 1 {:5.2f} |\".format(recall_add_1, recall_zeroshot_add_1))\n",
    "        print(\"| Recall add top 5 {:5.2f} | Recall zeroshot sub top 5 {:5.2f} |\".format(recall_add_5, recall_zeroshot_add_5))\n",
    "        print(\"| Recall add top 10 {:5.2f} | Recall zeroshot sub top 10 {:5.2f} |\".format(recall_add_10, recall_zeroshot_add_10))\n",
    "        print(\"| Recall mul top 1 {:5.2f} | Recall zeroshot mul top 1 {:5.2f} |\".format(recall_mul_1, recall_zeroshot_mul_1))\n",
    "        print(\"| Recall mul top 5 {:5.2f} | Recall zeroshot mul top 5 {:5.2f} |\".format(recall_mul_5, recall_zeroshot_mul_5))        \n",
    "        print(\"| Recall mul top 10 {:5.2f} | Recall zeroshot mul top 10 {:5.2f} |\".format(recall_mul_10, recall_zeroshot_mul_10))        \n",
    "\n",
    "    #select top 5 to calculate recall\n",
    "    for j, obj_distance in enumerate(object_distances):\n",
    "        top_1_obj = np.argsort(obj_distance)[0]\n",
    "        top_5_obj = np.argsort(obj_distance)[:5]\n",
    "        top_10_obj = np.argsort(obj_distance)[:10]\n",
    "        \n",
    "        #add\n",
    "        top_1_obj_add = np.argsort(object_add_distances[j])[0]\n",
    "        top_5_obj_add = np.argsort(object_add_distances[j])[:5]\n",
    "        top_10_obj_add = np.argsort(object_add_distances[j])[:10]\n",
    "        \n",
    "        #pred * add\n",
    "        obj_multiply_distances = [ item1 * item2 for item1,item2 in zip(obj_distance, object_add_distances[j])]\n",
    "        top_1_obj_mul = np.argsort(obj_multiply_distances)[0]\n",
    "        top_5_obj_mul = np.argsort(obj_multiply_distances)[:5]\n",
    "        top_10_obj_mul = np.argsort(obj_multiply_distances)[:10]\n",
    "\n",
    "        gt_subj = model.object_classes[relationships['subj_classes'][j]]\n",
    "        gt_pred = model.predicate_classes[relationships['pred_classes'][j]]\n",
    "        gt_obj = model.object_classes[relationships['obj_classes'][j]]\n",
    "            \n",
    "        #compute true positive        \n",
    "        if ('{}-{}-{}'.format(gt_subj, gt_pred, gt_obj) not in trained_triples):\n",
    "            \n",
    "            #default pred\n",
    "            if (top_1_obj == relationships['obj_classes'][j]):\n",
    "                total_true_positive_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_5_obj):\n",
    "                total_true_positive_5_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_10_obj):\n",
    "                total_true_positive_10_zeroshot += 1\n",
    "\n",
    "            #sub\n",
    "            if (top_1_obj_add == relationships['obj_classes'][j]):\n",
    "                total_true_positive_add_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_5_obj_add):\n",
    "                total_true_positive_5_add_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_10_obj_add):\n",
    "                total_true_positive_10_add_zeroshot += 1\n",
    "                \n",
    "            #mul (pred*sub)\n",
    "            if (top_1_obj_mul == relationships['obj_classes'][j]):\n",
    "                total_true_positive_mul_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_5_obj_mul):\n",
    "                total_true_positive_5_mul_zeroshot += 1\n",
    "            if (relationships['obj_classes'][j] in top_10_obj_mul):\n",
    "                total_true_positive_10_mul_zeroshot += 1\n",
    "                \n",
    "            total_relationships_zeroshot += 1\n",
    "        \n",
    "        #normal\n",
    "        if (top_1_obj == relationships['obj_classes'][j]):\n",
    "            total_true_positive += 1\n",
    "        if (relationships['obj_classes'][j] in top_5_obj):\n",
    "            total_true_positive_5 += 1\n",
    "        if (relationships['obj_classes'][j] in top_10_obj):\n",
    "            total_true_positive_10 += 1\n",
    "            \n",
    "        #add\n",
    "        if (top_1_obj_add == relationships['obj_classes'][j]):\n",
    "            total_true_positive_add += 1\n",
    "        if (relationships['obj_classes'][j] in top_5_obj_add):\n",
    "            total_true_positive_5_add += 1\n",
    "        if (relationships['obj_classes'][j] in top_10_obj_add):\n",
    "            total_true_positive_10_add += 1\n",
    "        \n",
    "        #pred * add\n",
    "        if (top_1_obj_mul == relationships['obj_classes'][j]):\n",
    "            total_true_positive_mul += 1\n",
    "        if (relationships['obj_classes'][j] in top_5_obj_mul):\n",
    "            total_true_positive_5_mul += 1\n",
    "        if (relationships['obj_classes'][j] in top_10_obj_mul):\n",
    "            total_true_positive_10_mul += 1\n",
    "    total_relationships += len(object_distances)\n",
    "\n",
    "print(\"Final recall top 1: \", total_true_positive / total_relationships * 100)\n",
    "print(\"Final recall top 5: \", total_true_positive_5 / total_relationships * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa87a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_1 =  total_true_positive / total_relationships * 100\n",
    "recall_5 = total_true_positive_5 / total_relationships * 100\n",
    "recall_10 = total_true_positive_10 / total_relationships * 100\n",
    "recall_zeroshot_1 = 0\n",
    "recall_zeroshot_5 = 0\n",
    "recall_zeroshot_10 = 0\n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_zeroshot_1 = total_true_positive_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_5 = total_true_positive_5_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_10 = total_true_positive_10_zeroshot / total_relationships_zeroshot * 100\n",
    "\n",
    "recall_add_1 = total_true_positive_add / total_relationships * 100\n",
    "recall_add_5 = total_true_positive_5_add / total_relationships * 100\n",
    "recall_add_10 = total_true_positive_10_add / total_relationships * 100\n",
    "recall_zeroshot_add_1 = 0\n",
    "recall_zeroshot_add_5 = 0\n",
    "recall_zeroshot_add_10 = 0        \n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_zeroshot_add_1 = total_true_positive_add_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_add_5 = total_true_positive_5_add_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_add_10 = total_true_positive_10_add_zeroshot / total_relationships_zeroshot * 100\n",
    "\n",
    "recall_mul_1 = total_true_positive_mul / total_relationships * 100\n",
    "recall_mul_5 = total_true_positive_5_mul / total_relationships * 100\n",
    "recall_mul_10 = total_true_positive_10_mul / total_relationships * 100\n",
    "recall_zeroshot_mul_1 = 0\n",
    "recall_zeroshot_mul_5 = 0\n",
    "recall_zeroshot_mul_10 = 0\n",
    "if (total_relationships_zeroshot > 0):\n",
    "    recall_zeroshot_mul_1 = total_true_positive_mul_zeroshot / total_relationships_zeroshot * 100\n",
    "    recall_zeroshot_mul_5 = total_true_positive_5_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "    recall_zeroshot_mul_10 = total_true_positive_10_mul_zeroshot / total_relationships_zeroshot * 100        \n",
    "\n",
    "print(\"|----------------------Iter {}------------------------|\".format(i))\n",
    "print(\"| Recall top 1 {:5.2f} | Recall zeroshot top 1 {:5.2f} |\".format(recall_1, recall_zeroshot_1))\n",
    "print(\"| Recall top 5 {:5.2f} | Recall zeroshot top 5 {:5.2f} |\".format(recall_5, recall_zeroshot_5))\n",
    "print(\"| Recall top 10 {:5.2f} | Recall zeroshot top 10 {:5.2f} |\".format(recall_10, recall_zeroshot_10))\n",
    "print(\"| Recall add top 1 {:5.2f} | Recall zeroshot sub top 1 {:5.2f} |\".format(recall_add_1, recall_zeroshot_add_1))\n",
    "print(\"| Recall add top 5 {:5.2f} | Recall zeroshot sub top 5 {:5.2f} |\".format(recall_add_5, recall_zeroshot_add_5))\n",
    "print(\"| Recall add top 10 {:5.2f} | Recall zeroshot sub top 10 {:5.2f} |\".format(recall_add_10, recall_zeroshot_add_10))\n",
    "print(\"| Recall mul top 1 {:5.2f} | Recall zeroshot mul top 1 {:5.2f} |\".format(recall_mul_1, recall_zeroshot_mul_1))\n",
    "print(\"| Recall mul top 5 {:5.2f} | Recall zeroshot mul top 5 {:5.2f} |\".format(recall_mul_5, recall_zeroshot_mul_5))        \n",
    "print(\"| Recall mul top 10 {:5.2f} | Recall zeroshot mul top 10 {:5.2f} |\".format(recall_mul_10, recall_zeroshot_mul_10))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_dataloader)\n",
    "data = next(data_iter)[0]\n",
    "# model = RelTransR(cfg)\n",
    "# negative_examples = model.generate_negative_examples(data)\n",
    "# results = model(data,negative_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130cd43b",
   "metadata": {},
   "source": [
    "# Training Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ab66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import (\n",
    "    DatasetCatalog, DatasetMapper,\n",
    "    build_detection_train_loader\n",
    ")\n",
    "from config import get_vrd_cfg\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "cfg = get_vrd_cfg()\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .7\n",
    "batch_size = 1\n",
    "\n",
    "# Dataloaders\n",
    "if ('test_dataloader' in vars()):\n",
    "    del test_dataloader\n",
    "\n",
    "train_dataloader = build_detection_train_loader(cfg,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f29772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetCatalog.get(\"vrd_train\")\n",
    "train_dataloader = build_detection_train_loader(cfg,dataset=train_dataset,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59bbfb89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#params\n",
    "n_datapoints = len(train_dataloader.dataset.dataset.dataset)\n",
    "# n_datapoints = 3780\n",
    "n_iters = cfg.SOLVER.MAX_ITER\n",
    "num_epochs = int(n_iters / n_datapoints)\n",
    "chkpoint_it = n_datapoints #create a checkpoint every 1000 iterations\n",
    "\n",
    "#model\n",
    "model = RelTransR(cfg)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "#scheduler\n",
    "learning_rate = 0.001\n",
    "gamma = cfg.SOLVER.GAMMA\n",
    "momentum = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "scheduler = StepLR(optimizer, step_size=4, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "019fbd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RelTransR(cfg)\n",
    "# device = torch.device(\"cuda\")\n",
    "# model.to(device)\n",
    "# wandb.watch(model, log=\"all\", log_freq=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38352314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mherobaby71\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/herobaby71/vrdtransr-project-vltranse-lang-con-2/runs/sasyf973\" target=\"_blank\">neat-pyramid-3</a></strong> to <a href=\"https://wandb.ai/herobaby71/vrdtransr-project-vltranse-lang-con-2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wandb\n",
    "log_interval = 20\n",
    "wandb.init(project=\"vrdtransr-project-vltranse-lang-con-2\", entity=\"herobaby71\")\n",
    "wandb.config = {\n",
    "    \"seed\": 0,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"n_iters\": n_iters,\n",
    "    \"batch_size\": 1\n",
    "}\n",
    "wandb.watch(model, log=\"all\", log_freq=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be4396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, lr, it, epoch, loss_subj, loss_pred, loss_obj, loss_transr):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"lr\":lr, \"epoch\": epoch, \"loss\": loss, \"loss_subj\": loss_subj, \"loss_pred\": loss_pred, \"loss_obj\": loss_obj, \"loss_transr\":loss_transr}, step=it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa65b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 2.142324GB\n",
      "torch.cuda.memory_reserved: 2.193359GB\n",
      "torch.cuda.free: 0.051035GB\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.free: %fGB\"%(f/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49a1b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "#checkpoint every 2000 steps\n",
    "chkpoint_it = 2000\n",
    "\n",
    "it = 0\n",
    "initial_it = 0 # checkpoint intial iteration to resume training\n",
    "start_time = time.time()\n",
    "losses = []\n",
    "\n",
    "#load checkpoint\n",
    "load_chkpoint = False\n",
    "if (load_chkpoint):\n",
    "    chkpoint_path = '../checkpoint/'\n",
    "    model_name = 'vrd2_model_18000.pt'\n",
    "    chkpoint_full_path = os.path.join(chkpoint_path, model_name)\n",
    "    it, start_epoch, losses = load_checkpoint(model, chkpoint_full_path, optimizer=optimizer)\n",
    "    initial_it = it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55a04184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "trained_triples_path = '../generated/trained_triples.json'\n",
    "trained_triples = {}\n",
    "if (os.path.exists(trained_triples_path)):\n",
    "    with open(trained_triples_path) as file:\n",
    "        trained_triples = json.load(file)\n",
    "else:\n",
    "    iter_dataloader = iter(train_dataloader)\n",
    "    n_iters = len(train_dataloader.dataset.dataset)\n",
    "    for i in range(n_iters):\n",
    "        print(i)\n",
    "        data = next(iter_dataloader)[0]\n",
    "        relationships = data['relationships']\n",
    "        for j in range(len(relationships['subj_classes'])):\n",
    "            subj_cls = model.object_classes[relationships['subj_classes'][j]]\n",
    "            pred_cls = model.predicate_classes[relationships['pred_classes'][j]]\n",
    "            obj_cls = model.object_classes[relationships['obj_classes'][j]]\n",
    "            trained_triples['{}-{}-{}'.format(subj_cls, pred_cls, obj_cls)] = 1\n",
    "\n",
    "    with open(trained_triples_path, 'w') as file:\n",
    "        file.write(json.dumps(trained_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "557ea1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| it 2000 | epoch 0 | lr 0.001 | ms/batch 47845.62 | loss  0.20\n",
      "an image has been removed for this batch\n",
      "| it 2020 | epoch 0 | lr 0.001 | ms/batch 1189.71 | loss  0.22\n",
      "an image has been removed for this batch\n",
      "| it 2040 | epoch 0 | lr 0.001 | ms/batch 1061.65 | loss  0.19\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2060 | epoch 0 | lr 0.001 | ms/batch 1023.86 | loss  0.20\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2080 | epoch 0 | lr 0.001 | ms/batch 1057.20 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2100 | epoch 0 | lr 0.001 | ms/batch 1030.11 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "| it 2120 | epoch 0 | lr 0.001 | ms/batch 1053.40 | loss  0.22\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2140 | epoch 0 | lr 0.001 | ms/batch 1016.23 | loss  0.18\n",
      "| it 2160 | epoch 0 | lr 0.001 | ms/batch 1035.69 | loss  0.21\n",
      "an image has been removed for this batch\n",
      "| it 2180 | epoch 0 | lr 0.001 | ms/batch 1078.68 | loss  0.20\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2200 | epoch 0 | lr 0.001 | ms/batch 1047.80 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2220 | epoch 0 | lr 0.001 | ms/batch 1018.25 | loss  0.23\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2240 | epoch 0 | lr 0.001 | ms/batch 1012.62 | loss  0.20\n",
      "an image has been removed for this batch\n",
      "| it 2260 | epoch 0 | lr 0.001 | ms/batch 1038.51 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2280 | epoch 0 | lr 0.001 | ms/batch 1059.78 | loss  0.19\n",
      "an image has been removed for this batch\n",
      "| it 2300 | epoch 0 | lr 0.001 | ms/batch 1019.12 | loss  0.15\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2320 | epoch 0 | lr 0.001 | ms/batch 1060.98 | loss  0.19\n",
      "an image has been removed for this batch\n",
      "| it 2340 | epoch 0 | lr 0.001 | ms/batch 993.98 | loss  0.19\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2360 | epoch 0 | lr 0.001 | ms/batch 1016.72 | loss  0.22\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2380 | epoch 0 | lr 0.001 | ms/batch 1019.55 | loss  0.20\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2400 | epoch 0 | lr 0.001 | ms/batch 1084.37 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2420 | epoch 0 | lr 0.001 | ms/batch 1033.01 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "| it 2440 | epoch 0 | lr 0.001 | ms/batch 1031.73 | loss  0.19\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2460 | epoch 0 | lr 0.001 | ms/batch 962.17 | loss  0.19\n",
      "| it 2480 | epoch 0 | lr 0.001 | ms/batch 1045.31 | loss  0.17\n",
      "| it 2500 | epoch 0 | lr 0.001 | ms/batch 988.97 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2520 | epoch 0 | lr 0.001 | ms/batch 946.30 | loss  0.21\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2540 | epoch 0 | lr 0.001 | ms/batch 1021.04 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "| it 2560 | epoch 0 | lr 0.001 | ms/batch 1011.79 | loss  0.15\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2580 | epoch 0 | lr 0.001 | ms/batch 1003.22 | loss  0.19\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2600 | epoch 0 | lr 0.001 | ms/batch 1043.49 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2620 | epoch 0 | lr 0.001 | ms/batch 1036.54 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2640 | epoch 0 | lr 0.001 | ms/batch 1051.61 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2660 | epoch 0 | lr 0.001 | ms/batch 1046.28 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2680 | epoch 0 | lr 0.001 | ms/batch 1046.74 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "| it 2700 | epoch 0 | lr 0.001 | ms/batch 1097.97 | loss  0.16\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2720 | epoch 0 | lr 0.001 | ms/batch 1051.41 | loss  0.19\n",
      "| it 2740 | epoch 0 | lr 0.001 | ms/batch 1035.41 | loss  0.18\n",
      "| it 2760 | epoch 0 | lr 0.001 | ms/batch 971.61 | loss  0.19\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2780 | epoch 0 | lr 0.001 | ms/batch 998.38 | loss  0.19\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2800 | epoch 0 | lr 0.001 | ms/batch 1016.06 | loss  0.20\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2820 | epoch 0 | lr 0.001 | ms/batch 1008.65 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2840 | epoch 0 | lr 0.001 | ms/batch 1067.83 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2860 | epoch 0 | lr 0.001 | ms/batch 1044.99 | loss  0.17\n",
      "| it 2880 | epoch 0 | lr 0.001 | ms/batch 1062.92 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2900 | epoch 0 | lr 0.001 | ms/batch 1024.71 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2920 | epoch 0 | lr 0.001 | ms/batch 984.89 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2940 | epoch 0 | lr 0.001 | ms/batch 1055.33 | loss  0.16\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2960 | epoch 0 | lr 0.001 | ms/batch 1078.01 | loss  0.16\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 2980 | epoch 0 | lr 0.001 | ms/batch 1072.75 | loss  0.16\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3000 | epoch 0 | lr 0.001 | ms/batch 964.45 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3020 | epoch 0 | lr 0.001 | ms/batch 1054.14 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3040 | epoch 0 | lr 0.001 | ms/batch 1026.93 | loss  0.19\n",
      "an image has been removed for this batch\n",
      "| it 3060 | epoch 0 | lr 0.001 | ms/batch 1040.56 | loss  0.17\n",
      "| it 3080 | epoch 0 | lr 0.001 | ms/batch 1068.21 | loss  0.16\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an image has been removed for this batch\n",
      "| it 3100 | epoch 0 | lr 0.001 | ms/batch 1041.84 | loss  0.15\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3120 | epoch 0 | lr 0.001 | ms/batch 1030.97 | loss  0.21\n",
      "an image has been removed for this batch\n",
      "| it 3140 | epoch 0 | lr 0.001 | ms/batch 1068.13 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "| it 3160 | epoch 0 | lr 0.001 | ms/batch 1078.71 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3180 | epoch 0 | lr 0.001 | ms/batch 1049.23 | loss  0.13\n",
      "an image has been removed for this batch\n",
      "| it 3200 | epoch 0 | lr 0.001 | ms/batch 1036.07 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3220 | epoch 0 | lr 0.001 | ms/batch 1059.68 | loss  0.20\n",
      "| it 3240 | epoch 0 | lr 0.001 | ms/batch 1061.62 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3260 | epoch 0 | lr 0.001 | ms/batch 1090.90 | loss  0.16\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3280 | epoch 0 | lr 0.001 | ms/batch 992.74 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3300 | epoch 0 | lr 0.001 | ms/batch 1005.63 | loss  0.13\n",
      "| it 3320 | epoch 0 | lr 0.001 | ms/batch 1018.28 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3340 | epoch 0 | lr 0.001 | ms/batch 1099.66 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3360 | epoch 0 | lr 0.001 | ms/batch 1085.05 | loss  0.16\n",
      "an image has been removed for this batch\n",
      "| it 3380 | epoch 0 | lr 0.001 | ms/batch 982.08 | loss  0.18\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3400 | epoch 0 | lr 0.001 | ms/batch 1014.16 | loss  0.15\n",
      "| it 3420 | epoch 0 | lr 0.001 | ms/batch 1031.59 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "| it 3440 | epoch 0 | lr 0.001 | ms/batch 1008.00 | loss  0.14\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3460 | epoch 0 | lr 0.001 | ms/batch 1075.18 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3480 | epoch 0 | lr 0.001 | ms/batch 1023.95 | loss  0.15\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3500 | epoch 0 | lr 0.001 | ms/batch 1048.01 | loss  0.13\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3520 | epoch 0 | lr 0.001 | ms/batch 982.06 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3540 | epoch 0 | lr 0.001 | ms/batch 1074.67 | loss  0.15\n",
      "an image has been removed for this batch\n",
      "| it 3560 | epoch 0 | lr 0.001 | ms/batch 1017.48 | loss  0.20\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3580 | epoch 0 | lr 0.001 | ms/batch 1076.24 | loss  0.17\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3600 | epoch 0 | lr 0.001 | ms/batch 1070.21 | loss  0.16\n",
      "an image has been removed for this batch\n",
      "| it 3620 | epoch 0 | lr 0.001 | ms/batch 1091.39 | loss  0.14\n",
      "an image has been removed for this batch\n",
      "| it 3640 | epoch 0 | lr 0.001 | ms/batch 1057.44 | loss  0.15\n",
      "an image has been removed for this batch\n",
      "| it 3660 | epoch 0 | lr 0.001 | ms/batch 1058.15 | loss  0.11\n",
      "an image has been removed for this batch\n",
      "| it 3680 | epoch 0 | lr 0.001 | ms/batch 1093.09 | loss  0.13\n",
      "| it 3700 | epoch 0 | lr 0.001 | ms/batch 1034.04 | loss  0.10\n",
      "an image has been removed for this batch\n",
      "| it 3720 | epoch 0 | lr 0.001 | ms/batch 1059.22 | loss  0.10\n",
      "an image has been removed for this batch\n",
      "| it 3740 | epoch 0 | lr 0.001 | ms/batch 1061.42 | loss  0.15\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3760 | epoch 0 | lr 0.001 | ms/batch 1064.40 | loss  0.13\n",
      "an image has been removed for this batch\n",
      "| it 3780 | epoch 0 | lr 0.001 | ms/batch 1062.84 | loss  0.16\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3800 | epoch 0 | lr 0.001 | ms/batch 1089.22 | loss  0.10\n",
      "an image has been removed for this batch\n",
      "| it 3820 | epoch 0 | lr 0.001 | ms/batch 1089.98 | loss  0.07\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3840 | epoch 0 | lr 0.001 | ms/batch 1014.64 | loss  0.08\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3860 | epoch 0 | lr 0.001 | ms/batch 1023.91 | loss  0.09\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3880 | epoch 0 | lr 0.001 | ms/batch 1040.84 | loss  0.07\n",
      "| it 3900 | epoch 0 | lr 0.001 | ms/batch 1072.04 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3920 | epoch 0 | lr 0.001 | ms/batch 1030.47 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3940 | epoch 0 | lr 0.001 | ms/batch 1067.07 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3960 | epoch 0 | lr 0.001 | ms/batch 1059.14 | loss  0.10\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 3980 | epoch 0 | lr 0.001 | ms/batch 1037.26 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4000 | epoch 1 | lr 0.001 | ms/batch 1059.69 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4020 | epoch 1 | lr 0.001 | ms/batch 1295.54 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "| it 4040 | epoch 1 | lr 0.001 | ms/batch 1047.24 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "| it 4060 | epoch 1 | lr 0.001 | ms/batch 1042.38 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4080 | epoch 1 | lr 0.001 | ms/batch 1027.11 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "| it 4100 | epoch 1 | lr 0.001 | ms/batch 1027.53 | loss  0.02\n",
      "| it 4120 | epoch 1 | lr 0.001 | ms/batch 1060.80 | loss  0.03\n",
      "an image has been removed for this batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| it 4140 | epoch 1 | lr 0.001 | ms/batch 1025.75 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4160 | epoch 1 | lr 0.001 | ms/batch 1080.34 | loss  0.06\n",
      "an image has been removed for this batch\n",
      "| it 4180 | epoch 1 | lr 0.001 | ms/batch 1092.91 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4200 | epoch 1 | lr 0.001 | ms/batch 1075.48 | loss  0.08\n",
      "| it 4220 | epoch 1 | lr 0.001 | ms/batch 1020.23 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "| it 4240 | epoch 1 | lr 0.001 | ms/batch 1060.29 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4260 | epoch 1 | lr 0.001 | ms/batch 1007.01 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4280 | epoch 1 | lr 0.001 | ms/batch 1091.82 | loss  0.04\n",
      "| it 4300 | epoch 1 | lr 0.001 | ms/batch 1051.16 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4320 | epoch 1 | lr 0.001 | ms/batch 1055.69 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4340 | epoch 1 | lr 0.001 | ms/batch 1060.22 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "| it 4360 | epoch 1 | lr 0.001 | ms/batch 1070.90 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4380 | epoch 1 | lr 0.001 | ms/batch 1035.33 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4400 | epoch 1 | lr 0.001 | ms/batch 1061.66 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "| it 4420 | epoch 1 | lr 0.001 | ms/batch 1018.73 | loss  0.09\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4440 | epoch 1 | lr 0.001 | ms/batch 1064.60 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4460 | epoch 1 | lr 0.001 | ms/batch 1056.22 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4480 | epoch 1 | lr 0.001 | ms/batch 1059.81 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4500 | epoch 1 | lr 0.001 | ms/batch 978.91 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4520 | epoch 1 | lr 0.001 | ms/batch 981.09 | loss  0.09\n",
      "an image has been removed for this batch\n",
      "| it 4540 | epoch 1 | lr 0.001 | ms/batch 1082.97 | loss  0.05\n",
      "| it 4560 | epoch 1 | lr 0.001 | ms/batch 1069.08 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4580 | epoch 1 | lr 0.001 | ms/batch 1040.64 | loss  0.02\n",
      "| it 4600 | epoch 1 | lr 0.001 | ms/batch 1004.57 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4620 | epoch 1 | lr 0.001 | ms/batch 1061.09 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "| it 4640 | epoch 1 | lr 0.001 | ms/batch 1078.23 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4660 | epoch 1 | lr 0.001 | ms/batch 1013.94 | loss  0.06\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4680 | epoch 1 | lr 0.001 | ms/batch 1069.13 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4700 | epoch 1 | lr 0.001 | ms/batch 1023.53 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4720 | epoch 1 | lr 0.001 | ms/batch 1035.38 | loss  0.09\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4740 | epoch 1 | lr 0.001 | ms/batch 1056.23 | loss  0.07\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4760 | epoch 1 | lr 0.001 | ms/batch 1075.27 | loss  0.06\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4780 | epoch 1 | lr 0.001 | ms/batch 1082.74 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4800 | epoch 1 | lr 0.001 | ms/batch 1081.00 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4820 | epoch 1 | lr 0.001 | ms/batch 1062.12 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4840 | epoch 1 | lr 0.001 | ms/batch 1044.50 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4860 | epoch 1 | lr 0.001 | ms/batch 1057.78 | loss  0.09\n",
      "an image has been removed for this batch\n",
      "| it 4880 | epoch 1 | lr 0.001 | ms/batch 1068.89 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "| it 4900 | epoch 1 | lr 0.001 | ms/batch 1061.91 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "| it 4920 | epoch 1 | lr 0.001 | ms/batch 1044.37 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "| it 4940 | epoch 1 | lr 0.001 | ms/batch 1069.96 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4960 | epoch 1 | lr 0.001 | ms/batch 1010.79 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 4980 | epoch 1 | lr 0.001 | ms/batch 1075.83 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5000 | epoch 1 | lr 0.001 | ms/batch 1074.81 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5020 | epoch 1 | lr 0.001 | ms/batch 1058.06 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5040 | epoch 1 | lr 0.001 | ms/batch 1083.56 | loss  0.08\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5060 | epoch 1 | lr 0.001 | ms/batch 1074.18 | loss  0.06\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5080 | epoch 1 | lr 0.001 | ms/batch 1072.23 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5100 | epoch 1 | lr 0.001 | ms/batch 1037.15 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5120 | epoch 1 | lr 0.001 | ms/batch 1065.90 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5140 | epoch 1 | lr 0.001 | ms/batch 1086.64 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5160 | epoch 1 | lr 0.001 | ms/batch 1068.21 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5180 | epoch 1 | lr 0.001 | ms/batch 1050.69 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "| it 5200 | epoch 1 | lr 0.001 | ms/batch 1070.28 | loss  0.04\n",
      "an image has been removed for this batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an image has been removed for this batch\n",
      "| it 5220 | epoch 1 | lr 0.001 | ms/batch 1092.63 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5240 | epoch 1 | lr 0.001 | ms/batch 1068.93 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5260 | epoch 1 | lr 0.001 | ms/batch 1076.47 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5280 | epoch 1 | lr 0.001 | ms/batch 1065.89 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5300 | epoch 1 | lr 0.001 | ms/batch 1023.64 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5320 | epoch 1 | lr 0.001 | ms/batch 1066.03 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5340 | epoch 1 | lr 0.001 | ms/batch 1059.47 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5360 | epoch 1 | lr 0.001 | ms/batch 1040.47 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5380 | epoch 1 | lr 0.001 | ms/batch 1073.43 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5400 | epoch 1 | lr 0.001 | ms/batch 1090.07 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "| it 5420 | epoch 1 | lr 0.001 | ms/batch 1082.39 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5440 | epoch 1 | lr 0.001 | ms/batch 1054.71 | loss  0.03\n",
      "| it 5460 | epoch 1 | lr 0.001 | ms/batch 1054.75 | loss  0.02\n",
      "| it 5480 | epoch 1 | lr 0.001 | ms/batch 1071.53 | loss  0.09\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5500 | epoch 1 | lr 0.001 | ms/batch 1071.11 | loss  0.03\n",
      "| it 5520 | epoch 1 | lr 0.001 | ms/batch 1045.39 | loss  0.04\n",
      "| it 5540 | epoch 1 | lr 0.001 | ms/batch 1057.76 | loss  0.03\n",
      "| it 5560 | epoch 1 | lr 0.001 | ms/batch 1060.42 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5580 | epoch 1 | lr 0.001 | ms/batch 1058.03 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5600 | epoch 1 | lr 0.001 | ms/batch 1073.62 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5620 | epoch 1 | lr 0.001 | ms/batch 1069.79 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "| it 5640 | epoch 1 | lr 0.001 | ms/batch 1051.08 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5660 | epoch 1 | lr 0.001 | ms/batch 1012.75 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5680 | epoch 1 | lr 0.001 | ms/batch 1019.38 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5700 | epoch 1 | lr 0.001 | ms/batch 1075.32 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5720 | epoch 1 | lr 0.001 | ms/batch 1089.29 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5740 | epoch 1 | lr 0.001 | ms/batch 1070.59 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5760 | epoch 1 | lr 0.001 | ms/batch 1072.08 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5780 | epoch 1 | lr 0.001 | ms/batch 1049.88 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5800 | epoch 1 | lr 0.001 | ms/batch 1062.86 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5820 | epoch 1 | lr 0.001 | ms/batch 1044.65 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "| it 5840 | epoch 1 | lr 0.001 | ms/batch 1071.05 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5860 | epoch 1 | lr 0.001 | ms/batch 1077.98 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5880 | epoch 1 | lr 0.001 | ms/batch 1044.45 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5900 | epoch 1 | lr 0.001 | ms/batch 1077.86 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5920 | epoch 1 | lr 0.001 | ms/batch 1034.49 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5940 | epoch 1 | lr 0.001 | ms/batch 1068.17 | loss  0.03\n",
      "| it 5960 | epoch 1 | lr 0.001 | ms/batch 1070.27 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 5980 | epoch 1 | lr 0.001 | ms/batch 1083.19 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "this image has bad label and has been removed.\n",
      "| it 6000 | epoch 1 | lr 0.001 | ms/batch 1105.66 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6020 | epoch 1 | lr 0.001 | ms/batch 1274.66 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "| it 6040 | epoch 1 | lr 0.001 | ms/batch 1042.58 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6060 | epoch 1 | lr 0.001 | ms/batch 1101.49 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6080 | epoch 1 | lr 0.001 | ms/batch 1096.60 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "| it 6100 | epoch 1 | lr 0.001 | ms/batch 1078.05 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6120 | epoch 1 | lr 0.001 | ms/batch 1068.83 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6140 | epoch 1 | lr 0.001 | ms/batch 1090.53 | loss  0.10\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6160 | epoch 1 | lr 0.001 | ms/batch 1102.63 | loss  0.27\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6180 | epoch 1 | lr 0.001 | ms/batch 1059.91 | loss  0.16\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6200 | epoch 1 | lr 0.001 | ms/batch 1073.77 | loss  0.12\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6220 | epoch 1 | lr 0.001 | ms/batch 1067.72 | loss  0.09\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6240 | epoch 1 | lr 0.001 | ms/batch 1074.21 | loss  0.10\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6260 | epoch 1 | lr 0.001 | ms/batch 1105.49 | loss  0.05\n",
      "| it 6280 | epoch 1 | lr 0.001 | ms/batch 1016.32 | loss  0.03\n",
      "| it 6300 | epoch 1 | lr 0.001 | ms/batch 1059.03 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6320 | epoch 1 | lr 0.001 | ms/batch 1034.77 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6340 | epoch 1 | lr 0.001 | ms/batch 1039.14 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6360 | epoch 1 | lr 0.001 | ms/batch 1082.77 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6380 | epoch 1 | lr 0.001 | ms/batch 1067.18 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6400 | epoch 1 | lr 0.001 | ms/batch 1062.08 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6420 | epoch 1 | lr 0.001 | ms/batch 1060.98 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6440 | epoch 1 | lr 0.001 | ms/batch 1060.45 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6460 | epoch 1 | lr 0.001 | ms/batch 1066.06 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6480 | epoch 1 | lr 0.001 | ms/batch 1030.45 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6500 | epoch 1 | lr 0.001 | ms/batch 1069.54 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6520 | epoch 1 | lr 0.001 | ms/batch 1097.96 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "| it 6540 | epoch 1 | lr 0.001 | ms/batch 1084.68 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6560 | epoch 1 | lr 0.001 | ms/batch 1039.86 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6580 | epoch 1 | lr 0.001 | ms/batch 1066.49 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "| it 6600 | epoch 1 | lr 0.001 | ms/batch 1091.01 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6620 | epoch 1 | lr 0.001 | ms/batch 1070.07 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6640 | epoch 1 | lr 0.001 | ms/batch 1064.85 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6660 | epoch 1 | lr 0.001 | ms/batch 1054.60 | loss  0.06\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6680 | epoch 1 | lr 0.001 | ms/batch 1055.83 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "| it 6700 | epoch 1 | lr 0.001 | ms/batch 1043.64 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "| it 6720 | epoch 1 | lr 0.001 | ms/batch 1070.12 | loss  0.04\n",
      "| it 6740 | epoch 1 | lr 0.001 | ms/batch 1059.80 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6760 | epoch 1 | lr 0.001 | ms/batch 1043.81 | loss  0.03\n",
      "| it 6780 | epoch 1 | lr 0.001 | ms/batch 1045.50 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "| it 6800 | epoch 1 | lr 0.001 | ms/batch 1079.24 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6820 | epoch 1 | lr 0.001 | ms/batch 1055.46 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6840 | epoch 1 | lr 0.001 | ms/batch 1064.22 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6860 | epoch 1 | lr 0.001 | ms/batch 1064.15 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6880 | epoch 1 | lr 0.001 | ms/batch 1049.92 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6900 | epoch 1 | lr 0.001 | ms/batch 1039.36 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6920 | epoch 1 | lr 0.001 | ms/batch 1079.21 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 6940 | epoch 1 | lr 0.001 | ms/batch 1028.52 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "| it 6960 | epoch 1 | lr 0.001 | ms/batch 1050.09 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "| it 6980 | epoch 1 | lr 0.001 | ms/batch 1047.17 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7000 | epoch 1 | lr 0.001 | ms/batch 1055.91 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7020 | epoch 1 | lr 0.001 | ms/batch 1051.73 | loss  0.05\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7040 | epoch 1 | lr 0.001 | ms/batch 1068.78 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7060 | epoch 1 | lr 0.001 | ms/batch 1064.70 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7080 | epoch 1 | lr 0.001 | ms/batch 1035.90 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7100 | epoch 1 | lr 0.001 | ms/batch 1040.01 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7120 | epoch 1 | lr 0.001 | ms/batch 1043.84 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7140 | epoch 1 | lr 0.001 | ms/batch 1065.02 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7160 | epoch 1 | lr 0.001 | ms/batch 1069.61 | loss  0.02\n",
      "an image has been removed for this batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7180 | epoch 1 | lr 0.001 | ms/batch 1049.10 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "| it 7200 | epoch 1 | lr 0.001 | ms/batch 1065.56 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7220 | epoch 1 | lr 0.001 | ms/batch 1072.06 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "| it 7240 | epoch 1 | lr 0.001 | ms/batch 1069.19 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7260 | epoch 1 | lr 0.001 | ms/batch 1083.35 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "| it 7280 | epoch 1 | lr 0.001 | ms/batch 1030.33 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7300 | epoch 1 | lr 0.001 | ms/batch 1057.98 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "| it 7320 | epoch 1 | lr 0.001 | ms/batch 1052.79 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7340 | epoch 1 | lr 0.001 | ms/batch 1112.28 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7360 | epoch 1 | lr 0.001 | ms/batch 1044.09 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "| it 7380 | epoch 1 | lr 0.001 | ms/batch 1043.68 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "| it 7400 | epoch 1 | lr 0.001 | ms/batch 1052.10 | loss  0.04\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7420 | epoch 1 | lr 0.001 | ms/batch 1062.50 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7440 | epoch 1 | lr 0.001 | ms/batch 1051.01 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7460 | epoch 1 | lr 0.001 | ms/batch 1070.93 | loss  0.02\n",
      "| it 7480 | epoch 1 | lr 0.001 | ms/batch 1039.55 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "| it 7500 | epoch 1 | lr 0.001 | ms/batch 1049.13 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "| it 7520 | epoch 1 | lr 0.001 | ms/batch 1102.92 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "| it 7540 | epoch 1 | lr 0.001 | ms/batch 1097.08 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "| it 7560 | epoch 1 | lr 0.001 | ms/batch 1044.50 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7580 | epoch 1 | lr 0.001 | ms/batch 1075.25 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "| it 7600 | epoch 1 | lr 0.001 | ms/batch 1080.43 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7620 | epoch 1 | lr 0.001 | ms/batch 1068.36 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "| it 7640 | epoch 1 | lr 0.001 | ms/batch 1086.59 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7660 | epoch 1 | lr 0.001 | ms/batch 1069.34 | loss  0.01\n",
      "| it 7680 | epoch 1 | lr 0.001 | ms/batch 1070.30 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7700 | epoch 1 | lr 0.001 | ms/batch 1071.07 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7720 | epoch 1 | lr 0.001 | ms/batch 1057.72 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7740 | epoch 1 | lr 0.001 | ms/batch 1070.46 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7760 | epoch 1 | lr 0.001 | ms/batch 1040.95 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7780 | epoch 1 | lr 0.001 | ms/batch 1032.64 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "| it 7800 | epoch 1 | lr 0.001 | ms/batch 1051.69 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7820 | epoch 1 | lr 0.001 | ms/batch 1038.39 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7840 | epoch 1 | lr 0.001 | ms/batch 1084.75 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7860 | epoch 1 | lr 0.001 | ms/batch 1068.10 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7880 | epoch 1 | lr 0.001 | ms/batch 1079.37 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7900 | epoch 1 | lr 0.001 | ms/batch 1062.32 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7920 | epoch 1 | lr 0.001 | ms/batch 1049.38 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7940 | epoch 1 | lr 0.001 | ms/batch 1042.37 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7960 | epoch 1 | lr 0.001 | ms/batch 1077.86 | loss  0.03\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 7980 | epoch 1 | lr 0.001 | ms/batch 1044.73 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "| it 8000 | epoch 2 | lr 0.001 | ms/batch 1017.45 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 8020 | epoch 2 | lr 0.001 | ms/batch 1277.76 | loss  0.01\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 8040 | epoch 2 | lr 0.001 | ms/batch 1077.32 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "| it 8060 | epoch 2 | lr 0.001 | ms/batch 1082.56 | loss  0.02\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n",
      "an image has been removed for this batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#compute gradient backward\u001b[39;00m\n\u001b[1;32m     46\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m triplet_losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m triplet_losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m triplet_losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubj\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m triplet_losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 47\u001b[0m \u001b[43mfinal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#total loss\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/research_env/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/research_env/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iter_dataloader = iter(train_dataloader)\n",
    "interval_cnt = 0\n",
    "chkpoint_path = '../checkpoint'\n",
    "\n",
    "#Losses\n",
    "total_loss = 0\n",
    "subj_loss = 0\n",
    "obj_loss = 0\n",
    "pred_loss = 0\n",
    "transr_loss = 0\n",
    "\n",
    "for i in range(n_iters):    \n",
    "    #continue training from the previous checkpoint\n",
    "    if (i < initial_it % n_datapoints):\n",
    "        continue\n",
    "        \n",
    "    #iterator\n",
    "    try:\n",
    "        data = next(iter_dataloader)[0]\n",
    "    except StopIteration:\n",
    "        print(\"iterator has reach its end at iteration {}. Initializing a new iterator.\".format(str(it)))\n",
    "        iter_dataloader = iter(train_dataloader)\n",
    "        data = next(iter_dataloader)[0]\n",
    "\n",
    "\n",
    "    if (len(data['relationships']['subj_bboxes']) <= 1):\n",
    "        #image has only one relationship, cannot train\n",
    "        print(\"an image has been removed for this batch\")\n",
    "        continue\n",
    "        \n",
    "    #other exclusion due to bad label\n",
    "    if ('1841.jpg' in data[\"file_name\"]):\n",
    "        print(\"this image has bad label and has been removed.\")\n",
    "        continue\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #forward passes\n",
    "    negative_examples = {}\n",
    "    negative_examples = model.generate_negative_examples(data)\n",
    "#     print(\"number of objects:\", len( data['instances'].get_fields()['gt_boxes']))\n",
    "#     print(\"number of neg examples:\", len(negative_examples))\n",
    "    triplet_losses = model(data, negative_examples)\n",
    "    \n",
    "    #compute gradient backward\n",
    "    final_loss = triplet_losses['obj'] + triplet_losses['pred'] + triplet_losses['subj'] + triplet_losses['transr']\n",
    "    final_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #total loss\n",
    "    total_loss += final_loss.item()\n",
    "    subj_loss += triplet_losses['subj'].item()\n",
    "    pred_loss += triplet_losses['pred'].item()\n",
    "    obj_loss += triplet_losses['obj'].item()\n",
    "    transr_loss += triplet_losses['transr'].item()\n",
    "    \n",
    "    interval_cnt += 1\n",
    "    if (it > initial_it and it % log_interval == 0 and it > 0):\n",
    "        current_loss = total_loss / interval_cnt\n",
    "        losses.append(current_loss)\n",
    "        elapsed = time.time() - start_time\n",
    "        epoch = it / n_datapoints\n",
    "        print('| it {} | epoch {} | lr {} | ms/batch {:5.2f} | loss {:5.2f}'.format(\n",
    "            it, int(epoch), scheduler.get_last_lr()[0], elapsed * 1000 / log_interval, current_loss))\n",
    "        train_log(current_loss, scheduler.get_last_lr()[0], it, int(epoch),\n",
    "                  loss_subj=subj_loss/interval_cnt, loss_pred=pred_loss/interval_cnt,\n",
    "                  loss_obj=obj_loss/interval_cnt, loss_transr=transr_loss/interval_cnt)\n",
    "        total_loss = 0\n",
    "        subj_loss = 0\n",
    "        pred_loss = 0\n",
    "        obj_loss = 0\n",
    "        transr_loss = 0\n",
    "        interval_cnt = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    if (it > initial_it and it % chkpoint_it == 0 and it > 0):\n",
    "        chkpnt = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"it\": it,\n",
    "            \"losses\": losses\n",
    "        }\n",
    "        torch.save(chkpnt, os.path.join(chkpoint_path, 'vltranse_langcon_model_{}.pt'.format(str(it))))\n",
    "        \n",
    "    #increment total count\n",
    "    it = it + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84d5723c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a4722",
   "metadata": {},
   "source": [
    "### [Optional] Test Eval Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4204290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import (\n",
    "    DatasetCatalog, DatasetMapper,\n",
    "    build_detection_train_loader\n",
    ")\n",
    "\n",
    "cfg = get_vrd_cfg()\n",
    "\n",
    "#model\n",
    "# model = RelTransR(cfg)\n",
    "# device = torch.device(\"cuda\")\n",
    "# model.to(device)\n",
    "\n",
    "#test dataloader\n",
    "cfg.DATASETS.TRAIN = (\"vrd_val\", )\n",
    "test_dataloader = build_detection_train_loader(cfg,\n",
    "    mapper=DatasetMapper(cfg, is_train=True, augmentations=[\n",
    "        T.Resize((800, 800))\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90815f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_loss = 0\n",
    "avg_loss = 0\n",
    "cnt = 0\n",
    "\n",
    "total_loss = 0\n",
    "pred_loss = 0\n",
    "interval_cnt = 0\n",
    "\n",
    "model.eval()\n",
    "test_data_iter = iter(test_dataloader)\n",
    "broken_image = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(2000):\n",
    "        data = next(test_data_iter)[0]\n",
    "\n",
    "        #If there is only one relationship (not suitable to be evaluate with the given loss)\n",
    "        if (len(data['relationships']['subj_bboxes']) == 1):\n",
    "            #image has only one relationship, cannot train\n",
    "            print(\"an image has been removed for this batch\")\n",
    "            broken_image.append(data)\n",
    "            continue\n",
    "\n",
    "        # get negative examples and compute losses\n",
    "        negative_examples = model.generate_negative_examples(data)\n",
    "        triplet_losses = model(data, negative_examples)\n",
    "        final_loss = triplet_losses['obj'] + triplet_losses['subj'] + triplet_losses['pred']\n",
    "\n",
    "        #output interval loss\n",
    "        total_loss += final_loss.item()\n",
    "        pred_loss += triplet_losses['subj'].item()\n",
    "        interval_cnt += 1\n",
    "        if (i > 0 and i % 20 == 0):\n",
    "            current_loss = total_loss / interval_cnt\n",
    "            pred_current_loss = pred_loss / interval_cnt\n",
    "            print('| iter: {} | loss {:5.2f} | pred_loss {:5.2f}'.format(i, current_loss, pred_current_loss))\n",
    "            total_loss = 0\n",
    "            pred_loss = 0\n",
    "            interval_cnt = 0\n",
    "\n",
    "        #update global loss\n",
    "        global_loss += final_loss.item()\n",
    "        cnt += 1\n",
    "avg_loss = total_loss / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a64a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b460f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c568deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([[1,2,3], [2,3,4]])\n",
    "y = torch.tensor([[1,2,3.2]])\n",
    "\n",
    "1- F.cosine_similarity(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.tensor([[1,2,3], [2,3,4]])\n",
    "y = torch.tensor([[1.5,2.25], [2.1,3.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2187cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b95cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((x, y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8480922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
